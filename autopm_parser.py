import pyarrow.fs
import os
import sys
import io
import zipfile
import xmltodict
import yaml
import copy
import re
import pandas as pd
import itertools
import logging
import datetime
import scipy.stats
import scipy.signal
import numpy as np
#import autopm.yaml 
#import importlib.resources
# TEST GIT 1124
logging.basicConfig(
    #filename='autotrd.log', # PS EIDH
    level=logging.INFO, 
    format= '%(asctime)s, %(name)s, %(levelname)s, %(funcName)s, %(lineno)d, %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    filemode='a'
)
logger = logging.getLogger(__name__)


class XmlAutoPM():
    DELIMITER = '\t'
    ZIP_FNAME = 'zipfile'
    ZIP_DTYPE = 'dtype'
    ZIP_DATA = 'data'
    ZIP_PMIDX = 'pmidx' #(int):: index used for extracting the module (pmalias) from the filename

    SPEC_ATTR = 'spec_attr' #(list of strs):: (optional) attribute in the rawdata to be compared against the SSHC specs
    SPEC_INFO = 'spec_info' #(list of strs):: attributes in the rawdata to add useful information for the report, e.g., gasline
    SPEC_AO = 'spec_ao' #(str):: a set-point attribute in the rawdata
    SPEC_SOFTLINK = 'softlink'
    SPEC_MIN = 'LCL'
    SPEC_MAX = 'UCL'
    SPEC_AUGM = 'spec_augm'
    SPEC_AGG = 'bool_agg'
    SPEC_CONFIG = 'ModInfo_Concat' #(str):: string pattern used for retrieving the platform configuration 
    SPEC_NAME = 'SSHC' #(str):: string pattern representing a particular SSHC
    SPEC_GASIDX = 'GasLine' #(str):: gas line index (e.g., Gas01)

    ####### INTERNAL DATA #######
    EXCEL_DATA = 'excel_data' #(df):: dataframe generated by concatenating all raw data  
    EXCEL_RPT = 'excel_doc' #(dict):: dataframe (comparison between the measured data and SSHC spec); {'SPEC_ATTR':df}
    EXCEL_ERR = 'excel_err' #(dict):: dataframe (duplicated data found when generating EXCEL_RPT); {'SPEC_ATTR':err_df}
    TOCSV_DATA = 'tocsv_data' #(dict):: human-readable dataframe; {'SPEC_ATTR':df}

    EF_PATTERN = '^ef_' # engineered features 
    ATTR_TIME = 'Timestamp.Timestamp'
    ATTR_GROUPBY = 'groupby' #(boolean):: True[=] if PMs needs to be grouped by the gasbox config
    ATTR_MODULE = 'module' # The ingestion class used to load the backup files
    ATTR_SVN = 'software_version'
    ATTR_KEY = 'key' #SSHC in trdObj[SSHC_alias]   
    ATTR_PMALIAS = 'pmalias' #alias of a module 
    ATTR_AO = 'Setpoints'
    ATTR_GASBOX = 'gasbox_idx'
    ATTR_MATCH ='Matching' #boolean data indicating if a system config is the same in all compared modules
    ATTR_NUMCONFIG = 'num_config' #numeric data corresponding to the number of configurations found in a system option
    ATTR_ISNULL ='is_null' #boolean data indicating if a system config is the same in all compared modules
    ATTR_COMMENT= 'Comment'
    ATTR_OOS = 'is_oos' #boolean data indicating if the measured data is within spec in EXCEL_RPT
    HDFS_PATH = '/tmp'
    
    XML_RAWDATA = 'macro_step'
    XML_MACRO_NAME = 'test_name'
    XML_SPEC = 'spec'
    XML_DATA = 'step_data'
    XML_TIME = 'EndTime'

    def __init__(self, filename = '', host='hdfs://hdpcluster01', port=8020, config='autopm_config.yaml'): # PS # host='hdfs://hdpclusterca'
        '''
        Parameters:
        host (str): hdfs host name
        port (int): hdfs port name
        config (str): yaml config filename specifying how to process autopm files
        '''
        try:
            self.hdfs = pyarrow.fs.HadoopFileSystem(host=host, port=port)
        # # # except AttributeError:
        #     self.hdfs = hdfs.connect(host=host, port=port)
        except OSError:
            self.hdfs = None
        self._load_config(config)
        self.filename = filename # 
        self.data = {}
        # self.trd = {}
        self.errorcode = 0

    def _load_config(self, filename='autopm_config.yaml'):
        try:
            with open(filename) as fObj:
                self.config_lib = yaml.load(fObj, Loader=yaml.FullLoader)
        except FileNotFoundError:
            self.errorcode = 2

    def _get_config(self, configfile=None, to_trd=False, config_lib={}):
        '''
        get_autotrd_config returns the config (dictonary) and the tool type
        Parameter:
        configfile (str/None): chamber type
        '''
        config = {}
        tool_models = ['GenericModule']
        if to_trd:
            tool_models.insert(0,'AutoTRD')
        if not config_lib:
            config_lib = self.config_lib
        KeyErrorFlag = True
        if configfile:
            if  configfile in config_lib:
                KeyErrorFlag = False
                tool_models.append(configfile)
            else:
                #select the longest matched name
                keys = {}
                for key in config_lib.keys():
                    mo = re.search(f'^{key}', configfile, re.IGNORECASE)
                    if mo: 
                        keys.update({key:len(key)})
                if keys:
                    key = max(keys, key=keys.get) 
                    tool_models.append(key)
                    KeyErrorFlag = False
                    configfile = key              
        if KeyErrorFlag:
            if configfile:
                logger.debug(f'{configfile} is not defined in config_lib')
            configfile = 'GenericModule'
        for tool_model in tool_models:
            config.update(copy.deepcopy(config_lib[tool_model]))
        keys = []
        #the followig commands allow the yaml file to be more flexible
        for subsys, data in config.items():
            mo = re.search(r'ef_(\w+)', subsys, re.IGNORECASE)
            if mo:
                config[mo.group(1)].update(**data)
                data = {}
            if data:
                if self.SPEC_INFO in data:
                    if isinstance(data[self.SPEC_INFO], list):
                        data[self.SPEC_INFO] ={_elem:None for _elem in data[self.SPEC_INFO]}
            else:
                keys.append(subsys)
        for key in keys:
            del config[key] 
        return config, configfile


    def _read_fs(self, hdfs_path, zipObj):
        zipData = zipfile.ZipFile(io.BytesIO(zipObj.read(hdfs_path)), 'r') 
        file_path = zipData.namelist()[0]
        with zipData.open(file_path) as file: 
            self.rawtxt = file.read()

    def _read_hdfs(self, hdfs_path, basename=''):
        try:
            if self.hdfs: # PS add for reading local file in EIDH
                #logger.info(f'hdfs_path-{hdfs_path}')
                if re.search( '/LamData', hdfs_path, re.IGNORECASE):
                    fObj = self.hdfs.open_input_file(hdfs_path)
                else:
                    fObj = open(hdfs_path, 'rb')
            else:
                fObj = open(hdfs_path, 'rb')
            with zipfile.ZipFile(fObj, 'r') as zipObj:
                if basename:
                    zipData = zipfile.ZipFile(io.BytesIO(zipObj.read(basename)), 'r') 
                else:
                    zipData = zipObj
                file_path = zipData.namelist()[0]
                with zipData.open(file_path) as file: 
                    self.rawtxt = file.read()    
        except OSError:
            if basename:
                # logger.err
                logger.info(f'hdfs_path_error-{hdfs_path}' )
            else:
                hdfs_path, basename = os.path.split(hdfs_path)
                hdfs_path = f'{hdfs_path}.zip'
                self._read_hdfs(hdfs_path, basename)

    def _xml2json(self):
        self.rawtxt = xmltodict.parse(self.rawtxt)
        self.rawtxt = self.rawtxt['document']
        self.groups = self.rawtxt.get(self.XML_RAWDATA) # macro_step 
        
        # self.groups
        if self.groups:
            if isinstance(self.groups, dict):
                self.groups = [self.groups]
                
            for group in self.groups:
                try:
                    del group['test_name']
                except KeyError:
                    pass

    def _parse_headers(self, json_data):
        keys = { 
            #'Version':'software_revision', 
            #'major_version':'major_version', 
            #'minor_version':'minor_version', 
            #'version_date':'version_date',
            'ToolID':'tool_id', 
            'UserID':'user_id', 
            'PMID':'chamber_id',
            'ModuleType':'chamber_type', 
            'ModuleSpec':'chamber_spec', 
            'macro_name':'macro_name', 
            'macro_date':'macro_date', 
            'Name':'macro_actual_filename', 
            'CompletionStatus':'macro_completion_status', 
            'StartTime':'macro_start_time', 
            'EndTime':'macro_end_time', 
            'Duration':'macro_total_elapse_time',
            'option_key':'option_key',
            'option_value':'option_value',
            #'cv_name':'config_var_name',
            #'applied_value':'config_var_file_value',
            #'current_value':'config_var_current_value', 
            #'default_value':'config_var_default_value'
        }
        properties = {}
        for key, attr in keys.items():
            try:
                item = json_data[attr]
                self.test = item
                if isinstance(json_data[attr],dict):
                    item = ' '.join(item.values())
                properties[key] = item
            except KeyError:
                paths = self._recursive_lookup(json_data, [attr])
                if paths:
                    for path in paths:
                        path.append(attr)
                        try:
                            if isinstance(properties[key], list):
                                properties[key].append(self._extract_nested_data(json_data, path))
                            else:
                                properties[key] = [properties[key], self._extract_nested_data(json_data, path)]
                        except KeyError:
                            properties[key] = self._extract_nested_data(json_data, path)
                else:
                    # raise KeyError(f'{key} does not exist')
                    logger.debug(f'{key} does not exist')
                    properties[key] = ''
        self._augm_properties(properties)
        self.properties.update(**properties)

    def _augm_properties(self, properties):
        for attr in ['StartTime', 'EndTime', 'macro_date']:
            # properties[attr] = f'{properties[attr]}'.replace('/','-')
            try:
                properties[attr] = (
                        datetime.datetime.strptime(f'{properties[attr]}', '%Y/%m/%d %H:%M:%S')
                        .strftime('%Y-%m-%d %H:%M:%S')
                )
            except ValueError:
                pass
        option_keys = ['ModelName', 'ModuleLabel', 'ModuleSpec', 'ModuleType']
        for option_key in option_keys:
            # 'option_key', & 'option_value' are elements of properties (dict)
            # The objective is to pull these parameters from the options into properties
            try:
                idx = properties['option_key'].index(option_key)
                properties['option_key'].remove(option_key)
                option_value = properties['option_value'][idx]
                if option_key not in properties:
                    properties[option_key] = option_value
                properties['option_value'].remove(option_value)
            except ValueError:
                if option_key not in properties:
                    properties[option_key] = 'None'
        try:
            option_key = 'ModelName'
            mo = re.search(r'#(\w+)\s',properties[option_key])
            if mo:
                properties[option_key] = mo.group(1)
        except KeyError:
            pass
    def _recursive_lookup(self, data, patterns, path=[]):
        '''
        patterns (list of strs)
        '''
        results = []
        if isinstance(data, dict):
            is_matched = []
            matched_attrs = {}
            for pattern in patterns:
                flag = False
                for key in data.keys():
                    mo=re.search(f'{pattern}$', key, re.IGNORECASE)
                    if mo:
                        if pattern != key:
                            matched_attrs.update({pattern:key})
                        flag = True
                        break
                is_matched.append(flag)
            if all(is_matched):
                for pattern, key in matched_attrs.items():
                    data.update({pattern:data[key]})
                results.append(path)
            else:
                for key, value in data.items():
                    results.extend(self._recursive_lookup(value, patterns, path= path+[key]))
        elif isinstance(data, list):
            for index, item in enumerate(data):
                results.extend(self._recursive_lookup(item, patterns, path= path+[index]))
        return results
    
    def _extract_nested_data(self, data, path):
        if path:
            key = path.pop(0)
            data = data[key]
            return self._extract_nested_data(data, path)
        else:
            return data

    def _augm_info(self, spec_info, group, df):
        df = df.copy()
        if self.SPEC_AGG not in df:
            df.loc[:,self.SPEC_AGG] = 0 
        for key, value in spec_info.items():
            if df.empty:
                break
            if re.search(self.EF_PATTERN, key, re.IGNORECASE): 
                continue
            msg_attr = '' if key in df else key
            if msg_attr:
                if re.search('(step_title|io_point_name)',msg_attr): # PS
                    pass
                elif not df.iloc[0][self.SPEC_AGG]:
                    df.loc[:,self.SPEC_AGG] = 1
                try:
                    df.loc[:,msg_attr] = group[msg_attr]
                except KeyError:
                    paths = self._recursive_lookup(group, [msg_attr])
                    try: 
                        path = paths[-1]
                        msg_value = self._extract_nested_data(group, path)
                        df.loc[:,msg_attr] = msg_value[msg_attr]
                    except IndexError:
                        logger.debug(f'{msg_attr} does not exist in {group.keys()}') # PS
                        return pd.DataFrame()
            if isinstance(value,str):
                df = df.loc[df.loc[:,key]
                        .apply(lambda item: True if re.search(value, f'{item}', re.IGNORECASE) else False)
                    ]
        df.reset_index(inplace=True, drop=True)
        return df

    def _format_numeric(self, data):
        nums = []
        if isinstance(data, list):
            for sample in data:
                nums.append(self._format_numeric(sample))
        else:
            return re.sub('[%]','', f'{data}', flags=re.IGNORECASE)
        return nums

    def _is_numeric(self, data):
        nums = []
        if isinstance(data, list):
            for sample in data:
                nums.append(self._is_numeric(sample))
        else:
            try:
                data = float(data)
                nums.append(True) 
            except ValueError:
                nums.append(False) 
        return all(nums)

    def _parse_data(self, spec_attr, spec_info, **kwargs):
        #db = pd.DataFrame()
        db = []
        recovery_flag = False
        if not spec_attr:
            attr = list(spec_info.keys()).pop(0)
            spec_attr = [attr]
            del spec_info[attr]
            recovery_flag=True

        for group in self.groups:
            paths = self._recursive_lookup(group, spec_attr)
            if paths: 
                data = []
                for path in paths:
                    try:
                        softlink = path[spec_info[self.SPEC_SOFTLINK]]
                    except:
                        softlink = ''
                    value = self._extract_nested_data(group, path)
                    if softlink:
                        value[self.SPEC_SOFTLINK] = softlink
                    data.append(value)
                try:
                    df = pd.DataFrame(data)
                except ValueError:
                    df = pd.DataFrame.from_dict(data, orient='index').T
                try: 
                    df[spec_attr] = df[spec_attr].applymap(self._format_numeric)
                    if recovery_flag:
                        pass
                    else:
                        is_numeric = df[spec_attr].applymap(self._is_numeric).values
                        is_numeric = itertools.chain.from_iterable(is_numeric)
                        if not any(is_numeric):
                            raise TypeError
                    df = self._augm_info(spec_info, group, df)
                except TypeError:
                    pass
                    df = self._augm_info(spec_info, group, df)
                    
                if df.empty: 
                    #db = df
                    continue
                db.append(df)
                
        if db:           
            data = db[-1] 
            # display(pd.concat(db))
            if data.loc[0,self.SPEC_AGG]:
                data = pd.concat(db)
            data.drop(columns=self.SPEC_AGG, inplace=True)
            data = data.to_dict("list")
            for key, value in data.items():
                if len(value) > 1:
                    data[key] = [value]
        else:
            data = {}
        return data

    def to_datetime(self, dt, dtfmt='%B %d, %Y %I:%M:%S %p', regex='^(.*?(pm|am))'):
        reObj = re.compile(regex, re.IGNORECASE)
        dt = datetime.datetime.strptime(reObj.search(dt).group(1), dtfmt)
        dt = [int(_elem) for _elem in dt.strftime('%Y,%m,%d,%H,%M,%S').split(',')]
        return dt    

    def _format_properties(self):
        args = {
            'dtfmt':'%Y-%m-%d %H:%M:%S',
            'regex':'^(.*)$'
        }
        self.properties[self.ATTR_TIME] = self.to_datetime(self.properties[self.XML_TIME],**args)
        
    def getmodelnames_xml(self, attrs, data=None, concat=False):
        '''
        Parameter:
        concat (bool): True [=] return the full ModelConcat; False [=] return the first None entry
        data (None/pd.DataFrame): use self.df, if no data is provided
        '''
        if isinstance(data, dict):
            data = pd.DataFrame.from_dict( data, orient='index', columns=['value'] )
        try:
            if data.empty:
                raise IndexError
            attr = attrs.pop(0)
            #an assumption here is that there is only one valid column
            ModelName = data.iloc[:,0][attr]    
            if re.search('^none', ModelName.strip(), re.IGNORECASE):
                raise KeyError
            else:
                if concat:
                    attrs.insert(0, attr)
                    ModelName = '_'.join(data.iloc[:,0][attrs].values)
        except KeyError:
            ModelName = self.getmodelnames_xml(attrs, data, concat)
        except (IndexError, AttributeError):
            ModelName = 'GenericModule'
        return ModelName        
        
    def _ingest_data(self, sshc, chamber_spec):
        if chamber_spec:
            module_spec = chamber_spec
        else:
            module_spec = self.getmodelnames_xml( attrs=['ModelName','ModuleSpec','ModuleType'], data = self.properties, concat=True ) # PS                        
        self.config, self.module = self._get_config( module_spec) 
        logger.info(f'self.config: {self.config}')
        if len(sshc) == 0:
            sshc = list(self.config.keys())
        # Find SSHC corresponding to the xml file
        item = None
        for i in sshc:
            if re.search( i , self.filename, re.IGNORECASE ):
                item = i
        self._format_properties()
        try:
            data = self._parse_data(**self.config[item])
            if data:
                self.data[item] = data
                self.data[item][self.ATTR_TIME] = [self.properties[self.ATTR_TIME]]
        except KeyError:
            logger.info( f"Couldn't find config file for '{self.filename}" )
        self._augm_data()
        logger.info(f'self.data-{self.data}')
        #special the outputdata to Z3_ and Z1_ (KiyoGX) form
        import splitZ3Data
        #logger.info(f'self.data-{self.data}') # PS
        if self.data: 
            self.data=splitZ3Data.dataFilter(self.data)

    def _augm_data(self):
        #logger.info(f'self.data-{self.data}') # PS debug
        try:
            moduleObj = getattr(sys.modules[__name__], self.module)(self.data)
        except AttributeError:
            moduleObj = getattr(sys.modules[__name__], 'GenericModule')(self.data)
        moduleObj()

    def __call__(self, hdfs_path, sshc, chamber_spec='', zipObj=None, **kwargs):
        logger.info(f'After {hdfs_path}')
        self.properties = {
            self.ZIP_FNAME:hdfs_path
        }
        if zipObj:
            self._read_fs(hdfs_path, zipObj)
        else:
            self._read_hdfs(hdfs_path)
        self._xml2json()
        if self.groups:
            self._parse_headers(self.rawtxt)
            self._ingest_data(sshc, chamber_spec)
        else:
            raise UserDefinedError
          
class TextAutoPM(XmlAutoPM):

    def _parse_rawtxt(self, encoding="utf-8"):
        self.rawtxt = [line.decode(encoding) for line in self.rawtxt]   
        self.rawtxt = [line for line in self.rawtxt if len(line.strip()) > 0]

    def _parse_metadata(self, rawtxt=[]):
        reObj_parser  = re.compile(r'''
            (\S+.*?)\s*:   # GET KEY WORD
            \s*(\S+.*)*$  # GET THE OPTION 
            ''',re.IGNORECASE|re.VERBOSE)
        metadata = [reObj_parser.findall(_elem.strip()) for _elem in rawtxt]
        metadata = dict(list(itertools.chain.from_iterable(metadata)))
        return metadata 

    def _insert_nested_data(self, json_data, path, data):
        key = path.pop(0)
        if path:
            self._insert_nested_data(json_data[key], path, data)
        else:
            try:
                json_data[key] = data
            except TypeError:
                json_data.append(data)

    def _augm_json(self, json_data):
        info ={
                'software_revision':['major_version', 'minor_version', 'version_date']
        }
        for key, values in info.items():
            try:
                json_data[key] = ' '.join([json_data[_elem] for  _elem in values])
            except KeyError:
                json_data[key] = 'unknown'

    def _xml2json(self, xml_data, json_data={}, metadata=[], path=[], prev_count=0):    
        try:
            line = xml_data.pop(0)
            logger.debug(f'{line.strip()}, {path}')
            mo = re.search(r'^(\s*)\S+', line)
            count = len(mo.group(1))
            if count == prev_count:
                if count == 0:
                    _dict = self._parse_metadata([line])
                    path = list(_dict.keys())
                    json_data.update(**_dict)
                    metadata=[]
                else:
                    metadata.append(line)
            elif count != prev_count: 
                if metadata:
                    try:
                        _dict = self._parse_metadata(metadata)
                        assert len(_dict) == len(metadata)
                    except AssertionError:
                        _dict = self._reconstruct_metadata(metadata)
                    tmp = path.copy()
                    self._insert_nested_data(json_data, tmp, _dict)
                if count < prev_count:
                    if count==0:
                        _dict = self._parse_metadata([line])
                        path = list(_dict.keys())
                        json_data.update(**_dict)
                        metadata=[]
                    else:
                        _ = [path.pop() for _ in range(prev_count-count)] 
                        metadata = [line]
                elif count > prev_count:
                    if prev_count==0:
                        metadata = [line]        
                    else:
                        path.append(list(_dict.keys())[-1])
                        metadata = [line]
                logger.debug(f'path={path}, count={count}, prev_count={prev_count}')
            return self._xml2json(xml_data, json_data, metadata, path, count)
        except IndexError:
            if metadata:
                try:
                    _dict = self._parse_metadata(metadata)
                    assert len(_dict) == len(metadata)
                except AssertionError:
                    _dict = self._reconstruct_metadata(metadata)
                self._insert_nested_data(json_data, path, _dict)
            self._augm_json(json_data)
            return [json_data]
    
    def _reconstruct_metadata(self, rawtxt=[]):
        data = []
        for line in rawtxt:
            line = [_elem.strip() for _elem in re.split(r':\s{2,}',line.strip())]
            line =': '.join(line)
            data.append(re.split(r'\s{2,}',line))
        data = pd.DataFrame(data)
        data = data.rename(columns=data.iloc[0]).drop(data.index[0])         
        return data.to_dict("list") 

    def __call__(self, hdfs_path, sshc, chamber_spec='', encoding="utf-8", zipObj=None, **kwargs):
        self.properties = {
            self.ZIP_FNAME:hdfs_path
        }
        #logger.info(f'zipObj-{zipObj}') 
        if zipObj:
            self._read_fs(hdfs_path, zipObj)
        else:
            self._read_hdfs(hdfs_path)
        self._parse_rawtxt(encoding)
        self.groups = self._xml2json(
                        xml_data=self.rawtxt, 
                        json_data={}, 
                        metadata=[], 
                        path=[], 
                        prev_count=0
                    )
        self._parse_headers(self.groups[0])
        self._ingest_data(sshc)

class GenericModule(XmlAutoPM):
    def __init__(self, data, **kwargs):
        self.data = data
    def _extract_nested_df(self, features, metadata, aliases=None):
        db = {}
        if not aliases:
            aliases = ['']*len(features)
        for alias, (pattern, attrs) in zip(aliases, features.items()):
            paths = self._recursive_lookup(metadata, [pattern])
            if paths:
                data = []
                for path in paths:
                    value = self._extract_nested_data(metadata, path)
                    data.append(pd.DataFrame(value[pattern]))
                data = pd.concat(data, axis=0)
                data = data.loc[:,attrs].to_dict("list")
                if alias:
                    db.update({f'{alias}_{attr}':[value] for attr, value in data.items()})
                else:
                    db.update({f'{attr}':[value] for attr, value in data.items()})
        return db
    
    def _augm_data(self, key, metadata):
        if re.search('Z5_|Z7_|Z10_|Z11_|Z12_|Z13_|Z14_|Z15_|Z16_|Z21_|Z27_|Z28_|Z34_|Z35_|Z37_|Z38_|Z40_|Z63_|Z67_|Z75_|Z76_|Z100_|Z101_|Z104_|Z105_|Z171_|Z172_|Z173_|Z175_|Z176_|Z183_|Z190_|Z201_|ZF101_|ZF104_|ZF105_|ZF107_|ZF307_|ZF108_|ZF109_|ZF110_|ZF117_|ZF118_|ZF534_|ZF121_|ZF124_|ZF125_|ZF126_|ZF134_|ZF174_|ZF177_|ZF181_|ZF203_|ZF318_|ZF607_|ZF372_|ZF210_|ZF308_|ZF309_|ZF433_|ZS146_|ZS147_|ZS148_|ZS158_|ZTM68_|ZTM82_|ZTM_N2_22_|ZTM_N2_23_|ZTM_N2_24_|Solo_Temperature|Solo_Leakrate|Solo_ESCHelium_202-A52722-025|Solo_PM_TCU_Temp_Flow|ZF503_|ZF509_|ZF304_|ZF533_|ZF310_|ZF311_|ZF511_|ZF520_|ZF618_|Z533_|Z556_|Z_N2_3_|Z_N2_6_|Z_N2_9_|Z_N2_14_', key, re.IGNORECASE):
            logger.info(f'key-{key}; metadata-{metadata} in generic module') # io_point_value
            attr_name, attr_name_var, attr_value = 'io_point_name', 'variable_name', 'io_point_value'
            
            if re.search('Solo_ESCHelium|Solo_Temperature', key, re.IGNORECASE): # Same io_point name for different setpoints; Use variable_name to replace io_point name
                for i in range(len(metadata[attr_name][0] ) ) :
                    metadata.update( { metadata[attr_name_var][0][i]: [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } ) 
            else: # metadata[attr_name][0]  type list or string
                if isinstance(metadata[attr_name][0], list):
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { metadata[attr_name][0][i]: [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } ) 
                    if re.search('Z11_', key, re.IGNORECASE):
                        attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
                        metadata[attr_value][0] = [float(elem) for elem in metadata[attr_value][0] if elem != None]
                        metadata.update({ f'{attr_key}_mean' : [ np.average( metadata[attr_value][0])  ] })
                else:  # str 
                    metadata[ metadata[attr_name][0] ] = [ float(metadata[attr_value][0]) if metadata[attr_value][0]!='None' else 'NULL' ] # PS
                # Status step10
            if re.search('ZF117_|ZF203_|ZF125_|ZF511_', key, re.IGNORECASE):  # PS for Flex FL
                # logger.info(f'metadata-{metadata}')
                attr_key = '_'.join( key.split('_')[1:] )
                attr_name2 = 'status'
                metadata.update( { f'{attr_key}1': [ metadata[attr_name2][0][0] ] } )  
            # Max value of two io points    
            elif re.search('Z104_|Z173_|Z176_', key, re.IGNORECASE):  # PS  float => max
                tmp = [ float(elem) for elem in metadata[attr_value][0] if elem != 'None' ] 
                metadata.update( { 'MaxTCURampDownTimeDelta': [ np.max( tmp ) ] } )
            # logger.info(f'metadata2-{metadata}')  
            elif re.search('Z21_', key, re.IGNORECASE):
                count = 0
                temperature_sum = 0
                for i in range(len(metadata[attr_name][0])):
                    if 'Temperature' in metadata[attr_name][0][i]:
                        count += 1
                        temperature_sum += float(metadata[attr_value][0][i])
                metadata.update( {'TCPTCRTemperatureCalibration_AI': [ round(temperature_sum / count, 2) ]} )

        # elif re.search('Z171_', key, re.IGNORECASE):
        #     #logger.info(f'key-{key}; metadata-{metadata}') # io_point_value; key value as variable
        #     attr_name, attr_value = 'io_point_name', 'io_point_value'
        #     attr_key = '_'.join( key.split('_')[1:] ) 
        #     if isinstance(metadata[attr_name][0], list):
        #         for i in range(len(metadata[attr_name][0] ) ) :
        #             metadata.update( { f'{attr_key}{i+1}' : [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } ) 
            #logger.info(f'metadata-{metadata}')  

        elif re.search('Z17_|Z39_|Z66_|Z71_|Z142_|Z160_|ZF102_|ZF133_|ZS143_|ZS161_|Z561_', key, re.IGNORECASE): # io_point_value; calcualtion: max flows/FlowDifferErrorPercentage
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_value = 'io_point_name', 'io_point_value'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0] if elem !='None' ] # if elem !='None'
                    metadata.update( { f'{attr_key}_calculated1': [ np.max( metadata[attr_value][0] ) ] } )  # max(multiple flow_AI)
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
                    logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}') 

        elif re.search('Z33_', key, re.IGNORECASE): # io_point_value; calcualtion
            logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_value = 'io_point_name', 'io_point_value'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0] if elem !='None' ] # if elem !='None'
                    metadata.update( { f'{attr_key}_calculated1': [ np.average( metadata[attr_value][0][:4] )-60 ] } )  # mean(4 zone ESCTemperature_AI)-60
                    metadata.update( { f'{attr_key}_calculated2': [ np.average( metadata[attr_value][0][4:] ) ] } )
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
                    logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}') 
        elif re.search('Solo_PM_Leakback', key, re.IGNORECASE):
            # logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_value = 'step_no', 'leakrate'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            try:
                if isinstance(metadata[attr_name][0], list):
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } ) 
                else:  # str 
                    metadata[ f'{attr_key}{metadata[attr_name][0]}' ] = [ float(metadata[attr_value][0]) if metadata[attr_value][0]!='None' else 'NULL' ] # PS
            except ValueError:         
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            logger.info(f'PM_Leakback metadata-{metadata}')    

        elif re.search('Z32_|ZF200_|ZF201_', key, re.IGNORECASE):  # result
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_value = 'step_no', 'result'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            try:
                if isinstance(metadata[attr_name][0], list):
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } ) 
                else:  # str 
                    metadata[ f'{attr_key}{metadata[attr_name][0]}' ] = [ float(metadata[attr_value][0]) if metadata[attr_value][0]!='None' else 'NULL' ] # PS
            except ValueError:         
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}')    
            
        elif re.search('Z4_|Z6_|Z8_|Z9_|Z18_|Z19_|Z20_|Z30_|Z42_|Z61_|Z62_|Z64_|Z70_|Z72_|Z74_|Z77_|Z80_|Z81_|Z83_|Z84_|Z86_|Z89_|Z103_|Z109_|Z145_|Z149_|Z178_|Z182_|Z202_|Z203_|Z209_|Z210_|Z528_|Z529_|Z540_|Z541_|Z542_|Z543_|Z544_|ZTM26_|ZTM58_|ZTM60_|ZTM69_|ZTM78_|ZTM87_|ZTM88_|ZTM89_|ZTM90_|ZTM518_|ZFR106_|ZFR103_|ZFR111_|ZFR301_|ZFR113_|ZFR309_|ZFR313_|ZFR115_|ZFR116_|ZFR136_|ZFR137_|ZFR139_|ZFR141_|ZF127_|ZF119_|ZF122_|ZF123_|ZF128_|ZF131_|ZF132_|ZF182_|ZF320_|ZF340_|ZS156_|ZR44_|ZR45_|ZR47_|ZR48_|ZR49_|ZR52_|ZR53_|ZR54_|ZR55_|ZR56_|ZR57_|ZR60_|ZR73_|ZR90_|ZR115_|ZR150_|ZR151_|ZSR152_|ZSR153_|ZSR154_|ZSR155_|ZSR159_|Solo_Capacitor|Solo_PinLifter|Solo_CorvusPin|Solo_ESCTempPerformance|Solo_ESCIdleTemp|Solo_PM_Leakback|Solo_ESCHeaterDutyCycle|Solo_ESCHeliumWithWaferNew|Solo_FI_CDA|Solo_AL_N2|Solo_PM_BH_Leak|Solo_PM_N2_CDA|Solo_PM_exhaust|Solo_PM_He|Solo_PM_PCW|Solo_Turbo_N2_PCW|Solo_Gasbox_exhaust_CDA|Solo_Drypump_PCW|Solo_Chiller_PCW|Solo_Bcl3|Solo_PM_V8|Solo_PM_Conductance|Solo_PM_GasLineLeak|Solo_PM_BasePressure|Solo_PM_DD_slope|Solo_PM_DD_NoPlasma|Solo_PM_ESC_Resistance|Solo_TM_N2_CDA|Solo_TM_AL_leak|Solo_TM_AL_PumpVent|Solo_TM_AL_PressureDelta|Z206_|Z519_|Z521_|Z524_|Z525_|Z526_|Z527_|Z530_|Z531_|Z532_|Z534_|Z535_|Z536_|Z537_|Z538_|ZTM517_|ZTM522_|ZTM523_|ZTM539_|ZTM540_|ZTM541_|ZTM_N2_58_|ZTM_N2_59_|Z545_|Z546_|Z547_|Z563_|Z564_|Z565_|Z566_|Z568_|Z571_|ZFR501_|ZF502_|ZF504_|ZF507_|ZF508_|ZFR510_|ZF319_|ZF512_|ZF518_|ZF519_|ZF521_|ZF522_|ZF523_|ZF524_|ZF525_|ZF526_|ZF527_|ZF528_|ZF529_|ZF530_|ZF531_|ZF532_|ZF513_|ZF514_|ZF515_|ZF516_|ZF619_|ZF620_|ZF708_|ZFR613_|Z550_|Z551_|Z552_|Z553_|Z554_|Z557_|Z558_|Z559_|Z560_|Z562_|Z_N2_1_|Z_N2_4_|Z_N2_5_|Z_N2_7_|Z_N2_8_|Z_N2_10_|Z_N2_11_|Z_N2_12_|Z_N2_13_', key, re.IGNORECASE): 
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            logger.info(f'key-{key}; metadata-{metadata}; attr_key-{attr_key}') # comments; Step
            try: # PS
                if isinstance(metadata[attr_name][0], list):
                    if len( np.unique( metadata[attr_name][0] ) ) == 1: # if sub-steps (@step_no) are same, use step_no; Z42
                        attr_name = 'step_no'
                    for i in range(len(metadata[attr_name][0] ) ) :
                        if re.search('ZTM87_', key, re.IGNORECASE):
                            metadata.update( { f'{attr_key}{i + 1}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'Yes', 'yes', 'Go', 'go', 'Normal', 'normal') else 'NULL' ] } ) 
                        else:
                            metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'Yes', 'yes', 'No', 'no' 'Go', 'go', 'Normal', 'normal', 'Fully open') else 'NULL' ] } ) 
                    # Z4 comments; Step; Calculation!
                    if re.search('Z4_', key, re.IGNORECASE): # PS change for Z4_
                        if isinstance(metadata[attr_name][0], list):
                            try: 
                                metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                                tmp = metadata[attr_value][0]
                                metadata.update( { f'{attr_key}7': [ np.average( tmp[:3] ) ] } )  # mean(step1~3) for Z4 and ZS163_
                                metadata.update( { f'{attr_key}8': [ np.average( tmp[3:] ) ] } )  # mean(step4~6)
                                
                                metadata.update( { f'{attr_key}9': [ max(tmp[3:])-min(tmp[3:]) ] } )  # step1~step3 兩兩相減取絕對值後, 取最大值 => 最大值減最小值
                            except ValueError:
                                logger.info( f'data in {key} is not calculable' )
                        logger.info(f'key-{key}; metadata-{metadata}')        
                    if re.search('ZFR309_', key, re.IGNORECASE): # Darren for MD1 2025/11 
                        if isinstance(metadata[attr_name][0], list):
                            try: 
                                metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                                tmp = metadata[attr_value][0]
                                metadata.update( { f'{attr_key}_calculated1': [ np.average( tmp[:6] ) ] } )  # mean(step1~6) for ZFR309
                            except ValueError:
                                logger.info( f'data in {key} is not calculable' )
                        logger.info(f'key-{key}; metadata-{metadata}')        
                    # max value of step1 and step2
                    if re.search('ZR49_|ZR52_|ZFR111_|ZSR153_', key, re.IGNORECASE):
                        try:
                            metadata.update( { f'{attr_key}3': [ max( metadata[f'{attr_key}1'][0], metadata[f'{attr_key}2'][0]  ) ] }  ) 
                        except (ValueError, TypeError):
                            logger.info(f'key-{key}; metadata-{metadata}') 
                            pass
                    # max value of step1 and step3
                    if re.search('ZF127_', key, re.IGNORECASE):
                        try:
                            metadata.update( { f'{attr_key}5': [ max( metadata[f'{attr_key}1'][0], metadata[f'{attr_key}3'][0]  ) ] }  )       
                        except (ValueError, TypeError):
                            logger.info(f'key-{key}; metadata-{metadata}') 
                            pass
                    # max value of step1-50 and step3-80
                    if re.search('ZF128_', key, re.IGNORECASE): 
                        try:
                            metadata.update( { f'{attr_key}5': [ max( metadata[f'{attr_key}1'][0]-50, metadata[f'{attr_key}3'][0]-80 ) ] }  )                         
                        except (ValueError, TypeError):
                            logger.info(f'key-{key}; metadata-{metadata}') 
                            pass       
                    if re.search('Z532_|Z547_', key, re.IGNORECASE): # PS add for Z352_ and Z547_ ; if key 0, change to NULL
                        try:                  
                            for i in range(len(metadata[attr_name][0] ) ) :
                                metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ 'NULL' if metadata[ f'{attr_key}{metadata[attr_name][0][i]}' ][0]  == 0 else metadata[ f'{attr_key}{metadata[attr_name][0][i]}' ][0]  ] } )                    
                        except (ValueError, TypeError):
                            logger.info(f'key-{key}; metadata-{metadata}') 
                            pass    
                    if re.search('ZTM90_', key, re.IGNORECASE):
                        attr_des = 'description'
                        try:
                            for i in range(len(metadata[attr_des][0])):
                                des_name = '_'.join(metadata[attr_des][0][i].split(' ')[:3])
                                metadata.update( {f'{des_name}' : [ float(metadata[attr_value][0][i])]})
                        except (ValueError, TypeError):
                            logger.info(f'key-{key}; metadata-{metadata}') 
                            pass    
                else:  # str 
                    logger.info(f'key-{key}') 
                    metadata[ f'{attr_key}{metadata[attr_name][0]}' ] = [ float(metadata[attr_value][0]) if metadata[attr_value][0] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES','yes','Yes','Normal','No alarm','Fully open') else 'NULL' ] # PS            
            
            except (ValueError, KeyError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name     
            if re.search('ZR57_', key, re.IGNORECASE): 
                if metadata['description'][0].startswith('DC'): # PS ZR57_old 
                    metadata.clear()
            logger.info(f'metadata-{metadata}')  # PS           

        elif re.search('ZTM28_|ZTM43_|ZTM50_|ZTM51_|ZTM101_|ZTM505_|ZTM507_|ZR165_|ZSR166_|Z162_|ZFR112_|ZF142_|ZF517_|ZFR315_|ZTM542_|Z191_|Z520_|Z567_|Z569_|Z570_', key, re.IGNORECASE): 
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step; value (0) or text/string ('No alarm', 'No RF alarms', part number, 'pass')
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            try: 
                if isinstance(metadata[attr_name][0], list):
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ metadata[attr_value][0][i] if re.search('no|pass', metadata[attr_value][0][i], re.IGNORECASE) else float(metadata[attr_value][0][i]) ] } ) 
                else: # Z162, ZR165, ZSR166, ZFR112, ZF142, ZTM505, ZF517
                    metadata[ f'{attr_key}{metadata[attr_name][0]}' ] = [ metadata[attr_value][0] ] # PS            
            except ValueError:
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            if re.search('ZTM43_', key, re.IGNORECASE):
                temp_values = metadata[attr_value][0][::2]
                ramp_up_value = [float(value) for value in temp_values if value not in (None, '')]
                temp_values = metadata[attr_value][0][1::2]
                ramp_down_value = [float(value) for value in temp_values if value not in (None, '')]
                metadata.update({f'{attr_key}_RampUpTimeMax': [np.max(ramp_up_value)]})
                metadata.update({f'{attr_key}_RampDownTimeMax': [np.max(ramp_down_value)]})
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name                   
            logger.info(f'metadata-{metadata}')  
            
        elif re.search('ZR87_', key, re.IGNORECASE): 
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step1 value; step 2 text (no burst)
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            try: # PS
                if isinstance(metadata[attr_name][0], list):
                    metadata.update( { f'{attr_key}{metadata[attr_name][0][0]}': [ float(metadata[attr_value][0][0]) if metadata[attr_value][0][0] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } ) 
                    metadata.update( { f'{attr_key}{metadata[attr_name][0][1]}': [ metadata[attr_value][0][1].lower() if metadata[attr_value][0][1] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } ) 
            except ValueError:
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name            
            #logger.info(f'metadata-{metadata}')    
            
        if re.search('Z555_', key, re.IGNORECASE): # comments; step 1~2 text, add attr3 if both of steps are 'pass'
            #logger.info(f'key-{key}; metadata-{metadata}')  
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]     
            try:
                if isinstance(metadata[attr_name][0], list):
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ metadata[attr_value][0][i] ] } ) 
                else:  # str 
                    raise ValueError
            except ValueError:         
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            try: #  add attr3 if both of steps are 'pass'
                metadata.update( { f'{attr_key}3': [ 'pass' if (metadata[f'{attr_key}1'][0] =='pass' and metadata[f'{attr_key}2'][0] =='pass') else 'failed' ] }  )                         
            except (ValueError, TypeError):
                pass            
            #logger.info(f'key-{key}; metadata-{metadata}') # PS   

        elif re.search('Z164_', key, re.IGNORECASE): 
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step1 value; step 2 text (less than ESC surface)
            attr_name, attr_value = 'step_no', 'comments' # 
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            try: # PS
                if isinstance(metadata[attr_name][0], list):
                    metadata.update( { f'{attr_key}{metadata[attr_name][0][0]}': [ float(metadata[attr_value][0][0]) if metadata[attr_value][0][0] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } ) 
                    metadata.update( { f'{attr_key}{metadata[attr_name][0][1]}': [ metadata[attr_value][0][1]  ] } ) 
            except ValueError:
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name            
            #logger.info(f'metadata-{metadata}')               
            

        elif re.search('ZS163_', key, re.IGNORECASE):  # comments; Step; Calculation! # old Z4_
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                    tmp = metadata[attr_value][0]
                    metadata.update( { f'{attr_key}1': [ np.average( tmp[:3] ) ] } )  # mean(step1~3) for Z4 and ZS163_
                    metadata.update( { f'{attr_key}2': [ np.average( tmp[3:] ) ] } )  # mean(step4~6)
                    metadata.update( { f'{attr_key}3': [ max(tmp[3:])-min(tmp[3:]) ] } )  # step1~step3 兩兩相減取絕對值後, 取最大值 => 最大值減最小值
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name                    
            logger.info(f'metadata-{metadata}')
            
        elif re.search('ZFR135_', key, re.IGNORECASE):  # comments; Step; Calculation! max of step1~3
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                    tmp = metadata[attr_value][0]
                    metadata.update( { f'{attr_key}1': [ max( tmp[:3] ) ] } )  # max(step1~3) 
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
                    logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name                    
            #logger.info(f'metadata-{metadata}')               
            
        elif re.search('ZF120_', key, re.IGNORECASE):  # comments; Step; Calculation!
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                    tmp = metadata[attr_value][0]
                    metadata.update( { f'{attr_key}1': [ np.average( tmp[:3] ) ] } )  # mean(step1~3)
                    metadata.update( { f'{attr_key}2': [ np.average( tmp[3:6] ) ] } )  # mean(step4~6)
                    metadata.update( { f'{attr_key}3': [ np.average( tmp[6:] ) ] } )  # mean(step7~9)
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
                    logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name                    
            #logger.info(f'metadata-{metadata}')              

        elif re.search('Z46_', key, re.IGNORECASE):  # comments; Step; Calculation!
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                    tmp = metadata[attr_value][0]
                    metadata.update( { f'{attr_key}1': [ np.min( tmp[:5] ) ] } )  # min(step1~5)
                    metadata.update( { f'{attr_key}2': [ np.max( tmp[5:] ) ] } )  # max(step6~10)
                    metadata.update( { f'{attr_key}3': [ np.average( tmp[2] ) ] } )  # step3
                    metadata.update( { f'{attr_key}4': [ np.max( tmp[3:5] ) ] } )  # max(step4~5)                    
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
                    logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name                    
            #logger.info(f'metadata-{metadata}')  
            
        elif re.search('Z31_', key, re.IGNORECASE):  # comments; Step; Calculation!
            #logger.info(f'key-{key}; metadata-{metadata}') # comments; Step
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            if isinstance(metadata[attr_name][0], list):
                try: 
                    metadata[attr_value][0] = [ float(elem) if elem not in ('None', 'NA') else 'NULL' for elem in metadata[attr_value][0] ] # if elem !='None' 
                    tmp = metadata[attr_value][0]
                    if tmp[0] != 'NULL' and tmp[2] != 'NULL' :
                        metadata.update( { f'{attr_key}_calculated1': [ tmp[0]-tmp[2] ] } ) # step1-step3
                    if tmp[1] != 'NULL' and tmp[3] != 'NULL' :
                        metadata.update( { f'{attr_key}_calculated2': [ tmp[1]-tmp[3] ] } ) # step2-step4
                    if tmp[4] != 'NULL' and tmp[6] != 'NULL' :
                        metadata.update( { f'{attr_key}_calculated3': [ tmp[4]-tmp[6] ] } ) # step5-step7
                    if tmp[5] != 'NULL' and tmp[7] != 'NULL' :
                        metadata.update( { f'{attr_key}_calculated4': [ tmp[5]-tmp[7] ] } ) # step6-step8
                except ValueError:
                    logger.info( f'data in {key} is not calculable' )
                    logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name                    
            #logger.info(f'metadata-{metadata}')             
 
        elif re.search('Z168_|Z169_|ZF170_|ZF399_', key, re.IGNORECASE):  # One PT "error percent" abs
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_name2, attr_value = 'step_title', 'gas_line_no', 'percent_error' #  Gas6_95_abs
            parser = re.compile( r'\W*(\d+)%') 
            try: # PS
                if isinstance(metadata[attr_name][0], list): # abs
                    for i in range(len(metadata[attr_name][0] ) ) :
                        point =  parser.findall( metadata[attr_name][0][i] )[0] 
                        gas = metadata[attr_name2][0][i]                        
                        metadata.update( { f'{gas}_{point}_abs': [ abs( float(metadata[attr_value][0][i]) ) if metadata[attr_value][0][i] not in ('None') else 'NULL'    ] } ) 
                else:  # str 
                    point =  parser.findall( metadata[attr_name][0] )[0] 
                    gas = metadata[attr_name2][0]
                    metadata[ f'{gas}_{point}_abs' ] = [ abs( float(metadata[attr_value][0]) ) if metadata[attr_value][0] not in ('None') else 'NULL' ]       
                if re.search('ZF170_', key, re.IGNORECASE):  # max( abs(O2) )
                    metadata[ 'Gas2Gas15_9_abs' ] = [ max( abs(metadata['Gas2_9_abs'][0]), abs(metadata['Gas15_9_abs'][0] ) ) ] 
            
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}')   

        elif re.search('Z102_', key, re.IGNORECASE):  # One PT "error percent" #2025/11 Darren for MeatalM and NXP 
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_name2, attr_value = 'step_title', 'gas_line_no', 'percent_error' #  Gas6_95_abs
            parser = re.compile( r'\W*(\d+(?:\.\d+)?)%') #include float %
            try: # PS
                if isinstance(metadata[attr_name][0], list): # abs
                    for i in range(len(metadata[attr_name][0] ) ) :
                        point =  parser.findall( metadata[attr_name][0][i] )[0] 
                        gas = metadata[attr_name2][0][i]                        
                        metadata.update( { f'{gas}_{point}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None') else 'NULL'    ] } ) 
                else:  # str 
                    point =  parser.findall( metadata[attr_name][0] )[0] 
                    gas = metadata[attr_name2][0]
                    metadata[ f'{gas}_{point}' ] = [ float(metadata[attr_value][0])  if metadata[attr_value][0] not in ('None') else 'NULL' ]    
            
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}')   
        elif re.search('Z181_', key, re.IGNORECASE):  # One PT "error percent" no abs
            logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_name2, attr_value = 'step_title', 'gas_line_no', 'percent_error' #  Gas7_5
            parser = re.compile( r'\W*(\d+)%') 
            try: # PS
                if isinstance(metadata[attr_name][0], list): 
                    for i in range(len(metadata[attr_name][0] ) ) :
                        point =  parser.findall( metadata[attr_name][0][i] )[0] 
                        gas = metadata[attr_name2][0][i]                        
                        metadata.update( { f'{gas}_{point}': [ float(metadata[attr_value][0][i])  if metadata[attr_value][0][i] not in ('None') else 'NULL'    ] } )             
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}')      

        elif re.search('Z167_|Z179_', key, re.IGNORECASE):  # One PT "error percent" under specific sccm
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_name2, attr_value = 'step_title', 'gas_line_no', 'percent_error' #  Gas1_255
            parser = re.compile( r'set point (\w+)sccm' ) # PS #   r'\W*(\w+)sccm'
            try: # PS
                if isinstance(metadata[attr_name][0], list): 
                    for i in range(len(metadata[attr_name][0] ) ) :
                        sccm =  parser.findall( metadata[attr_name][0][i] )[0] 
                        gas = metadata[attr_name2][0][i]                        
                        metadata.update( { f'{gas}_{sccm}': [ abs( float(metadata[attr_value][0][i]) ) if metadata[attr_value][0][i] not in ('None') else 'NULL'    ] } ) 
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}')
        elif re.search('ZF401', key, re.IGNORECASE):  # One PT "error percent" under specific sccm 2025/11 Darren for GL MD1
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_name2, attr_value = 'step_title', 'gas_line_no', 'percent_error' #  Gas1_255
            parser = re.compile( r'(\d+(?:\.\d+)?)\s*sccm' ) # Darren for include float
            try: # PS
                if isinstance(metadata[attr_name][0], list): 
                    for i in range(len(metadata[attr_name][0] ) ) :
                        sccm =  parser.findall( metadata[attr_name][0][i] )[0] 
                        gas = metadata[attr_name2][0][i]                        
                        metadata.update( { f'{gas}_{sccm}': [ float(metadata[attr_value][0][i])  if metadata[attr_value][0][i] not in ('None') else 'NULL'    ] } ) 
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            #logger.info(f'metadata-{metadata}')
        elif re.search('ZF180_', key, re.IGNORECASE):  # One PT "flow_actual" under specific sccm
            logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_name2, attr_value = 'step_title', 'gas_line_no', 'flow_actual' #  Gas2_9s
            parser = re.compile( r'\W*(\w+)sccm') # PS
            try: # PS
                if isinstance(metadata[attr_name][0], list): 
                    for i in range(len(metadata[attr_name][0] ) ) :
                        sccm =  parser.findall( metadata[attr_name][0][i] )[-1] 
                        gas = metadata[attr_name2][0][i]                        
                        metadata.update( { f'{gas}_{sccm}s': [ abs( float(metadata[attr_value][0][i]) ) if metadata[attr_value][0][i] not in ('None') else 'NULL' ] } ) 
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}')
            #logger.info(f'metadata-{metadata}')               

        elif re.search('ZTM59_', key, re.IGNORECASE): # comments; Step; decription; seperate PMs
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_value = 'description', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            parser = re.compile( r'When ([\w]*) Chamber' )
            try: 
                if isinstance(metadata[attr_name][0], list):            
                    for i in range(len(metadata[attr_name][0] ) ) :
                        alias = parser.findall( metadata[attr_name][0][i] )[0]   #
                        metadata.update( { f'{attr_key}_{alias}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } )   
                else: # 'str'
                    alias = parser.findall( metadata[attr_name][0] )[0]
                    metadata.update( { f'{attr_key}_{alias}': [ float(metadata[attr_value][0]) if metadata[attr_value][0] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } ) 
            except (ValueError, IndexError):
                logger.info(f'Cannot convert to value for {key}')  
                logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name            
            #logger.info(f'metadata-{metadata}') 

        elif re.search('ZTM91_|ZTM157_|ZTM506_', key, re.IGNORECASE): # comments; Step; decription; seperate PMs
            logger.info(f'key-{key}; metadata-{metadata}') 
            attr_name, attr_value = 'description', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) 
            parser = re.compile( r'([\w]*) counts' )
            try: 
                if isinstance(metadata[attr_name][0], list):            
                    for i in range(len(metadata[attr_name][0] ) ) :
                        alias = parser.findall( metadata[attr_name][0][i] )[0]
                        metadata.update( { f'{attr_key}_{alias}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } )   
                else: # 'str'
                    alias = parser.findall( metadata[attr_name][0] )[0]
                    metadata.update( { f'{attr_key}_{alias}': [ float(metadata[attr_value][0]) if metadata[attr_value][0] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes') else 'NULL' ] } ) 
            except (ValueError, IndexError):
                logger.info(f'Cannot convert to value for {key}')  
                logger.info(f'metadata-{metadata}')                     
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name            
            #logger.info(f'metadata-{metadata}')    
 
        elif re.search('ZTM129_|ZTM130_|ZTM329_', key, re.IGNORECASE): # io_point_value; seperate PMs for 17 gases [Flex FL]; calculation
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_value = 'io_point_value'
            attr_key = '_'.join( key.split('_')[1:] ) 
            # PMx mean
            try:            
                if isinstance(metadata[attr_value][0], list):  
                    metadata[attr_value][0] = [ float(i) for i in metadata[attr_value][0] if i !='None'  ]  
                    for i in range(5) :
                        metadata.update( { f'{attr_key}_PM{i+1}': [ np.average( metadata[attr_value][0][ i*17: (i*17)+17 ] )  ] } )   
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}')     
                logger.info(f'metadata-{metadata}')
            #logger.info(f'metadata-{metadata}') 
            
        elif re.search('ZTM140_', key, re.IGNORECASE): # io_point_value; seperate PMs
            #logger.info(f'key-{key}; metadata-{metadata}') 
            attr_value = 'io_point_value'
            attr_key = '_'.join( key.split('_')[1:] ) 
            try:            
                if isinstance(metadata[attr_value][0], list):  
                    metadata[attr_value][0] = [ float(i) for i in metadata[attr_value][0] if i !='None'  ]  
                    for i in range( len( metadata[attr_value][0] ) ) :
                        metadata.update( { f'{attr_key}_PM{i+1}': [ metadata[attr_value][0][i] ] } )   
            except (ValueError, TypeError):
                logger.info(f'Cannot convert to value for {key}')  
                logger.info(f'metadata-{metadata}')
            #logger.info(f'metadata-{metadata}')              
        elif re.search('ZTM22_|ZTM23_|ZTM24_', key, re.IGNORECASE):
            attr_name, attr_value, attr_resource = 'io_point_name', 'io_point_value', 'resource'
            if isinstance(metadata[attr_name][0], list):
                if 'Press_AI' in metadata[attr_name][0]:
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { f'{metadata[attr_name][0][i]}_{metadata[attr_resource][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } )   
                else:
                    for i in range(len(metadata[attr_name][0] ) ) :
                        metadata.update( { metadata[attr_name][0][i]: [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } ) 
            logger.info(f'metadata-{metadata}')
        elif re.search('ZTM25_|ZTM_N2_25_', key, re.IGNORECASE): # ZTM; resources
            #logger.info(f'key-{key}; metadata-{metadata}')
            attr_name, attr_value, attr_resource = 'io_point_name', 'io_point_value', 'resource'
            if isinstance(metadata[attr_name][0], list):
                for i in range(len(metadata[attr_name][0] ) ) :
                    metadata.update( { f'{metadata[attr_name][0][i]}_{metadata[attr_resource][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } )   
            #logger.info(f'metadata-{metadata}')
        elif re.search('ZTM27_|ZTM_N2_27_', key, re.IGNORECASE): # ZTM; resources; calculation msec -> sec
            #logger.info(f'key-{key}; metadata-{metadata}')
            attr_name, attr_value, attr_resource = 'io_point_name', 'io_point_value', 'resource'
            if isinstance(metadata[attr_name][0], list):
                for i in range(len(metadata[attr_name][0] ) ) :
                    metadata.update( { f'{metadata[attr_name][0][i]}_{metadata[attr_resource][0][i]}': [ float(metadata[attr_value][0][i])/1000 if metadata[attr_value][0][i]!='None' else 'NULL' ] } )   
            #logger.info(f'metadata-{metadata}')              

    def _augm_control_limits(self, metadata):
        attr = 'tolerance_spec'
        try:
            if attr in metadata:
                tolerance_value = self._format_numeric(metadata.get(attr))
                tolerance_value =np.array(tolerance_value, dtype=float)
                metadata.update({'ef_ucl':tolerance_value.tolist()})
                metadata.update({'ef_lcl':(-tolerance_value).tolist()})
        except ValueError:
            keys = list(metadata.keys())
            for key in keys:
                del metadata[key]
    def __call__(self):
        errs = []
        for attr, metadata in self.data.items():
            self._augm_data(attr, metadata)
            self._augm_control_limits(metadata)
            if len(metadata) == 0:
                errs.append(attr)
        for key in errs:
            del self.data[key]

class KiyoF(GenericModule):
    def _augm_data(self, key, metadata):
        if re.search('Z4_', key, re.IGNORECASE):
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            logger.info(f'key-{key}; metadata-{metadata}; attr_key-{attr_key}') # comments; Step
            try: # PS
                if isinstance(metadata[attr_name][0], list):
                    if len( np.unique( metadata[attr_name][0] ) ) == 1: # if sub-steps (@step_no) are same, use step_no; Z42
                        attr_name = 'step_no'
                    # SOLO Z4 do not need the each value of all of them.
                    # Only need the mean value of step 1~3, 4~6 7~9 10~12
                    # for i in range(len(metadata[attr_name][0] ) ) :
                    #     metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes' ) else 'NULL' ] } ) 
                    # Z4 comments; Step; Calculation!
                    if re.search('Z4_', key, re.IGNORECASE): # PS change for Z4_
                        if isinstance(metadata[attr_name][0], list):
                            try: 
                                metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                                tmp = metadata[attr_value][0]
                                mean_count = len(tmp) // 3
                                print(f'tmp: {tmp}')

                                logger.info( f'data in solo class' )
                                for i in range(mean_count):
                                    metadata.update( { f'{attr_key}{i + 1}': [ np.average( tmp[3 * i:3 * i + 3] ) ] } )  # mean(step1~3) for Z4 and ZS163_
                                logger.info( f'metadata: {metadata}' )

                            except ValueError:
                                logger.info( f'data in {key} is not calculable' )
                        logger.info(f'key-{key}; metadata-{metadata}')        
            except (ValueError, KeyError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name     
        else:
            super()._augm_data(key, metadata)

class KiyoG(GenericModule):
    def _augm_data(self, key, metadata):
        super()._augm_data(key, metadata)

class Flex(GenericModule):
    def _augm_data(self, key, metadata):
        super()._augm_data(key, metadata)

class Solo(GenericModule):
    def _augm_data(self, key, metadata):
        if re.search('Z4_', key, re.IGNORECASE):    # 
            attr_name, attr_value = '@step_no', 'comments'
            attr_key = '_'.join( key.split('_')[1:] ) # key.split('_')[-1]
            logger.info(f'key-{key}; metadata-{metadata}; attr_key-{attr_key}') # comments; Step
            try: # PS
                if isinstance(metadata[attr_name][0], list):
                    if len( np.unique( metadata[attr_name][0] ) ) == 1: # if sub-steps (@step_no) are same, use step_no; Z42
                        attr_name = 'step_no'
                    # SOLO Z4 do not need the each value of all of them.
                    # Only need the mean value of step 1~3, 4~6 7~9 10~12
                    # for i in range(len(metadata[attr_name][0] ) ) :
                    #     metadata.update( { f'{attr_key}{metadata[attr_name][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i] not in ('None', 'test', 'NA', 'OK', 'ok', 'PASS', 'pass', 'YES', 'yes' ) else 'NULL' ] } ) 
                    # Z4 comments; Step; Calculation!
                    if re.search('Z4_', key, re.IGNORECASE): # PS change for Z4_
                        if isinstance(metadata[attr_name][0], list):
                            try: 
                                metadata[attr_value][0] = [ float(elem) for elem in metadata[attr_value][0]  ] # if elem !='None'
                                tmp = metadata[attr_value][0]
                                mean_count = len(tmp) // 3
                                print(f'tmp: {tmp}')

                                logger.info( f'data in solo class' )
                                for i in range(mean_count):
                                    metadata.update( { f'{attr_key}{i + 1}': [ np.average( tmp[3 * i:3 * i + 3] ) ] } )  # mean(step1~3) for Z4 and ZS163_
                                logger.info( f'metadata: {metadata}' )

                            except ValueError:
                                logger.info( f'data in {key} is not calculable' )
                        logger.info(f'key-{key}; metadata-{metadata}')        
            except (ValueError, KeyError):
                logger.info(f'Cannot convert to value for {key}') 
                logger.info(f'metadata-{metadata}') 
            if '@step_no' in metadata:
                metadata['step'] = metadata.pop('@step_no') # change key name     
        elif re.search('ZTM23_|ZTM24_', key, re.IGNORECASE): # ZTM; resources
            logger.info(f'key-{key}; metadata-{metadata} in solo class')
            attr_name, attr_value, attr_resource = 'io_point_name', 'io_point_value', 'resource'
            if isinstance(metadata[attr_name][0], list):
                for i in range(len(metadata[attr_name][0] ) ) :
                    metadata.update( { f'{metadata[attr_name][0][i]}_{metadata[attr_resource][0][i]}': [ float(metadata[attr_value][0][i]) if metadata[attr_value][0][i]!='None' else 'NULL' ] } )   
            logger.info(f'metadata-{metadata}')
        else:
            super()._augm_data(key, metadata)
        
class UserDefinedError(Exception):
    '''
    'ERROR-00' # [=] AUTOPM CONTENT IS EMPTY
    '''
    pass

