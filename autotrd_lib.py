__author__ = 'A. Tran'
__date__ = '2024-05-31'
__copyright__ = 'Copyright Lam Research 2024'
__status__ = 'Beta'
__authors__ = ["Moulds, Joshua", "Tran, Anh", "Fiendell, Cory",
               "Srinivasan, Karthik",'Li, Pei-Syuan']
# Modified for filename_changes

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from autotrd_mos_convert import *
from pyarrow import hdfs
import tkinter as tk
import ipywidgets as widgets
import tkinter.font
import configparser
import os
import sys
import re 
import json
import xlsxwriter
import datetime
import shutil
import zipfile
import io
import itertools
import logging
import functools
import ast
import copy
import sqlite3
import textwrap
import subprocess
import yaml
import seaborn as sns
from IPython.display import display, clear_output, FileLink
import multiprocessing as mp
import pickle
import statistics # PS

import sqlite # as sqlite #
import lamsqlite # as lamsqlite #
import ipywidgets 
import ast # add for AFV 

try:
    import rps_solr
    import autopm
except ModuleNotFoundError:
    pass 
# import lamsqlite # PS
# set up logging to file
logging.basicConfig(
     # filename='autotrd.log', # PS EIDH
     level=logging.DEBUG,  # PS DEBUG INFO
     format= '%(asctime)s, %(name)s, %(levelname)s, %(funcName)s, %(lineno)d, %(message)s',
     datefmt='%Y-%m-%d %H:%M:%S',
 )
logger = logging.getLogger(__name__)
plt.set_loglevel('WARNING') # PS close INFO from matplotlib.category

class Lam2300():
    '''
    This class generates an trdObj, which contains all information required
    for generating a technical report document for Kioyo F tools
    Parameter:
    zipfile (str): filename of the Lam2300 data
    specfile (str): filename of the system spec (xlsx)
    kwargs 
    - 'configfile' (str/list) = 'GenericModule' or ['GenericModule', 'KiyoF']
    
    How to use this class:
    After creating a new instance and specifying the zipfile and specfile, 
    run self.create_trd() to generate a trdObj

    ####### USER-DEFINED INFORMATION #######
    ZIP_KW (str):: user-defined string pattern used for retrieving the SSHC reports
    ZIP_DTYPE (rawtxt/cv/mos):: data type of the SSHC reports generated by the Lam2300 SW
    MOS_INFO (str):: name of the element in a dictionary generate by [convert_to_json(file_data)]
    SPEC_ATTR (list of strs):: (optional) attribute in the rawdata to be compared against the SSHC specs
    SPEC_INFO (list of strs):: attributes in the rawdata to add useful information for the report, e.g., gasline
    SPEC_AO :: a set-point attribute in the rawdata 
    ATTR_GROUPBY (boolean):: True[=] if PMs needs to be grouped by the gasbox config
    ZIP_ATTR (list of strs):: (optional) user-defined attributes for the SSHC rawdata

    Useful commands when working  with multiindex dataframe
    1. TO FIND THE IDX LEVEL GIVEN THE INDEX NAME
    > df.index.names.index(<level_name>)
    2. TO GET ALL INDEX LEVEL NAMES
    > df.index.names
    3. TO GET THE NUMBER OF MULTIINDEX LEVELS
    > df.index.nlevels
    4. TO GET ALL ROWS OF WHICH A iLEVEL VALUE = <STR>
    > df.query(f"{<level_name>} == '{<level_value>}'")
    '''
    ####### USER-DEFINED INFORMATION #######
    ZIP_KW = 'kw' #(str):: user-defined string pattern used for retrieving the SSHC reports 
    ZIP_DTYPE ='dtype' #(rawtxt/cv/mos):: data type of the SSHC reports generated by the Lam2300 SW
    MOS_INFO = 'mos_info' #(str):: name of the element in a dictionary generate by [convert_to_json(file_data)]
    SPEC_ATTR = 'spec_attr' #(list of strs):: (optional) attribute in the rawdata to be compared against the SSHC specs
    SPEC_INFO = 'spec_info' #(list of strs):: attributes in the rawdata to add useful information for the report, e.g., gasline
    SPEC_AO = 'spec_ao' #(str):: a set-point attribute in the rawdata 
    SPEC_AUGM = 'spec_augm'
    ATTR_GROUPBY = 'groupby' #(boolean):: True[=] if PMs needs to be grouped by the gasbox config
    ZIP_ATTR = 'attr' #(list of strs):: (optional) user-defined attributes for the SSHC rawdata

    ####### SPEC.xlsx #######
    SPEC_CONFIG = 'ModInfo_Concat' #(str):: string pattern used for retrieving the platform configuration 
    SPEC_NAME = 'SSHC' #(str):: string pattern representing a particular SSHC
    SPEC_GASIDX = 'GasLine' #(str):: gas line index (e.g., Gas01)
    ATTR_AO = 'Setpoints' #(mixed dtype):: pattern used for joining the measured data and the SSHC specs 
    SPEC_APPLLICATION = 'Application' # PS
    SPEC_MIN = 'LCL' #(float):: lower control limit 
    SPEC_MAX = 'UCL' #(float):: upper control limit
    
    ####### INTERNAL DATA #######
    ZIP_FNAME = 'zipfile' #(list of strs):: list of the filenames, each of which corresponds to a SSHC report
    ZIP_PMIDX = 'pmidx' #(int):: index used for extracting the module (pmalias) from the filename
    ZIP_DATA = 'data' #(dict):: raw data; {'pmalias':df}
    EXCEL_DATA = 'excel_data' #(df):: dataframe generated by concatenating all raw data  
    EXCEL_RPT = 'excel_doc' #(dict):: dataframe (comparison between the measured data and SSHC spec); {'SPEC_ATTR':df}
    EXCEL_ERR = 'excel_err' #(dict):: dataframe (duplicated data found when generating EXCEL_RPT); {'SPEC_ATTR':err_df}
    TOCSV_DATA = 'tocsv_data' #(dict):: human-readable dataframe; {'SPEC_ATTR':df}
    EXCEL_DATA_A3 = 'excel_data_A3' #(df):: dataframe generated by concatenating all raw data # PS

    ####### SHARED KEYWORDS #######
    ATTR_TIME = 'Timestamp.Timestamp' #date & time on which the data was taken in EXCEL_DATA 
    ATTR_MODULE = 'module' # The ingestion class used to load the backup files
    ATTR_PMALIAS = 'pmalias' #alias of a module  
    ATTR_AI = 'value' #measured data corresponding to an attr of interest in EXCEL_DATA  
    ATTR_NUMCONFIG = 'num_config' #numeric data corresponding to the number of configurations found in a system option
    ATTR_MATCH ='Matching' #boolean data indicating if a system config is the same in all compared modules
    ATTR_ISNULL ='is_null' #boolean data indicating if a system config is the same in all compared modules
    ATTR_OOS = 'is_oos' #boolean data indicating if the measured data is within spec in EXCEL_RPT
    ATTR_FNAME = 'fname' #filename of a SSHC report in trdObj[SSHC_alias]
    ATTR_KEY = 'key' #SSHC in trdObj[SSHC_alias]
    ATTR_SVN = 'SoftwareVersion'
    ATTR_FIG = 'fig'
    ATTR_PM_TRD = ['GasBox_Config', 'PM_ConfigOpt', 'PM_CV']
    ATTR_GASBOX = 'gasbox_idx'
    ATTR_PMINFO = 'module_info' 
    INGEST_FUNCT = 'funct1'
    ANALYZE_FUNCT= 'funct2'
    HDFS_PATH = '/tmp'
    ATTR_COMMENT= 'Comment'

    '''
    ATTRIBUTE:
    configfile
    zipfile
    toolid
    module
    TEMPORARY ATTRIBUTE:
    __pmidx
    __filename
    __subsys
    df
    '''

    def __init__(self, zipfile, golden_pm=None, **kwargs):
        self.zipfile = zipfile
        self.golden_pm = golden_pm
        self.hdfs = kwargs['hdfs'] if 'hdfs' in kwargs else None
        configfile = kwargs['configfile'] if 'configfile' in kwargs else []
        self.ErrorCode = 0   

    def to_datetime(self, dt, 
            dtfmt='%B %d, %Y %I:%M:%S %p', 
            regex='^(.*?(pm|am))'
        ):
        reObj = re.compile(regex, re.IGNORECASE)
        dt = datetime.datetime.strptime(reObj.search(dt).group(1), dtfmt)
        dt = [int(_elem) for _elem in dt.strftime('%Y,%m,%d,%H,%M,%S').split(',')]
        return dt

    def query_status(self):
        if self.ErrorCode == 0:
            logger.info(f'Complete ingesting {self.zipfile})')
        elif self.ErrorCode == 1:
            logger.error(f'{self.zipfile} does not exist')
        elif self.ErrorCode == 2:
            logger.error(f'autotrd_config.yaml does not exist')

    def load_autotrd_config(self, config='autotrd_config.yaml', inplace=True, **kwargs):
        '''
        load_autotrd_config reads the init file (.ini) of the analysis
        The init file contains all user-defined information
        ZIP_KW (str):: user-defined string pattern used for retrieving the SSHC reports
        ZIP_DTYPE (rawtxt/cv/mos):: data type of the SSHC reports generated by the Lam2300 SW
        MOS_INFO (str):: name of the element in a dictionary generate by [convert_to_json(file_data)]
        SPEC_ATTR (list of strs):: (optional) attribute in the rawdata to be compared against the SSHC specs
        SPEC_INFO (list of strs):: attributes in the rawdata to add useful information for the report, e.g., gasline
        SPEC_AO :: a set-point attribute in the rawdata 
        ATTR_GROUPBY (boolean):: True[=] if PMs needs to be grouped by the gasbox config
        ZIP_ATTR (list of strs):: (optional) user-defined attributes for the SSHC rawdata
        '''
        if self.ErrorCode: 
            self.query_status()
            return
        try:
            with open(config) as fObj:
                config_lib = yaml.load(fObj, Loader=yaml.FullLoader)
                
            if inplace:
                self.config_lib = config_lib
            else:
                return config_lib
        except FileNotFoundError as err:
            logger.info(f'err-{err}')
            self.ErrorCode = 2

    def open_zipfile(self):
        if self.ErrorCode: 
            self.query_status()
            return
        try:
            with open(self.zipfile, 'rb') as fObj:
                self.zipObj = zipfile.ZipFile(io.BytesIO(fObj.read()))
        except FileNotFoundError as err:
            if self.hdfs:
                with self.hdfs.open(self.zipfile, 'rb') as fObj:
                    self.zipObj = zipfile.ZipFile(io.BytesIO(fObj.read()))
            else:
                self.ErrorCode = 1
        self.augm_zipfile()

    def augm_zipfile(self, filename=''):
        if filename:
            self.zipObj = zipfile.ZipFile(io.BytesIO(self.zipObj.read(filename)), 'r') 
        filenames = [filename for filename in self.zipObj.namelist()
            if re.search(r'^(all|config).*?\.zip$', filename, re.IGNORECASE)]
        try:
            filename = filenames[0]
            self.augm_zipfile(filename)
        except IndexError:
            logger.info(f"Parsing {self.zipfile}")   


    def get_autotrd_config(self, configfile=None, config_lib={}):
        '''
        get_autotrd_config returns the config (dictonary) and the tool type
        Parameter:
        configfile (str/None): chamber type
        config_lib (dict): a dictionary containing the whole yaml file
        '''
        config = {}
        tool_models = ['GenericModule']
        if not config_lib:
            config_lib = self.config_lib
        KeyErrorFlag = True
        if configfile:
            if  configfile in config_lib:
                KeyErrorFlag = False
                tool_models.append(configfile)
            else:
                #select the longest matched name
                keys = {}
                for key in config_lib.keys():
                    mo = re.search(f'^{key}', configfile, re.IGNORECASE)
                    if mo: 
                        keys.update({key:len(key)})
                if keys:
                    key = max(keys, key=keys.get) 
                    tool_models.append(key)
                    KeyErrorFlag = False
                    configfile = key  
        if KeyErrorFlag:
            if configfile:
                logger.debug(f'{configfile} is not defined in config_lib')
            configfile = 'GenericModule'
        for tool_model in tool_models:
            config.update(copy.deepcopy(config_lib[tool_model]))
        keys = []
        for subsys, data in config.items():
            mo = re.search(r'ef_(\w+)', subsys, re.IGNORECASE)
            if mo:
                config[mo.group(1)].update(**data)
                data = {}
            if data:
                if self.SPEC_INFO in data:
                    if isinstance(data[self.SPEC_INFO], list):
                        data[self.SPEC_INFO] ={_elem:None for _elem in data[self.SPEC_INFO]}
            else:
                keys.append(subsys)

        for key in keys:
            del config[key] 
        return config, configfile
    def get_filename(self, regex_pattern=None):
        '''
        get_filename finds all filenames of each
        class based on the user-defined string pattern 
        Parameter:
        regex_pattern (str/None): an additional required pattern
        '''
        empty_keys = []
        for key, data in self.trd.items():
            pattern = f'({data[self.ZIP_KW]})\\.{data[self.ZIP_DTYPE]}$'
            if regex_pattern:
                pattern = f'^(?=.*/{regex_pattern}/)(?=.*{pattern}).*$'
                if self.ZIP_FNAME in data: 
                    continue
            reObj = re.compile(pattern, re.IGNORECASE)
            data[self.ZIP_FNAME] = [_elem for _elem in self.zipObj.namelist()\
                            if reObj.search(_elem)]
            N_paths = len(data[self.ZIP_FNAME])
            if N_paths==0: 
                empty_keys.append(key)
                pmidx = np.NaN
            elif N_paths==1: 
                path = os.path.normpath(data[self.ZIP_FNAME][0])
                path = [_elem.lower() for _elem in path.split(os.sep)]
                pmidx =  path.index("lam") + 2
            else:
                path = os.path.commonpath(data[self.ZIP_FNAME])
                path = os.path.normpath(path)
                path = path.split(os.sep)
                if regex_pattern:
                    pmidx = path.index(regex_pattern)
                else:
                    pmidx = len(path)

            data[self.ZIP_PMIDX] = pmidx
            if regex_pattern: 
                continue
            tmp = {}
            reobj = re.compile(r'^(pm\d+$|((?!.*pm).)*)$', re.IGNORECASE)
            for filename in data[self.ZIP_FNAME]:
                pmalias = filename.split('/')[pmidx]
                mo = reobj.search(pmalias)
                if mo:
                    try:
                        tmp[pmalias].append(filename)
                    except:
                        tmp[pmalias] = [filename]
            data[self.ZIP_FNAME] = tmp
        for key in empty_keys:
            del self.trd[key]
        self.augm_filename()

    def augm_filename(self):
        '''
        augm_filename removes legacy files
        This function is written to migate two common issues,
        1. When a user creates a copy of an existing file 
        2. When a user places a reference file in the folder
        '''
        for key, data in self.trd.items():
            # filter out any files that contains any of the following patterns
            pattern = 'copy|old|original|org_factory|org' # PS
            reObj = re.compile(f'^((?!.*({pattern})).)*$', re.IGNORECASE)
            try:
                augm_data = {}
                for pmalias, zipfiles in data[self.ZIP_FNAME].items():
                    zipfiles = [_elem for _elem in zipfiles\
                        if reObj.search(_elem)]
                    if key == 'PM_CV':
                        zipfiles = [_elem for _elem in zipfiles\
                            if re.search(pmalias+'.cv', os.path.basename(_elem), re.IGNORECASE)] # PS
                    augm_data.update({pmalias:zipfiles})
                data[self.ZIP_FNAME] = augm_data
            except AttributeError:
                data[self.ZIP_FNAME] = [_elem for _elem in  data[self.ZIP_FNAME]\
                        if reObj.search(_elem)]

    def assign_ingest_funct(self):
        '''
        assign_ingest_funct assigns an ingestion function to
        each class based on the dtype and filename 
        '''
        for key, data in self.trd.items():
            # if data[self.ZIP_DTYPE] == 'mos': 
            #     ingest_funct = self.ingest_mosfile
            # elif data[self.ZIP_DTYPE] == 'cv': 
            #     ingest_funct = self.ingest_mosfile
            if data[self.ZIP_DTYPE] in ['mos', 'cv', 'dat']: 
                ingest_funct = self.ingest_mosfile                
            elif data[self.ZIP_DTYPE] == 'txt':
                if re.search('configopt', key, re.IGNORECASE): 
                    ingest_funct = self.ingest_configopt
                elif re.search('hiddenopt', key, re.IGNORECASE): 
                    ingest_funct = self.ingest_hiddenopt
                elif re.search('toolid', key, re.IGNORECASE):  
                    ingest_funct = self.ingest_toolid
                elif re.search('gasbox_config', key, re.IGNORECASE): # PS
                    ingest_funct = self.ingest_gasbox
                elif re.search('gasbox_interlock', key, re.IGNORECASE): # PS
                    ingest_funct = self.ingest_gasbox_interlock                    
                elif re.search('TxtBasedPTSDB', key, re.IGNORECASE): 
                    ingest_funct = self.ingest_ptsdb
                elif re.search('comp2_history', key, re.IGNORECASE): # PS
                    ingest_funct = self.ingest_comp2_history                
                else: ingest_funct = self.ingest_generic_config
            data[self.INGEST_FUNCT] = ingest_funct 

    def append_trd_obj(self, subsys):
        content = {
            self.ZIP_FNAME:[],
            self.ZIP_DATA:{}
        }
        try:
            self.trd[subsys].update(content)
        except KeyError:
            paths = self._recursive_lookup(self.config_lib, [subsys])
            if paths:
                path = paths[0]
                path.append(subsys)
                data = self._extract_nested_data(self.config_lib, path)
                content.update(data)
            self.trd[subsys] = content

    def augm_trd_obj(self, key, data):
        '''
        augm_trd_obj adds a complete SSHC into the trd object

        '''
        self.append_trd_obj(key)
        if isinstance(data, dict):
            self.trd[key][self.ZIP_DATA] = data
        else:
            self.trd[key][self.ZIP_DATA] = {
                'user-defined':data
            }
            self.module['user-defined'] = {
                self.ATTR_KEY: [],
                self.ATTR_MODULE: 'GenericModule'
            }
        trd = {key:self.trd[key]}
        del self.trd[key]
        trd.update(** self.trd)
        self.trd = trd

    def create_trd_obj(self):
        '''
        create_trd_obj creates a trd object, that contains all information
        about this analysis. Each platform should has its own trd object.
        '''
        if self.ErrorCode: 
            self.query_status()
            return
        self.trd, _ = self.get_autotrd_config()
        logger.debug(f"Creating a trd object") 
        for key, data in self.trd.items():
            self.append_trd_obj(key)
        self.get_filename()
        self.assign_ingest_funct()
        self.run_decentralize()
    def extract_zip(self, filename = None):
        '''
        extract_zip extracts a file from a zipObject based on the specified filename.
        This function creates two attributes, i.e., pmalias, & filename
        Parameter:
        fname (str):: an absolute path to the file (in the zip tree)
        pmidx (int):: index used for extracting the module (pmalias) from the filename
        '''
        if filename:
            source = self.zipObj.open(filename)
            target = open(os.path.basename(filename), "wb")
            with source, target:
                shutil.copyfileobj(source, target)
        else:
            path, basename = os.path.split(self.__filename)
            try:
                pmalias = re.split('/',path)[self.__pmidx]
            except IndexError:
                pmalias = re.split('/',path)[-1]
                self.aliases.append(pmalias)
            self.__pmalias = pmalias

    def ingest_ptsdb(self, encoding="utf-8", **kwargs):
        self.extract_zip()
        db = {}
        isinstalled_attr = 'Installed?'
        pn_attr = 'Part Number'
        sn_attr = 'Serial Number'
        rev_attr = 'Revision' 
        self.df = pd.DataFrame()
        with io.TextIOWrapper(self.zipObj.open(self.__filename), encoding=encoding) as fObj:
            rawtxt = fObj.readlines()

            self.dummy =rawtxt
            ids = [idx for idx, item in enumerate(rawtxt) 
                 if re.search(r'^(\w+)\d*$', item.strip(), re.IGNORECASE)]
            for start_id, end_id in zip(ids, ids[1:]):
                pmalias = rawtxt[start_id].strip()
                start_id+=1
                df = pd.DataFrame([item.split('\t') for item in rawtxt[start_id:end_id]])
                df.columns = df.iloc[0]
                
                if 'Installed Date Time' in df:
                    dt_attr = 'Installed Date Time'
                    args = {
                        'dtfmt':'%m/%d/%Y %H:%M:%S',
                        'regex':'^(.*)$'
                    }
                elif 'Time' in df:
                    dt_attr = 'Time'
                    args = {
                        'dtfmt':'%b %d, %y %H:%M:%S',
                        'regex':'^(.*)$'
                    }
                else:
                    return 
                attrs = [pn_attr, rev_attr, isinstalled_attr, sn_attr, dt_attr]
                df = (df.drop(index=0)
                  .replace('',np.NaN)
                  .replace({isinstalled_attr:{'true':True,'false':False}})
                  .drop(columns = [attr for attr in df.columns if attr not in attrs])
                  .T
                )  
                df = df.loc[:, df.loc[[pn_attr, rev_attr, isinstalled_attr, dt_attr]].notnull().all(axis=0)]
                if not df.empty: 

                    func = functools.partial(self.to_datetime,**args)
                    df.loc[self.ATTR_TIME] = df.loc[dt_attr].apply(func)
                    logger.info(f' 2: { df.loc[self.ATTR_TIME]  } ')     
                    df.columns = [f'value_{_elem:03d}' for _elem in range(df.shape[1])]
                    db[pmalias] = df
            self.df = db
            

    def ingest_generic_config(self, encoding="utf-8", **kwargs):
        self.extract_zip()
        reObj_parser  = re.compile(r'''
            (\w+):              # GET KEY WORD
            \W*([^\s'\(\)]+)\W* # GET THE FIRST OPTION 
            ''',re.IGNORECASE|re.VERBOSE)   
        with io.TextIOWrapper(self.zipObj.open(self.__filename), encoding=encoding) as fObj:
            rawtxt = fObj.readlines()
            # drop all elements that start with a white space character
            # rawtxt=[re.sub(r'\s#', ',',_elem) for _elem in rawtxt if re.search(r'\w',_elem[0])]
            rawtxt=[_elem for _elem in rawtxt if re.search(r'\w',_elem[0])]
            rawtxt = [reObj_parser.findall(_elem) for _elem in rawtxt]
            rawtxt = {key:value for key,value in list(itertools.chain.from_iterable(rawtxt))
                if re.search('[a-zA-Z]', key)
            }
            self.df = (pd.DataFrame.from_dict(rawtxt, orient='index'))
        
    def ingest_toolid(self, attr, **kwargs):
        '''
        ingest_toolid ingests the toolid info.
        This function creates two attributes, i.e., df, & toolid
        Parameter:
        attr (list of strs):: attributes of data 
        fname (str):: an absolute path to the file
        pmidx (int):: an index that defines the position of the module in the path
        '''
        self.ingest_generic_config()
        self.df.index.name = self.ATTR_PMALIAS
        self.df.columns = attr
        self.toolid = self.df.loc['toolID'][0]
        logger.info(f'Begin to ingest SSHC data files of {self.toolid} platform')
    def ingest_gasbox(self, attr, **kwargs):
        '''
        ingest_gasbox ingests the gasbox info
        Parameter:
        attr (list of strs):: attributes of data 
        fname (str):: an absolute path to the file
        pmidx (int):: an index that defines the position of the module in the path
        '''
        reObj_parser  = re.compile(r'''
            (\w*[(Tuning)|(gas)]\w*):# GET GAS ALIAS
            \W*MaxFlow:\W*([\d.]+)   # GET THE MFC CAPACITY 
            \W*Name:\W*(\w+)         # GET GAS IDENTITY
            ''',re.IGNORECASE|re.VERBOSE)
        self.extract_zip()
        with io.TextIOWrapper(self.zipObj.open(self.__filename), encoding="utf-8") as fObj:
            rawtxt = fObj.read()  
            self.df = pd.DataFrame(reObj_parser.findall(rawtxt))
            self.df.columns = attr  
            self.df.set_index(['Gas_Value', 'Gas'], inplace=True)
            self.df.loc[:,'MaxFlow'] = self.df.loc[:,'MaxFlow'].astype(float)
        #logger.info(f' self.df-ingest_gasbox-{self.df}')
        
    def ingest_gasbox_interlock (self, **kwargs):
        '''
        ingest_gasbox_interlock  ingests the gasbox_interlock info
         '''
        self.extract_zip()
        reObj_parser  = re.compile(r"('*\w+.\d*'*): #\((.+?)\)") #  r'(\w+.\d*): #\((.+?)\)'
        with io.TextIOWrapper(self.zipObj.open(self.__filename), encoding="utf-8") as fObj:
            rawtxt = fObj.read()
            self.df = pd.DataFrame(reObj_parser.findall(rawtxt))
            self.df.columns = [ 'GasName', 'InterlockedGases' ]
        if not self.df.empty :
            if self.df.loc[0, 'InterlockedGases'].startswith("'"): # Metal NXP
                self.df['InterlockedGases'] = self.df['InterlockedGases'].apply( lambda x: x.strip().replace( 'nil', '').replace(" ", "").replace("''", ",").replace(",'", "").replace("'", "") )
            elif self.df.loc[0, 'InterlockedGases'].startswith("#"): # Kiyo 
                self.df['InterlockedGases'] = self.df['InterlockedGases'].apply( lambda x: x.strip().replace( 'nil', '').replace( ' #', ',' ).replace( '#', '' ).replace(",''", '').strip() )
                
            self.df['InterlockedGases'] = self.df['InterlockedGases'].str.split(',') # PS TBD
            self.df = self.df.apply(pd.Series.explode)  
            
    def ingest_hiddenopt(self,  attr, **kwargs):
        '''
        ingest_hiddenopt ingests the hidden-option info
        Parameter:
        attr (list of strs):: attributes of data 
        fname (str):: an absolute path to the file
        pmidx (int):: an index that defines the position of the module in the path
        '''   
        self.extract_zip()
        self.df = pd.read_csv(self.zipObj.open(self.__filename),header=None, names=attr)
        self.df['value_000'] = 1
        self.df.set_index(attr, inplace=True, drop=True)
    def ingest_configopt(self, **kwargs):
        '''
        ingest_configopt ingests the config-option info
        Parameter:
        attr (list of strs):: attributes of data 
        fname (str):: an absolute path to the file
        pmidx (int):: an index that defines the position of the module in the path
        ''' 
        self.ingest_generic_config(encoding="ISO-8859-1")
        self.df.columns = [f'value_{_elem:03d}' for _elem in range(self.df.shape[1])]
        self.df.drop(index='ModuleLabel', errors='ignore',inplace=True)

    def ingest_comp2_history(self, **kwargs):  # PS
        '''
        ingest AcceptedGasCompHistory.txt for Accepted Timestamp, Max Flow, Setpoint, Cali Setpoint, MFC Error and Setpoint Error %
        '''
        self.extract_zip()        
        with io.TextIOWrapper(self.zipObj.open(self.__filename), encoding="utf-8") as fObj:
            rawtxt = fObj.read()    
        rawtxt = rawtxt.strip().split('\n')
        tmp = dict()
        for _elem in iter(rawtxt): 
            if re.search( r'Accepted', str(_elem) ):
                key = _elem.split('\t')[0]
                value = ' '.join( i for i in _elem.split('\t')[1:] )  
                tmp[key] = value
            if re.search( r'Max Flow|Setpoint|MFC Error', str(_elem) ): 
                key = _elem.split('\t')[0]
                value =  [ float(i) for i in _elem.split('\t')[1:] if i != '' and i != 'N/A' ]   
                if len(value) ==1:
                    value = value[0]
                tmp[key] = value
        self.df = pd.DataFrame.from_dict(tmp, orient = 'index')
        self.df.columns =  [f'value_{_elem:03d}' for _elem in range(self.df.shape[1])]
        parser = re.compile( r'\W*(\w+)AcceptedGasCompHistory')
        self.df.loc['GasLine'] = parser.findall(self.__filename)[0]
        self.df.rename( { 'Accepted Timestamp': self.ATTR_TIME, 'Max Flow': 'MaxFlow', 'MFC Error %': 'MFCError', 
                         'Setpoint Error %': 'SetpointError', 'Cali Setpoint': 'CaliSetpoint', 'Setpoint': 'Setpoints' }, inplace =True) 
        try:
            if len(self.df.loc['SetpointError'].values[0] ) == 0 : # PS blank SetpointError for some gas
                self.df.loc['SetpointError'] = [ [np.NaN]*len( self.df.loc['MFCError'].values[0] ) ]
        except:
            pass
        args = {
               'dtfmt':'%m/%d/%Y %H:%M:%S',
               'regex':'^(.*)$'
                  }
        func = functools.partial(self.to_datetime, **args)
        self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func)  
        #logger.debug( f'ingest comp hist- {self.df} ' ) # PS

    def ingest_mosfile(self, mos_info=None, **kwargs):
        '''
        ingest_mosfile ingests the mos info
        Parameter:
        fname (str):: an absolute path to the file
        pmidx (int):: an index that defines the position of the module in the path
        mos_info (str):: the name of the database
        '''   
        file_data = io.BytesIO(self.zipObj.read(self.__filename))
        data=convert_to_json(file_data)
        #logger.info( f'{self.__filename}' ) # PS
        if isinstance(mos_info,str):
           data = data[mos_info]
        elif isinstance(mos_info,list):
           data = self._extract_nested_data(data, mos_info)
        self.df = pd.json_normalize(data)
        attr = [_elem for _elem in self.df.columns if re.search('[^_]',_elem[0])]
        self.df = self.df[attr].transpose()
        self.df.columns = [f'value_{_elem:03d}' for _elem in range(self.df.shape[1])]
        if re.search('airlock|port|coolstation', self.__subsys, re.IGNORECASE):
            self.df.index = [re.sub(r'(airlock|port|^\w{1,6})(\d+)', r'\1#', _elem,flags=re.IGNORECASE)
                            for _elem in self.df.index]
        elif re.search('AFV_Ten_PT', self.__subsys, re.IGNORECASE):
            # logger.info(f'df-{self.df}')  #　PS
            # logger.info(f'index {self.df.index}')
            if self.ATTR_TIME not in self.df.index:
                attr_dt = []
                for value in self.df:
                    date_dt = self.df[value].loc['Date']
                    date_obj = datetime.datetime.strptime(date_dt, '%m/%d/%Y')
                    time_dt = self.df[value]['EndTime']
                    time_obj = datetime.datetime.strptime(time_dt, '%H:%M:%S')
                    temp_dt = [date_obj.year, date_obj.month, date_obj.day, time_obj.hour, time_obj.minute, time_obj.second]
                    attr_dt.append(temp_dt)
                self.df.loc[self.ATTR_TIME] = attr_dt
        self.extract_zip()
        try:
            self.run_data_augm() 
        except Exception as err:
            logger.warning(f'Encounter Exception in {self.__filename}, {err}')
            self.df = pd.DataFrame()
        logger.info(f'df-{self.df}')  #　PS

    def run_data_augm(self):
        '''
        run_data_augm does nothing in Lam2300 class
        '''
        def polyfit(x, y, degree): # Partial Pressure 
            coeffs = np.polyfit(x, y, degree)
            p = np.poly1d(coeffs)
            # fit values, and mean
            yhat = p(x)                         # or [p(z) for z in x]
            ybar = np.sum(y)/len(y)          # or sum(y)/len(y)
            ssreg = np.sum((yhat-ybar)**2)   # or sum([ (yihat - ybar)**2 for yihat in yhat])
            sstot = np.sum((y - ybar)**2)    # or sum([ (yi - ybar)**2 for yi in y])
            result = ssreg / sstot
            return round(result, 4)        
        
        if re.search('TCCT_NoPlasmaTest', self.__subsys, re.IGNORECASE):
            attr_ao, attr_y = 'TCCTSetpoints','Setpoints'
            self.df.loc[attr_y] = [np.round(_elem,decimals=1) for _elem in self.df.loc[attr_ao]]
            attr_ai, attr_y = 'RSquareds','Rsquared'
            self.df.loc[attr_y] = self.df.loc[attr_ai]  
            # PS_A3 extract slopes
            spec_attr, attr_y =  'Slopes','Setpoints'
            tmp = self.df.loc[attr_y].values[0]
            len_attr = len(tmp)
            len_temp = len( set( tmp ) )
            if len_attr != len_temp : # select the first four setpoints for 5 duplicate setpoints
                len_attr = len_temp
            for i in range( len_attr ):
                attr_name = 'TCCT_slope'+ str(i+1) 
                self.df.loc[ attr_name ] = self.df.loc[spec_attr].values[0][i] 
            #logger.info(f'self.df-{self.df}')
            #logger.info(f'tmp-{tmp}')
            for i in range( len_attr ):
                attr_y1, attr_y2 = 'C4MinPositions', 'C4MaxPositions'
                attr_name = f'TCCT_SP{ tmp[i] }_C4'
                self.df.loc[ attr_name ] = np.mean( [ self.df.loc[attr_y1].values[0][i], self.df.loc[attr_y2].values[0][i] ] )

                attr_y1, attr_y2 = 'C5MinPositions', 'C5MaxPositions'
                attr_name = f'TCCT_SP{ tmp[i] }_C5'
                self.df.loc[ attr_name ] = np.mean( [ self.df.loc[attr_y1].values[0][i], self.df.loc[attr_y2].values[0][i] ] )
            #logger.info( f'{self.__subsys};self.df-{self.df}' )
                
                #attr_name = 'MeterReadings_6_BiasRF'  
                #if 6 in self.df.loc[attr_ao].values[0]:
                #    loc =  self.df.loc[attr_ao].values[0].index(6)
                #    self.df.loc[ attr_name ] = self.df.loc[attr_ai].values[0][ loc ] 
                #else: 
                #    self.df.loc[ attr_name ] = 'NULL'    

        elif re.search('AbsoluteFlowVerifier', self.__subsys, re.IGNORECASE): # PS add AFV
            #logger.info( f'{self.__subsys};self.df-{self.df}' )
            #self.df.to_csv('AbsoluteFlowVerifier.csv')
            # 抓所有的GasXX.OnePointOrificeCalError, 取絕對值後, 判斷都小於 1 (單位%) => Pass
            tmp = self.df.filter(regex='Gas\d*.OnePointOrificeCalError', axis=0)
            #logger.info( f"test-{ tmp.loc['Gas10.OnePointOrificeCalError'].values[0]  }" )
            #logger.info( f"max-{ tmp['value_000'].explode().abs().max() }" )
            self.df = self.df.filter(regex='Gas\d*.OnePointOrificeCal(Error|Setpoint|Actual)', axis=0).copy()
            self.df.loc['Pass_AllGas_AFV '] = ['Pass' if tmp['value_000'].explode().abs().max() < 1 else 'Failed' ]
            #logger.info( f'self.df-{self.df}' )            

        elif re.search('One_PT_Cal', self.__subsys, re.IGNORECASE): # PS 
            attr_y = 'RequestedFlow'
            self.df.loc[attr_y] = [ int(_elem) for _elem in self.df.loc[attr_y] ] 

        elif re.search('Ten_PT_Cal', self.__subsys, re.IGNORECASE): # PS
            attr_y = ['error', 'requestedFlow', 'actualFlow'] 
            attr_limit, spec_attr = 'maxFlow', '%MaxFlow'
            data = pd.DataFrame(self.df.loc['data','value_000']).dropna()
            
            for attr in attr_y:
                if attr ==  'requestedFlow':
                    temp = [ int(_elem) for _elem in data[attr] ]
                    self.df.loc[spec_attr] =  [ [ int( _elem/self.df.loc[attr_limit]*100 ) for _elem in temp ] ]
                    self.df.loc[attr] = [ temp ]
                else:
                    self.df.loc[attr] = [ data[attr].values.tolist() ]    
            self.df.rename(index={'gasName':'GasName', 'gasLine':'GasLine', 'error':'Error', 'requestedFlow':'RequestedFlow', 'actualFlow':'ActualFlow'},inplace =True)                
            # PS_A3 extract _10, _50, _90, _100
            #target = [10, 50, 90, 100]
            target = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # For F20 Kiyo FX
            tmp = self.df.loc[spec_attr].values[0]
            for i in range( len( tmp ) ):
                if tmp[i] in target:
                    attr_name = '_'+ str(tmp[i]) # M_
                    self.df.loc[ attr_name ] = self.df.loc['Error'].values[0][i] 
                    self.df.loc[ attr_name + '_abs'] = abs(self.df.loc['Error'].values[0][i]) # abs for MEOL
            #_MaxError     
            self.df.loc[ '_MaxError' ] = max( [ abs(_elem) for _elem in self.df.loc['Error'].values[0] ] )
            #logger.info( f'{self.__subsys};self.df-{self.df}' )
            
        elif re.search('AFV_Ten_PT|^PartialPressure|AFV\d{0,1}_Compensation', self.__subsys, re.IGNORECASE):   # PS AFV_Compensation
            '''
            %max flow = TenPointOrificeCalSetpoint/MaxFlow
            '''
            if re.search('AFV_Ten_PT|Ten_PT_Cal|ICS_Ten_PT', self.__subsys, re.IGNORECASE):
                attr_ai, attr_limit =  'TenPointOrificeCalSetpoint', 'MaxFlow'
                target = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # For F20 Kiyo FX
                for sample in self.df.columns: # PS
                    data = self.df[sample]
                    try:
                        for attr_ao in [ 'ActualFlow', 'Error' ]: # PS for Kiyo FX AFV_Ten_PT Gas01
                            data[attr_ao] = [ np.NaN if _elem == -9999 else _elem  for _elem in data[attr_ao] ] 
                    except:
                        pass
                for j in range(len(target)):
                    attr_name = '_' + str(target[j])
                    errors = []
                    errors_abs = []
                    max_errors = []
                    for col in range(len(self.df.columns)):
                        errors.append(self.df.loc['Error'].values[col][j])
                        errors_abs.append(abs(self.df.loc['Error'].values[col][j]))
                        max_errors.append(max([ abs(_elem) for _elem in self.df.loc['Error'].values[col]]))
                    self.df.loc[attr_name] = errors
                    self.df.loc[attr_name + '_abs'] = errors_abs
                    self.df.loc['_MaxError'] = max_errors                
            elif re.search('^PartialPressure', self.__subsys, re.IGNORECASE):  
                attr_ai, attr_limit =  'Flows', 'GasMaxFlow'  
                self.df.loc[attr_ai] = (self.df.loc[attr_ai]
                                   .apply(lambda _elem: [round(value) for value in _elem]))
                # Calculate R2 
                attr_name = 'R2'
                attr_y = 'Pressures'
                self.df.loc[ attr_name ] = polyfit( self.df.loc[attr_ai].values[0], self.df.loc[attr_y].values[0], 1)
                #logger.info( f'{self.__subsys};self.df-{self.df}' )  # PS
                                            
            elif re.search('AFV\d{0,1}_Compensation', self.__subsys, re.IGNORECASE): # PS AFV_Compensation
                attr_ai, attr_limit =  'AcceptedCompensationSetpoint', 'MaxFlow'
            try:
                augm_data = (self.df.apply(lambda data: [[int(_elem/data[attr_limit]*100) for _elem in data[attr_ai] ] ], axis=0))
                #logger.info( f' {self.__subsys}-df: {self.df} - {augm_data}  ' ) # PS
                augm_data.index = [r'%MaxFlow']
                self.df = pd.concat([self.df, augm_data], axis=0) 
            except KeyError as err: 
                logger.warning(f'encounter KeyError in {self.__filename}, {err} does not exist')
                
            #logger.info( f'{self.__subsys};self.df-{self.df}' ) 

        elif re.search('VoDMPartialPressure', self.__subsys, re.IGNORECASE):   # PS AFV_Compensation
            self.df.rename(index={'baseGasName':'GasName', 'baseGasLine':'GasLine', 'flows':'Flows'}, inplace =True)                                  
            #logger.info( f'{self.__subsys};self.df-{self.df}' )  # PS          
        
        elif re.search('(Linearity|RFLinearity)', self.__subsys, re.IGNORECASE):   # for Strip
            '''
                UCL is Generator max power. LCL is max percentage error
                PerSetpoint = MeterReadings/attr_ao'''
            attr_ai, attr_ao, attr_y = 'MeterReadings','Setpoints', 'PerSetpoint'
            tmp = self.df.T[[attr_ai, attr_ao]].apply(pd.Series.explode)
            self.df.loc[attr_y]=[tmp.apply(lambda _elem: (_elem[attr_ai]-_elem[attr_ao])/_elem[attr_ao]*100 
                                                    if _elem[attr_ao] else np.NaN,axis=1).values.tolist()]
            attr_ai, attr_ao, attr_y = 'ActualMax', 'Max', 'MaxPower'
            self.df.loc[attr_ai] = self.df.loc[attr_ai].fillna(0)
            self.df.loc[attr_ao] = self.df.apply(lambda _elem: round((_elem[attr_ao])/500+.49)*500 if  _elem[attr_ao]
                    # WHEN THE ActualMax IS NONE, SET THIS TO 1e-6 TO ERROR OUT THE RESULT
                    else round((_elem[attr_ai])/500+.49)*500
                )
            self.df.loc[attr_y]= self.df.apply(
                    lambda _elem: (_elem[attr_ao]- _elem[attr_ai])/_elem[attr_ao] if  _elem[attr_ai]
                    # WHEN THE ActualMax IS NONE, SET THIS TO 1 TO ERROR OUT THE RESULT
                    else 1 
                )
            # PS_A3 extract max abs PerSetpoint
            attr_ai, attr_ao, attr_y = 'MeterReadings','Setpoints', 'PerSetpoint'
            if re.search('TCP', self.__subsys, re.IGNORECASE):
                attr_name = 'MaxError_TCPRF'
                temp = [ abs(_elem) for _elem in self.df.loc[attr_y].values[0] ] # self.df.loc[attr_y].values[0]
                self.df.loc[ attr_name ] = max(temp) 
                """
                attr_name1, attr_name2, attr_y = 'OuterSlope_TCPRF', 'InnerSlope_TCPRF', 'Slope' # Solo
                if re.search('Outer', self.__subsys, re.IGNORECASE):
                    self.df.loc[ attr_name1 ] = self.df.loc[ attr_y ]
                else:
                    self.df.loc[ attr_name2 ] = self.df.loc[ attr_y ]
                """  
                setpoints_target = self.df.loc[attr_ao].values[0]
                for j in range(len(setpoints_target)):
                    attr_setpoint = 'TCP_' + str(setpoints_target[j]) + 'W'
                    per_diff = []
                    for col in range(len(self.df.columns)):
                        per_diff.append(self.df.loc[attr_y].values[col][j])
                    self.df.loc[attr_setpoint] = per_diff

            if re.search('Bias_Linearity', self.__subsys, re.IGNORECASE):
                #logger.info( f'{self.__subsys};self.df-{self.df}' ) 
                attr_name1, attr_name2 = 'MaxError_BiasRF', 'Pass_BiasRF'
                temp = [ abs(_elem) for _elem in self.df.loc[attr_y].values[0] ] # self.df.loc[attr_y].values[0]
                self.df.loc[ attr_name1 ] = max(temp)
                self.df.loc[ attr_name2 ] = 'Pass' if max(temp)<1 else 'Failed' # unit: %

                # for Kiyo FXE CMG-HMO, CMG-ME
                attr_name = 'MeterReadings_6_BiasRF'  
                if 6 in self.df.loc[attr_ao].values[0]:
                    loc =  self.df.loc[attr_ao].values[0].index(6)
                    self.df.loc[ attr_name ] = self.df.loc[attr_ai].values[0][ loc ] 
                else: 
                    self.df.loc[ attr_name ] = 'NULL'                
                # for Kiyo EXP OD4
                attr_name = 'MaxError_6_25_50_80_BiasRF'  
                target = [6, 25, 50, 80] # 
                tmp = []
                for i in target:
                    if i in self.df.loc[attr_ao].values[0]:
                        loc =  self.df.loc[attr_ao].values[0].index(i)
                        tmp.append( abs( self.df.loc[attr_y].values[0][ loc ] ) )          
                if tmp:    
                    self.df.loc[ attr_name ] = max(tmp)
                else: 
                    self.df.loc[ attr_name ] = 'NULL'
                setpoints_target = self.df.loc[attr_ao].values[0]
                for j in range(len(setpoints_target)):
                    attr_setpoint = 'Bias_' + str(setpoints_target[j]) + 'W'
                    per_diff = []
                    for col in range(len(self.df.columns)):
                        per_diff.append(self.df.loc[attr_y].values[col][j])
                    self.df.loc[attr_setpoint] = per_diff

                # logger.info( f'{self.__subsys};self.df-{self.df}' ) 

            if re.search('Bias2_Linearity', self.__subsys, re.IGNORECASE): # PS add for Kiyo GX
                #logger.info( f'{self.__subsys};self.df-{self.df}' ) 
                attr_name1, attr_name2 = 'MaxError_Bias2RF', 'Pass_Bias2RF'
                temp = [ abs(_elem) for _elem in self.df.loc[attr_y].values[0] ] # self.df.loc[attr_y].values[0]
                self.df.loc[ attr_name1 ] = max(temp)
                self.df.loc[ attr_name2 ] = 'Pass' if max(temp) <6 else 'Failed' # unit: %
                #logger.info( f'{self.__subsys};self.df-{self.df}' ) 

            if re.search('[\d]+MHzRFLinearity', self.__subsys, re.IGNORECASE): 
                attr_ao = 'Setpoints'
                parser = re.compile( r'(\d+)M')
                parser.findall(self.__subsys)[0]   
                # MaxError100_BiasRF2
                tmp1 = np.array( self.df.loc[attr_ao].values[0] )
                pos = max( np.where( tmp1 <=100 )[0] ) # Poistion for setpoint 100
                tmp2 = self.df.loc[attr_y].values[0][:pos+1]
                tmp2 = [ abs(_elem) for _elem in tmp2 ]
                attr_name = 'MaxError100_BiasRF'+ parser.findall(self.__subsys)[0]
                self.df.loc[ attr_name ] = max( tmp2)
                # MaxError_BiasRF
                temp = [ abs(_elem) for _elem in self.df.loc[attr_y].values[0] ]
                attr_name = 'MaxError_BiasRF'+ parser.findall(self.__subsys)[0]
                self.df.loc[ attr_name ] = max(temp)
                # MaxError_BiasRF_All
                if hasattr( self, 'MaxError_BiasRF_All') : # 'MaxError_BiasRF_All' in self.__dict__.keys()
                    self.MaxError_BiasRF_All = max( max(temp), self.MaxError_BiasRF_All )
                else:
                    self.MaxError_BiasRF_All = max( temp )
                if re.search('60MHzRFLinearity', self.__subsys, re.IGNORECASE): 
                    self.df.loc[ 'MaxError_BiasRF_All' ] = self.MaxError_BiasRF_All
            #logger.info( f'{self.__subsys};self.df-{self.df}' )

        elif re.search('CorvusCalibration', self.__subsys, re.IGNORECASE): 
            attr_ai, attr_ao = 'measured', 'setpoint'
            attr_y = ['calibration', 'verification']
            attr_dt = 'calibrationDate'
            decimals = 3
            tmp = [] 
            args = {
                'dtfmt':'%Y-%m-%d %H:%M:%S',
                'regex':'^(.*)$'
            }
            func = functools.partial(self.to_datetime,**args)
            attr_dt = [_elem for _elem in self.df.index
                if re.search(attr_dt, _elem, re.IGNORECASE)][0]
            self.df.loc[self.ATTR_TIME] = self.df.loc[attr_dt].apply(func)

            for attr in [attr_ai, attr_ao]:
                data = self.df.loc[[_elem for _elem in self.df.index
                                if re.search(attr, _elem, re.IGNORECASE)],]
                data.index = data.index.str.replace(attr,'',case=False)
                data.index = [[_elem for _elem in  index if len(_elem) > 0][-1] 
                             for index in data.index.str.split('.')]
                data.columns = [attr]
                tmp.append(data)
            AI, AO = tmp
            tmp = {}
            for _elem in AI.index:
                mo = re.search(r'(.*?)axis(\d+)', _elem, re.IGNORECASE)
                if mo:
                    attr, axis = mo.groups()
                    tmp[_elem] = (attr, axis)
            tmp = pd.DataFrame.from_dict(tmp,orient='index', columns=['index', 'axis'])
            AO = AO.applymap(lambda value: np.round(value,decimals=decimals))
            AI = (AI.merge(tmp, left_index=True, right_index=True)
                    .set_index('index', drop=True)
                    .explode('axis'))
            data = AI.merge(AO, how='left',left_index=True, right_index=True)
            data['Delta'] = data[attr_ai] - data[attr_ao]
            data = (data[~data.isna().apply(any, axis=1)]
                    .reset_index()
                    .rename(columns={'index':'name', attr_ao:self.ATTR_AO}))
            for y in attr_y:
                tmp = data[data.name.str.contains(y)].copy()
                tmp.name = tmp.name.apply(lambda value: re.search(f'{y}(\d+)', value, re.IGNORECASE).group(1))
                for attr in tmp:
                    self.df.loc[f'{y}.{attr}'] = [tmp[attr].values.tolist()]
            # PS_A3 extract verification.measured for 1~5
            attr_y = 'verification.measured'
            tmp_len =  len(self.df.loc[attr_y].values[0])//3
            tmp = self.df.loc[attr_y].values[0]
            for i in range(tmp_len): 
                attr_name = 'Verification'+ str(i+1)+'_range_calculated'
                self.df.loc[ attr_name ] = np.ptp( [ tmp [i*3], tmp [i*3+1], tmp [i*3+2] ] ) # range of each verification

        elif re.search('VATAutoLearn', self.__subsys, re.IGNORECASE): 
            attr_ao, attr_ai = 'Position', 'Pressure'
            attr_y = 'PressureData'
            AO, AI = list(zip(*self.df.loc[attr_y][0]))
            for attr, value in zip([attr_ao, attr_ai], [AO, AI]):
                self.df.loc[attr] = [value]
            args = {
                'dtfmt':'%m/%d/%Y %H:%M:%S',
                'regex':'^(.*)$'
            }
            func = functools.partial(self.to_datetime,**args)
            self.df.loc[self.ATTR_TIME] = ' '.join(self.df.loc[['Date', 'EndTime']].values.flatten())
            self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func)

            #logger.info( f'{self.__subsys};self.df-{self.df}' )
            # Ar_check for Kiyo FX
            attr_name1 = 'Ar_check'
            self.df.loc[ attr_name1 ] = 'Pass' if (self.df.loc['GasName'] =='Ar')[0] else 'Failed'
            # Ar_flow_check for Kiyo GX
            attr_name2 = 'Ar_flow_check'
            self.df.loc[ attr_name2 ] = 'Pass' if (self.df.loc['GasName'] =='Ar')[0] and (self.df.loc['GasFlow'] == 450)[0] else 'Failed'
            #logger.info( f'{self.__subsys};self.df2-{self.df}' )

        elif re.search('PTSDB', self.__subsys, re.IGNORECASE):
            args = {
                'dtfmt':'%m/%d/%Y %H:%M:%S',
                'regex':'^(.*)$'
            }
            dt = datetime.datetime.now().strftime('%m/%d/%Y %H:%M:%S')
            if 'installedDate' in self.df.index:
                self.df.loc['installedDate'] = self.df.loc['installedDate'].apply(
                        lambda value: dt if value=='' else value
                )
            else:
                self.df.loc['installedDate'] = dt
            func = functools.partial(self.to_datetime,**args)
            self.df.loc[self.ATTR_TIME] = self.df.loc['installedDate'].apply(func)
            attrs = ['revision', 'partNumber', 'serialNumber', 'installed','resourceName', self.ATTR_TIME]
            if all([attr in self.df.index for attr in attrs]):
                self.df = (self.df.loc[attrs,:]
                    .loc[:, self.df.loc['installed']]
                    .replace('',np.NaN)
                    .dropna(axis=1)
                    .T
                    .groupby('resourceName')
                )
                db = {}
                for pmalias, data  in self.df:
                    db[pmalias] = data.T
                self.df = db
            else:
                self.df = pd.DataFrame()

        elif re.search('TCP_Voltage_Probe', self.__subsys, re.IGNORECASE):  # PS Add for TCP_Voltage_Probe Kiyo GX
            attr_ao, attr_y = 'TCCTSetpoints','Setpoints'
            self.df.loc[attr_y] = [np.round(_elem,decimals=1) for _elem in self.df.loc[attr_ao]] 
        
        #elif re.search('Bias.*Voltage_Probe', self.__subsys, re.IGNORECASE):  # PS
            #logger.info( f'{self.__subsys};self.df-{self.df}' ) 
        
        elif re.search('Fixed.*GasOrifice', self.__subsys, re.IGNORECASE):  # PS Add FixedOrificeCurrent  => FixedTriGasOrifice
            print(self.df.to_string())
            logger.info(f'self.df-{self.df}')
            spec_attr = 'current'
            self.df.loc['TestNumber'] = [ list( range( 1, len( self.df.loc[spec_attr].values[0] )+1 ) )  ] 
            self.df.rename(index={'currentUpdateDateTime': self.ATTR_TIME },inplace =True)
            args = {
                   'dtfmt':'%m/%d/%Y  %H:%M:%S', 
                   'regex':'^(.*)$'
                      }
            func = functools.partial(self.to_datetime, **args)
            self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func) # 拆成5個      
            for i in range( len(self.df.loc[spec_attr].values[0]) ):
                attr_name = self.__subsys.split('_')[0]
                self.df.loc[ f'{attr_name}{i+1}' ] = self.df.loc[spec_attr].values[0][i] 
            logger.info(f'self.df_new-{self.df}')
            
        elif re.search('GapDriveCalibration', self.__subsys, re.IGNORECASE):  # PS Add GapDriveCalibration 
            #logger.info(f'self.df-{self.df}')
            self.df.loc[self.ATTR_TIME] = [ self.df.loc['Date'].values[0] + ' ' + self.df.loc['Time'].values[0]  ]
            args = {
                   'dtfmt':'%m/%d/%Y %H:%M:%S', 
                   'regex':'^(.*)$'
                      }
            func = functools.partial(self.to_datetime, **args)
            self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func) # 拆成5個
            # Calculation
            # self.df.loc['PosAt9-PosAt3'] = [ self.df.loc['PosAt9'].values[0] - self.df.loc['PosAt3'].values[0] ]
            # self.df.loc['PosAt6-PosAt12'] = [ self.df.loc['PosAt6'].values[0] - self.df.loc['PosAt12'].values[0] ]
            #logger.info(f'self.df_new-{self.df}')            

        elif re.search('Leakback_PM', self.__subsys, re.IGNORECASE):  # PS Add for Leakback_PM
            parser = re.compile( r'(\w+)/Leakback')
            #logger.info( f'leak_df_0:{self.df}' )
            self.df.loc['Name'] = parser.findall(self.__filename)[0]
            length = np.min( [ 10, len( self.df.loc['LeakRate'].values[0]) ] )
            self.df.loc['LeakRate'] = [ self.df.loc['LeakRate'].values[0][:length] ] # 取前10筆
            self.df.loc['Date'] = [ self.df.loc['Date'].values[0][:length] ] # 取前10筆
            self.df.loc[self.ATTR_TIME] = [ self.df.loc['Date'].values[0][0] ] # 取Date第一筆     
            args = {
                   'dtfmt':'%m-%d-%Y %H:%M:%S',
                   'regex':'^(.*)$'
                      }
            func = functools.partial(self.to_datetime, **args)
            self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func) # 拆成5個  
            dtfmt = ['year', 'month', 'day', 'hour', 'minute', 'second'] # Date 字串改成datetime
            tmp = self.df.apply( lambda x: [ [ datetime.datetime(**dict(list(zip( dtfmt, func(_elem) )))) for _elem in x['Date'] ] ] )
            tmp.index = ['Date_new']
            self.df = pd.concat([self.df, tmp], axis=0)
            logger.info( f'leak_df:{self.df}' )
        elif re.search('Leakback_TM', self.__subsys, re.IGNORECASE):  # PS Add for Leakback_TM
            parser = re.compile( r'(\w+)/Leakback')
            self.df.loc['Name'] = parser.findall(self.__filename)[0]
            length = np.min( [ 10, len( self.df.loc['LeakRate'].values[0]) ] )            
            self.df.loc['LeakRate'] = [ self.df.loc['LeakRate'].values[0][:length] ] # 取前10筆
            self.df.loc['Date'] = [ self.df.loc['Date'].values[0][:length] ] # 取前10筆
            self.df.loc[self.ATTR_TIME] = [ self.df.loc['Date'].values[0][0] ] # 取Date第一筆     
            args = {
                   'dtfmt':'%m/%d/%Y %H:%M:%S %p', 
                   'regex':'^(.*)$'
                      }
            func = functools.partial(self.to_datetime, **args)
            self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func) # 拆成5個  
            dtfmt = ['year', 'month', 'day', 'hour', 'minute', 'second'] # Date 字串改成datetime
            tmp = self.df.apply( lambda x: [ [ datetime.datetime(**dict(list(zip( dtfmt, func(_elem) )))) for _elem in x['Date'] ] ] )
            tmp.index = ['Date_new']
            self.df = pd.concat([self.df, tmp], axis=0)
            #logger.info( f'leak_df:{self.df}' )            
        elif re.search('Leakback_Airlock', self.__subsys, re.IGNORECASE):  # PS Add for Leakback_TM
            parser = re.compile( r'(\w+)/Leakback')
            self.df.loc['Name'] = parser.findall(self.__filename)[0]
            length = np.min( [ 10, len( self.df.loc['LeakRate'].values[0]) ] )
            
            self.df.loc['LeakRate'] = [ self.df.loc['LeakRate'].values[0][:length] ] # 取前10筆
            self.df.loc['Date'] = [ self.df.loc['Date'].values[0][:length] ] # 取前10筆
            self.df.loc[self.ATTR_TIME] = [ self.df.loc['Date'].values[0][0] ] # 取Date第一筆     
            args = {
                   'dtfmt':'%m/%d/%Y %H:%M:%S %p', 
                   'regex':'^(.*)$'
                      }
            func = functools.partial(self.to_datetime, **args)
            self.df.loc[self.ATTR_TIME] = self.df.loc[self.ATTR_TIME].apply(func) # 拆成5個  
            dtfmt = ['year', 'month', 'day', 'hour', 'minute', 'second'] # Date 字串改成datetime
            tmp = self.df.apply( lambda x: [ [ datetime.datetime(**dict(list(zip( dtfmt, func(_elem) )))) for _elem in x['Date'] ] ] )
            tmp.index = ['Date_new']
            self.df = pd.concat([self.df, tmp], axis=0)
            #logger.info( f'leak_df:{self.df}' )              
    def run_ingest(self):
        '''
        run_ingest reads and parses the data files associated 
        with each subsystem of a module
        This function only works on the trd attribute
        '''
        for subsys, data in self.trd.items():
            logger.debug(f"Ingesting {subsys} SSHC data")
            self.__subsys = subsys
            self.__pmidx = data[self.ZIP_PMIDX]
            for fname in data[self.ZIP_FNAME]:  
                self.__filename = fname 
                data[self.INGEST_FUNCT](**data)
                if isinstance(self.df, dict):
                    data[self.ZIP_DATA].update(**self.df)
                else:
                    if self.ATTR_TIME in self.df.index:
                        self.df = self.df.loc[:, ~self.df.loc[self.ATTR_TIME].isna()]
                    if self.__pmalias in data[self.ZIP_DATA]:
                        data[self.ZIP_DATA][self.__pmalias] = data[self.ZIP_DATA][self.__pmalias].merge(
                            self.df, how='outer', left_index=True, right_index=True)
                        data[self.ZIP_DATA][self.__pmalias].columns = [f'value_{_elem:03d}' 
                            for _elem in np.arange(data[self.ZIP_DATA][self.__pmalias].shape[1])]
                    else:
                        data[self.ZIP_DATA][self.__pmalias] = self.df

    def run_decentralize(self):
        '''
        run_decentralize derives self.module from self.trd such that each element in 
        self.module represents a module on the specific platform. The data structure 
        of self.module[<pmalias>] is similar to that of self.trd, and 
        self.module[<pmalias>][<subsys>][ZIP_DATA] is linked to self.trd[<subsys>][ZIP_DATA]
        Note:
        this is possible because python uses Pass Object by Reference, 
        and list is a mutable object
        '''
        self.module = {}
        for subsys, info in self.trd.items():
            for pmalias, data in info[self.ZIP_FNAME].items():
                if isinstance(self.golden_pm,str)\
                    and re.search(r'pm\d+', pmalias, re.IGNORECASE)\
                    and self.golden_pm.lower() != pmalias.lower():
                    continue
                try:
                    assert(pmalias in self.module)
                except:
                    self.module[pmalias] = {}
                # link self.module[pmalias][subsys] to info, and overwrite the self.ZIP_FNAME attr
                self.module[pmalias][subsys] = {**info, **{self.ZIP_FNAME:data}} 
        if isinstance(self.golden_pm,str)\
            and self.golden_pm not in self.module.keys():
            logger.error(f'{self.golden_pm} is not found')

    def run_synchronize(self, pmalias):
        for subsys, info in self.module[pmalias].items():
            if subsys in self.trd:
                info[self.ZIP_DATA] = self.trd[subsys][self.ZIP_DATA]
            else:
                self.trd[subsys] = {**info, **{self.ZIP_FNAME:{} } }
            self.trd[subsys][self.ZIP_FNAME].update({pmalias:info[self.ZIP_FNAME]})   

    def getmodelnames(self, attrs, data=None, concat=False):
        '''
        Parameter:
        concat (bool): True [=] return the full ModelConcat; False [=] return the first None entry
        data (None/pd.DataFrame): use self.df, if no data is provided
        '''
        if not isinstance(data, pd.DataFrame):
            data = self.df
        try:
            if data.empty:
                raise IndexError
            attr = attrs.pop(0)
            #an assumption here is that there is only one valid column
            ModelName = data.iloc[:,0][attr]    
            if re.search('^none', ModelName.strip(), re.IGNORECASE):
                raise KeyError
            else:
                if concat:
                    attrs.insert(0, attr)
                    ModelName = '_'.join(data.iloc[:,0][attrs].values)
        except KeyError:
            ModelName = self.getmodelnames(attrs, data, concat)
        except IndexError:
            ModelName = 'GenericModule'
        return ModelName

    def __call__(self):
        self.open_zipfile()
        self.load_autotrd_config()
        self.create_trd_obj()
        for pmalias, pminfo in self.module.items():
            try:
                key = 'PM_ConfigOpt'
                data = pminfo[key]
                self.__subsys = key
                self.__pmidx = data[self.ZIP_PMIDX] 
                self.__filename = data[self.ZIP_FNAME][0]
                self.ingest_configopt(**data)
                ModelName = self.getmodelnames(
                    attrs=['ModelName','ModuleSpec','ModuleType'], 
                    concat=True
                )
                module_config, module_alias = self.get_autotrd_config(ModelName)
                if module_alias == 'GenericModule':
                    logger.warning(f'There is no class associated with {pmalias}({ModelName})')
                    for subsys, data in pminfo.items():
                        if subsys not in self.ATTR_PM_TRD:
                            data[self.ZIP_FNAME]=[]
                logger.info(f'Ingesting {pmalias}-{ModelName} via {module_alias} class')           
                module_inputs = {
                    'zipObj' : self.zipObj, 
                    'config' : module_config,
                    'rawdata' : pminfo,
                    'pmalias' : pmalias
                }
                moduleObj = getattr(sys.modules[__name__], module_alias)(**module_inputs)
                self.run_synchronize(pmalias)
            except KeyError as err:
                logger.debug(f'Ingesting {pmalias}')
                module_alias = 'GenericModule'
                ModelName = 'GenericModule'
                moduleObj = GenericModule(
                    zipObj = self.zipObj, 
                    rawdata = pminfo
                )
            moduleObj.assign_ingest_funct()
            moduleObj.run_ingest()
            pminfo[self.ATTR_MODULE] = ModelName
            pminfo[self.ATTR_KEY] = moduleObj.aliases
            if hasattr(moduleObj, 'toolid'):
                self.toolid = moduleObj.toolid 
        if not hasattr(self, 'toolid'):
            self.augm_toolid()
        #self._augm_tpo() # PS

    def approx_msg_value(self, df, attr, x0, num_neighbors=1):
        logger.warning(f'The setpoint of {x0} is not in {attr}')
        attr_diff, attr_sign, attr_wt = 'diff', 'sign', 'wt'
        df[attr_diff] = df.index - x0
        df[attr_sign] = df[attr_diff].apply(np.sign)
        df[attr_wt] = df[attr_diff]**2
        _df = df.groupby(attr_sign)
        neighbors = []
        for sign, data in _df:
            data.sort_values(attr_wt,inplace=True)
            data = data.iloc[:num_neighbors]
            neighbors.append(data)
        neighbors = pd.concat(neighbors)
        neighbors[attr_wt] = 1/neighbors[attr_wt]
        return (neighbors[attr]*neighbors[attr_wt]).sum()/neighbors[attr_wt].sum()

    def _recursive_lookup(self, data, patterns, path=[]):
        '''
        patterns (list of strs)
        '''
        results = []
        if isinstance(data, dict):
            if all([_elem in data for _elem in patterns]):
                results.append(path)
            else:
                for key, value in data.items():
                    results.extend(self._recursive_lookup(value, patterns, path= path+[key]))
        elif isinstance(data, list):
            for index, item in enumerate(data):
                results.extend(self._recursive_lookup(item, patterns, path= path+[index]))
        return results

    def _extract_nested_data(self, data, path):
        '''
        _extract_nested_data accesses elements of a nested dictionary with any
        arbitrary depth
        Paramater:
        keys (list of strs): key at each level of a nested dictionary
        '''
        if path:
            key = path.pop(0)
            data = data[key]
            return self._extract_nested_data(data, path)
        else:
            return data


    def _augm_tpo(self):
        return 
        pto_key =  'TPO' 
        tasks = {
            'pm':[ 'PumpOUTBeforePinsDown', 'PinsUpAfterProcessEnd',
                'WaferTransferPressure', 'WaferTransferPressureTolerance',
                'PressureStabilizedTimeSeconds', 'EnableSchedulingBeforeWacEndTime',
                'EnableWaferReceivingBeforeWacEndTime', 'EnableSchedulingBeforeWaferProcessEndTime'],
            # 'ui':['EnhancedTput'],
            # 'vtm':['EnhancedTput', 'TMUPCSetpoint'],
            # 'airlock':['Alx_OverventTime']
        }
        tpo = {}
        for pattern, attrs in tasks.items():
            keys = [key for key in self.trd.keys() if re.search(pattern, key, re.IGNORECASE)]
            for key in keys:
                for pmalias, df in self.trd[key][self.ZIP_DATA].items():
                    for attr in attrs:
                        try:
                            sample = [attr,df.loc[attr].values[0]]
                        except KeyError:
                            sample = []
                        if sample:
                            try:
                                tpo[pmalias].append(sample)
                            except KeyError:
                                tpo.update({pmalias:[sample]})
        for pmalias in tpo.keys():
            data = tpo.get(pmalias)
            tpo[pmalias]=(pd.DataFrame(data, 
                        columns = ['ConfigOpt', 'ConfigVal'])
             .T
             .rename(columns={idx:f'value_{idx:03d}' for idx in range(len(data))})
            )
        print(pto_key)
        self.augm_trd_obj(pto_key, tpo)

    def augm_toolid(self):
        logger.warning('Executing the toolid-recovery protocol')
        key =  'toolID' 
        kw = r'archived_config_PM\d+'
        dtype = 'txt'
        reObj = re.compile(f'{kw}\\.{dtype}$', re.IGNORECASE)
        filenames = [_elem for _elem in self.zipObj.namelist() if reObj.search(_elem)]
        pmaliases = self._recursive_lookup(self.module, [self.ATTR_MODULE])
        pmaliases = [item for item in itertools.chain.from_iterable(pmaliases) 
            if re.search(r'pm\d+', item, re.IGNORECASE)]
        pmaliases = {pmalias:self._extract_nested_data(self.module, [pmalias, self.ATTR_MODULE])\
            for pmalias in pmaliases}
        if filenames:
            if set(pmaliases.values()) != {'GenericModule'}:
                pmaliases = {pmalias:module for pmalias, module in pmaliases.items()
                            if module!='GenericModule'}
                filenames = [filename for filename in filenames
                    if re.search(f'({"|".join(pmaliases.keys())})',filename, re.IGNORECASE)]
            reObj = re.compile(r':(\S*?)/(PM\d+)')            
            data = {}
            for filename in filenames:
                with io.TextIOWrapper(self.zipObj.open(filename)) as fObj:
                    rawtxt = fObj.readlines()
                    for line in rawtxt:
                        mo = reObj.search(line)
                        if mo:
                            toolid, chamberid = mo.groups()
                            dirname, basename = os.path.split(filename)
                            if re.search(chamberid, dirname, re.IGNORECASE):
                                item = {chamberid:filename}
                                try:
                                    data[toolid].update(item)
                                except KeyError:
                                    data[toolid] = item
                            break
            metadata = {toolid:len(value) for toolid, value in data.items()}
            self.toolid = max(metadata, key=metadata.get)         
            metadata = {pmalias: f'{self.toolid}_{pmalias}' for pmalias in data[self.toolid].keys()}   
             
        else:
            try:
                self.toolid = re.search(r'^\w+?_(.*?)_\d',os.path.basename(self.zipfile)).group(1)
                metadata = {pmalias: f'{self.toolid}_{pmalias}' for pmalias in pmaliases}  
            except AttributeError:
                logger.error('The toolid-recovery protocol failed')
                return 
        data = pd.DataFrame.from_dict(metadata, orient='index', columns=['Tool_Mod'])
        self.augm_trd_obj(key, data)
        

class GenericModule(Lam2300):
    def __init__(self, zipObj, rawdata,  **kwargs):
        self.trd = rawdata
        self.zipObj = zipObj
        self.aliases = []
        if kwargs:
            config = kwargs['config']
            for key, data in config.items():
                if key in self.trd: continue
                self.trd.update({key:copy.deepcopy(data)})
                self.trd[key][self.ZIP_DATA] = {}
            self.get_filename(kwargs['pmalias'])

class VantexB(GenericModule):
    pass

class ICS(GenericModule):
    pass

class Microwave2_Strip(GenericModule):
    pass

class FlexHX(GenericModule):
    def run_data_augm(self):
        super().run_data_augm()
        if re.search('WAP_Conductance', self._Lam2300__subsys, re.IGNORECASE):            
            self.df.rename(index={'CurrentConductance.Timestamp.Timestamp':self.ATTR_TIME}, inplace = True)
            if self.ATTR_TIME not in self.df.index:
                self.df = pd.DataFrame()

class FlexGX(FlexHX):
    pass

class FlexFX(FlexHX):
    pass

class KiyoF(GenericModule):
    def run_data_augm(self):
        '''
        run_data_augm derives additional features based on the dataframe information
        Parameter:
        df (dataframe): generated by self.INGEST_FUNCT(.)
        subsys (str): SSHC 
        Note: 
        This function contains many user-defined functions to derive additional features
        required to generate the tool readiness document (trd)
        '''
        super().run_data_augm()
        if re.search('Valve_Conductance', self._Lam2300__subsys, re.IGNORECASE):
            '''
                Slope (60, near closed) = (conductance at 60 – conductance at 0) / 60 counts
                Slope (180, steep section) = (conductance at 200 – conductance at160) / 40 counts
                Slope (800, near open) = (conductance at 1000 – conductance at 800) / 200 counts
            ''' 
            attr_y, attr_x = 'ConductancesAtDown','Positions'
            tmp = self.df.T[[attr_y, attr_x]].apply(pd.Series.explode).set_index(attr_x)
            usr_cmd = [(0,60,60), (160, 200, 180), (800, 1000, 800)]
            #usr_cmd = [(0,60,60), (160, 200, 180), (800, 1000, 1000)] # Winbond EIDH change to 1000            
            usr_data = {'Setpoints':[], 'Slope':[]}
            for x0, x1, setpoint in usr_cmd:
                usr_data['Setpoints'].append(setpoint)
                MissingValueErr = False
                # interpolate the ConductancesAtDown at the unknown setpoint
                for xi in [x0,x1]:
                    if xi not in tmp.index:
                        tmp.loc[xi, attr_y] = self.approx_msg_value(tmp, attr_y, xi)
                        MissingValueErr=True
                    if MissingValueErr: break
                if MissingValueErr:
                    slope = -1
                else:
                    slope = (tmp.loc[x1, attr_y] - tmp.loc[x0, attr_y])/(x1 - x0)                    
                usr_data['Slope'].append(slope)
            for _attr, _data in usr_data.items():
                self.df.loc[_attr]=[_data]
            # PS_A3 extract slope
            spec_attr = 'Slope'
            tmp = self.df.loc[spec_attr].values[0]
            for i in range( len( tmp ) ):
                attr_name = 'Conductance_slope'+ str(i+1) 
                self.df.loc[ attr_name ] = tmp[i] 
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS
            
        elif re.search('Bias_Voltage_Probe', self._Lam2300__subsys, re.IGNORECASE):
            attr_ai, attr_ao, attr_y =  'VProbes', 'Setpoints', 'VProbeSquareds'
            MAX_VProbes = 600
            for attr in self.df:
                sample = self.df[attr]
                prev_ai = 0 
                for idx, value_ai in enumerate(sample.loc[attr_ai]):
                    if prev_ai < value_ai:
                        prev_ai = value_ai
                    else:
                        idx+=1
                        for _elem in [attr_ai, attr_ao, attr_y]:
                            sample.loc[_elem]=sample.loc[_elem][:idx]
                        break
                # PS delete dropping VProbes and VProbeSquareds  # Winbond EIDH
                #sample.loc[attr_ai] = [ np.NaN if _elem <10 else _elem for _elem in sample.loc[attr_ai] ]
                #sample.loc[attr_y] = [ np.NaN if _elem <100 else _elem for _elem in sample.loc[attr_y] ]
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS

class Selis(KiyoF):
    pass 

class Prevos(KiyoF):
    pass 
    


class Metal(GenericModule): # Versys Metal
    def run_data_augm(self):
        '''
        run_data_augm derives additional features based on the dataframe information
        Parameter:
        df (dataframe): generated by self.INGEST_FUNCT(.)
        subsys (str): SSHC
        Note:
        This function contains many user-defined functions to derive additional features
        required to generate the tool readiness document (trd)
        '''
        super().run_data_augm()
        if re.search('Valve_Conductance', self._Lam2300__subsys, re.IGNORECASE):
            '''
                Slope (60, near closed) = (conductance at 60 – conductance at 0) / 60 counts
                Slope (180, steep section) = (conductance at 200 – conductance at160) / 40 counts
                Slope (800, near open) = (conductance at 1000 – conductance at 800) / 200 counts
            '''
            attr_y, attr_x = 'ConductancesAtDown', 'Positions'
            tmp = self.df.T[[attr_y, attr_x]].apply(pd.Series.explode).set_index(attr_x)
            usr_cmd = [(0, 60, 60), (160, 200, 180), (800, 1000, 800)]
            usr_data = {'Setpoints': [], 'Slope': []}
            for x0, x1, setpoint in usr_cmd:
                usr_data['Setpoints'].append(setpoint)
                MissingValueErr = False
                # interpolate the ConductancesAtDown at the unknown setpoint
                for xi in [x0, x1]:
                    if xi not in tmp.index:
                        tmp.loc[xi, attr_y] = self.approx_msg_value(tmp, attr_y, xi)
                        logger.debug(tmp)
                        MissingValueErr = True
                    if MissingValueErr: break
                if MissingValueErr:
                    slope = -1
                else:
                    slope = (tmp.loc[x1, attr_y] - tmp.loc[x0, attr_y]) / (x1 - x0)
                usr_data['Slope'].append(slope)
            for _attr, _data in usr_data.items():
                self.df.loc[_attr] = [_data]

        elif re.search('Bias_Voltage_Probe', self._Lam2300__subsys, re.IGNORECASE):
            attr_ai, attr_ao, attr_y = 'VProbes', 'Setpoints', 'VProbeSquareds'
            MAX_VProbes = 600
            for attr in self.df:
                sample = self.df[attr]
                prev_ai = 0
                for idx, value_ai in enumerate(sample.loc[attr_ai]):
                    if prev_ai < value_ai:
                        prev_ai = value_ai
                    else:
                        idx += 1
                        for _elem in [attr_ai, attr_ao, attr_y]:
                            sample.loc[_elem] = sample.loc[_elem][:idx]
                        break
        elif re.search('(Linearity|RFLinearity)', self._Lam2300__subsys, re.IGNORECASE):
            '''
                UCL is Generator max power. LCL is max percentage error
                PerSetpoint = MeterReadings/attr_ao'''
            attr_ai, attr_ao, attr_y = 'MeterReadings','Setpoints', 'PerSetpoint'
            tmp = self.df.T[[attr_ai, attr_ao]].apply(pd.Series.explode)
            self.df.loc[attr_y]=[tmp.apply(lambda _elem: (_elem[attr_ai]-_elem[attr_ao])/_elem[attr_ao]*100
                                                    if _elem[attr_ao] else np.NaN,axis=1).values.tolist()]
            attr_ai, attr_ao, attr_y= 'ActualMax', 'Max', 'MaxPower'
            attr_gen = 'GeneratorType'
            attr_z = self.df.loc[attr_gen]
            self.df.loc[attr_ai] = self.df.loc[attr_ai].fillna(0)
            #self.df.loc[attr_ao] = self.df.apply(lambda _elem: round((_elem[attr_ao])/500+.49)*500 if  _elem[attr_ao]
                # WHEN THE ActualMax IS NONE, SET THIS TO 1e-6 TO ERROR OUT THE RESULT
                #else round((_elem[attr_ai])/500+.49)*500)
            if re.search('RFDS', str(attr_z), re.IGNORECASE):
                self.df.loc[attr_ao] = 1250
            elif re.search('APEX', str(attr_z), re.IGNORECASE):
                self.df.loc[attr_ao] = 1500
            self.df.loc[attr_y]= self.df.apply(
                lambda _elem: (_elem[attr_ao]- _elem[attr_ai])/_elem[attr_ao] if  _elem[attr_ai]
                # WHEN THE ActualMax IS NONE, SET THIS TO 1 TO ERROR OUT THE RESULT
                else 1
                        )

class KiyoGP(KiyoF):
    pass

class FlexFL(GenericModule):
    def run_data_augm(self):
        super().run_data_augm()
        if re.search('WAP_Conductance', self._Lam2300__subsys, re.IGNORECASE):       
            attr_x , attr_y = 'CurrentConductance.Positions', 'CurrentConductance.Conductances'
            for i in range( 1, len( self.df.loc[attr_y].values[0] ) ):
                if (self.df.loc[attr_y].values[0][i-1] - self.df.loc[attr_y].values[0][i]) < 0.1:
                    target1 = self.df.loc[attr_y].values[0][i-1]
                    target2 = self.df.loc[attr_x].values[0][i-1]
                    break
            attr_x , attr_y = 'BaselineConductance.Comment', 'CurrentConductance.Comment'
            target3 = 'Pass' if self.df.loc[attr_x].values[0] == self.df.loc[attr_y].values[0] else 'Failed'
            self.df.loc[ 'Conductance_Knee' ] = target1
            self.df.loc[ 'Conductance_Knee_Poistion' ] = target2
            self.df.loc[ 'Conductance_Comment' ] = target3
            #logger.info(f'self.df flexfl-{self.df}')
            
        elif re.search('Valve_Conductance', self._Lam2300__subsys, re.IGNORECASE):
            logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )
            # Add ConductancesAtUp and ConductancesAtDown at poistion 1000
            attr_y1, attr_y2 = 'ConductancesAtUp', 'ConductancesAtDown'
            attr_pos = 'Positions'
            positions = self.df.loc[attr_pos].values[0]
            target_position = [400, 1000]
            for pos in target_position:
                idx = positions.index(pos)
                self.df.loc[f'{attr_y1}_{pos}'] = self.df.loc[attr_y1].values[0][idx] 
                self.df.loc[f'{attr_y2}_{pos}'] = self.df.loc[attr_y2].values[0][idx] 

            # self.df.loc[ attr_y1 + '_1000' ] = self.df.loc[attr_y1].values[0][-1] 
            # self.df.loc[ attr_y2 + '_1000' ] = self.df.loc[attr_y2].values[0][-1] 
            logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  

class FlexGL(FlexFL):
    pass
    # def run_data_augm(self):
    #     super().run_data_augm()
    #     # ??   
    #     if re.search('Valve_Conductance', self._Lam2300__subsys, re.IGNORECASE):
    #         #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )
    #         # Add ConductancesAtUp and ConductancesAtDown at poistion 1000
    #         attr_y1, attr_y2 = 'ConductancesAtUp', 'ConductancesAtDown'
    #         self.df.loc[ attr_y1 + '_1000' ] = self.df.loc[attr_y1].values[0][-1] 
    #         self.df.loc[ attr_y2 + '_1000' ] = self.df.loc[attr_y2].values[0][-1] 
    #         logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  



class Flex45(GenericModule):
    def run_data_augm(self):
        super().run_data_augm()
        if re.search('WAP_Conductance', self._Lam2300__subsys, re.IGNORECASE):       
            attr_x , attr_y = 'CurrentConductance.Positions', 'CurrentConductance.Conductances'
            for i in range( 1, len( self.df.loc[attr_y].values[0] ) ):
                if (self.df.loc[attr_y].values[0][i-1] - self.df.loc[attr_y].values[0][i]) < 0.1:
                    target1 = self.df.loc[attr_y].values[0][i-1]
                    target2 = self.df.loc[attr_x].values[0][i-1]
                    break
            self.df.loc[ 'Conductance_Knee' ] = target1
            self.df.loc[ 'Conductance_Knee_Poistion' ] = target2
            self.df.loc[ 'Conductance_Comment' ] = 'NULL' # PS
            #logger.info(f'self.df-{self.df}')
            
        elif re.search('VC_At_Up|VC_At_Down', self._Lam2300__subsys, re.IGNORECASE):      
            # Find the conductance at poistion 65
            attr_x , attr_y = 'Positions', 'ConductancesAtUp'
            if 65 in self.df.loc[attr_x].values[0] : 
                pos65 = self.df.loc[attr_x].values[0].index(65)
                self.df.loc[ f'{ self._Lam2300__subsys}_65' ] = self.df.loc[attr_y].values[0][pos65]
            #logger.info(f'self.df-{self.df}')

class Flex(GenericModule):
    pass

class Syndion(GenericModule):
    def run_data_augm(self):
        '''
        run_data_augm derives additional features based on the dataframe information
        Parameter:
        df (dataframe): generated by self.INGEST_FUNCT(.)
        subsys (str): SSHC 
        Note: 
        This function contains many user-defined functions to derive additional features
        required to generate the tool readiness document (trd)
        '''
        super().run_data_augm()
        if re.search('Bias_Voltage_Probe', self._Lam2300__subsys, re.IGNORECASE):
            attr_ai, attr_ao, attr_y =  'VProbes', 'Setpoints', 'VProbeSquareds'
            MAX_VProbes = 600
            for attr in self.df:
                sample = self.df[attr]
                prev_ai = 0 
                for idx, value_ai in enumerate(sample.loc[attr_ai]):
                    if prev_ai < value_ai:
                        prev_ai = value_ai
                    else:
                        idx+=1
                        for _elem in [attr_ai, attr_ao, attr_y]:
                            sample.loc[_elem]=sample.loc[_elem][:idx]
                        break

class Solo(GenericModule):
    def run_data_augm(self):
        super().run_data_augm()
        if re.search('Valve_Conductance', self._Lam2300__subsys, re.IGNORECASE):
            '''
                Slope (30, near closed) = (conductance at 30 – conductance at 0) / 60 counts
                Slope (150, steep section) = (conductance at 200 – conductance at150) / 40 counts
                Slope (800, near open) = (conductance at 1000 – conductance at 800) / 200 counts
            ''' 
            attr_y, attr_x = 'ConductancesAtDown','Positions'
            tmp = self.df.T[[attr_y, attr_x]].apply(pd.Series.explode).set_index(attr_x)
            usr_cmd =  [(0,30,30), (150, 200, 200), (800, 1000, 1000)]             
            usr_data = {'Setpoints':[], 'Slope':[]}
            for x0, x1, setpoint in usr_cmd:
                usr_data['Setpoints'].append(setpoint)
                MissingValueErr = False
                # interpolate the ConductancesAtDown at the unknown setpoint
                for xi in [x0,x1]:
                    if xi not in tmp.index:
                        tmp.loc[xi, attr_y] = self.approx_msg_value(tmp, attr_y, xi)
                        MissingValueErr=True
                    if MissingValueErr: break
                if MissingValueErr:
                    slope = -1
                else:
                    slope = (tmp.loc[x1, attr_y] - tmp.loc[x0, attr_y])/(x1 - x0)                    
                usr_data['Slope'].append(slope)
            for _attr, _data in usr_data.items():
                self.df.loc[_attr]=[_data]
            # PS_A3 extract slope
            spec_attr = 'Slope'
            tmp = self.df.loc[spec_attr].values[0]
            for i in range( len( tmp ) ):
                attr_name = 'Conductance_slope'+ str(i+1) 
                self.df.loc[ attr_name ] = tmp[i] 
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS

        if re.search('TCP.*VProbe', self._Lam2300__subsys, re.IGNORECASE):
            alias = 'rSeriesDict.lastCal.Timestamp' # PS
            self.df.rename(index={alias:self.ATTR_TIME}, inplace = True)
            # self.df.loc[ attr_name ] = tmp[i]
            # rSeriesDict.slope  # InnerSlope_TCPNPT
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS  

        if re.search('TCP.*Linearity', self._Lam2300__subsys, re.IGNORECASE):
            '''
            UCL is Generator max power. LCL is max percentage error
            PerSetpoint = MeterReadings/attr_ao'''
            compute_y = lambda ao, ai: (ai-ao)/ao*100 if ao else np.NaN
            AI_ATTR, AO_ATTR, Y_ATTR = 'MeterReadings', 'Setpoints', 'PerSetpointMeter'
            tmp = self.df.T.loc[:,[AO_ATTR, AI_ATTR]].apply(pd.Series.explode)
            self.df.loc[Y_ATTR]=[tmp.apply(lambda values: compute_y(*values), axis=1).values.tolist()]

            AI_ATTR, AO_ATTR, Y_ATTR = 'CalibratedPower', 'Setpoints', 'PerSetpointDelivered'
            tmp = self.df.T.loc[:,[AO_ATTR, AI_ATTR]].apply(pd.Series.explode)
            self.df.loc[Y_ATTR]=[tmp.apply(lambda values: compute_y(*values), axis=1).values.tolist()]
            # Slope
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS  

        if re.search('TCP_JunctionBoxTune', self._Lam2300__subsys, re.IGNORECASE):
            self.df.rename(index={'LastCalibrationTime.Timestamp':self.ATTR_TIME}, inplace = True)
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS 

class Coronus(GenericModule):
    pass

class Bevel(GenericModule):
    pass

class KiyoCSeries(KiyoF):
    pass

class KiyoESeries(KiyoCSeries):
    pass        
class KiyoEXP(KiyoCSeries): # Kiyo EXP
    pass   
class Kiyo_Poly(KiyoF):
    #standard Versys Kiyo Config
    pass

class Kiyo_Metal(KiyoF):
    #Versys Metal L or Metal45 or Metal HP+
    pass

class MetalM(KiyoF): #Versys Metal M
    pass

class MetalNXP(KiyoF): #Versys Metal NXP
    pass

class Kiyo45_Poly(KiyoF):
    pass

class Kiyo45_Metal(KiyoF):
    #Versys Metal L or Metal45
    pass

class Strata(GenericModule): 
    def run_data_augm(self):
        '''
        run_data_augm derives additional features based on the dataframe information
        Parameter:
        df (dataframe): generated by self.INGEST_FUNCT(.)
        subsys (str): SSHC 
        Note: 
        This function contains many user-defined functions to derive additional features
        required to generate the tool readiness document (trd)
        '''  
        dtfmt='%B %d, %Y %I:%M:%S %p'
        dfrft ='%m/%d/%Y%H:%M:%S %p'
        super().run_data_augm()
        if re.search('FlowCalibration', self._Lam2300__subsys, re.IGNORECASE): 
            reObj = re.compile('^(.*?(pm|am))', re.IGNORECASE)
            data = self.df
            data.index = data.index.str.split('.', expand=True)
            data = data.unstack(level = 0)
            data.columns = data.columns.get_level_values(1)
            data = data.T.apply(pd.Series.explode)
            data = data[~data['LastCalibratedTime'].isna()]
            data['LastCalibratedTime'] = data['LastCalibratedTime'].apply(lambda dt: datetime.datetime.strptime(reObj.search(dt).group(1), dtfmt))
            data['Timestamp.Timestamp']  = data['LastCalibratedTime'].apply(lambda x: [int(_elem) 
                for _elem in x.strftime('%Y,%m,%d,%H,%M,%S').split(',')])
            data['GasName']  = data['CalibrationGasName']
            data.index.names = [attr if isinstance(attr, str) else 'GasLine' for attr in data.index.names]
            db = []
            for _, sample in data.groupby(level='GasLine'):
                db.append(sample.sort_values('LastCalibratedTime', ascending=True).tail(1))
            db = pd.concat(db, axis=0, sort =False).reset_index().T
            db.columns = [f'value_{idx:03}' for idx in range (db.shape[1])]
            self.df = db              

class Halo(Strata):
    pass   

class KiyoGX(GenericModule):
    def run_data_augm(self):
        '''
        run_data_augm derives additional features based on the dataframe information
        Parameter:
        df (dataframe): generated by self.INGEST_FUNCT(.)
        subsys (str): SSHC 
        Note: 
        This function contains many user-defined functions to derive additional features
        required to generate the tool readiness document (trd)
        '''
        super().run_data_augm()
        if re.search('Valve_Conductance', self._Lam2300__subsys, re.IGNORECASE):            
            '''
                Slope (30, near closed) = (conductance at 30 – conductance at 0) / 30 counts
                Slope (180, steep section) = (conductance at 200 – conductance at160) / 40 counts
                Slope (800, near open) = (conductance at 1000 – conductance at 800) / 200 counts
            '''        
            attr_y, attr_x = 'ConductancesAtDown','Positions'
            tmp = self.df.T[[attr_y, attr_x]].apply(pd.Series.explode).set_index(attr_x)
            usr_cmd = [(0,30,30), (160, 200, 180), (800, 1000, 800)]
            usr_data = {'Setpoints':[], 'Slope':[]}
            for x0, x1, setpoint in usr_cmd:
                usr_data['Setpoints'].append(setpoint)
                MissingValueErr = False
                # interpolate the ConductancesAtDown at the unknown setpoint
                for xi in [x0,x1]:
                    if xi not in tmp.index:
                        tmp.loc[xi, attr_y] = self.approx_msg_value(tmp, attr_y, xi)
                        logger.debug(tmp)
                        MissingValueErr=True
                    if MissingValueErr: break
                if MissingValueErr:
                    slope = -1
                else:
                    slope = (tmp.loc[x1, attr_y] - tmp.loc[x0, attr_y])/(x1 - x0)                    
                usr_data['Slope'].append(slope)
            for _attr, _data in usr_data.items():
                self.df.loc[_attr]=[_data]

            # PS_A3 extract slope
            spec_attr = 'Slope'
            tmp = self.df.loc[spec_attr].values[0]
            for i in range( len( tmp ) ):
                attr_name = 'Conductance_slope'+ str(i+1) 
                self.df.loc[ attr_name ] = tmp[i] 
            #logger.info( f'{self._Lam2300__subsys};self.df-{self.df}' )  # PS

class KiyoGXE(KiyoGX):
    pass

class autotrd(Lam2300):
    GOLDEN_SUBSYS = ['PM_ConfigOpt', 'PM_CV']
    def __init__(
        self, 
        rawdata={}, 
        specfile=None, 
        specfile_A3=None,  # PS
        golden_pm=None, 
        rawinfo = {},
        rawlog = {},
        autopm_auth={},
        workers=1, 
        application = None, # PS
        **kwargs
    ):
        '''
        Parameters:
        rawdata (dictionary of trdObj)
        specfile (str): name of the xlsx spec file
        workers (int): number of processes to be run in parallel
        '''
        self.rawdata = rawdata
        self.rawinfo = rawinfo
        self.rawlog = rawlog
        self.specfile = specfile
        self.specfile_A3 = specfile_A3 # PS
        self.golden_pm = golden_pm+'(golden)' if golden_pm else golden_pm # PS golden_pm
        self.autopm_auth = autopm_auth
        self.ErrorCode = 0
        self.workers = mp.cpu_count() if (workers == -1) | (workers > mp.cpu_count()) else workers
        self.application = application # PS
    
    def _make_xmllog(self):
        def flatten(data):
            #https://www.geeksforgeeks.org/python-program-to-flatten-a-nested-list-using-recursion/
            if data == []:
                return data
            if isinstance(data[0], list):
                return flatten(data[0]) + flatten(data[1:])
            return data[:1] + flatten(data[1:])
        xml_files = []
        for tool_id, rawdata in self.rawdata.items():
            xml_files.extend([list(data[self.ZIP_FNAME].values())
                for data in rawdata.values() if data[self.ZIP_DTYPE]=='xml'])
        xml_files = list(set(flatten(xml_files)))
        self.xmllog = [os.path.basename(path) for path in xml_files]

    def append_trd_obj(self, subsys, trdObj=None):
        '''
        append_trd_obj adds NEW elements to self.trd dictionary
        Parameter:
        subsys (str): key 
        '''
        if not isinstance(trdObj, dict):
            trdObj = self.trd
        content = {
            self.ATTR_KEY:subsys,
            self.ZIP_DTYPE:'',
            self.ZIP_DATA:{},
            self.ATTR_PMALIAS:[],
            self.EXCEL_ERR:[],
            self.EXCEL_RPT:{},
            self.TOCSV_DATA:{}
        }
        try:
            trdObj[subsys].update(content)
        except KeyError:
            paths = self._recursive_lookup(self.config_lib, [subsys])
            if paths:
                path = paths[0]
                path.append(subsys)
                data = self._extract_nested_data(self.config_lib, path)
                content.update(data)
            trdObj[subsys] = content
    def __create_db(self):
        '''
        create_db aggregates the ingested data from multiple platforms,
        to create a database, i.e., self.db.
        1. the name of an element in self.db is (platform)_(subsystem)
        2. each element contains all data recorded on the corresponding subsystem
        '''
        self.db = {}
        self.trd = {}
        rawlog = {}
        for tool_idx, rawdata in self.rawdata.items():
            if self.rawlog:
                rawlog[tool_idx] = self.rawlog.get(tool_idx)
            else:
                rawlog[tool_idx] = ''
            for subsys, info in rawdata.items():
                logger.info(f'subsys-{subsys}; info-{info}') # PS
                logger.info( f'self.rawinfo-{self.rawinfo}' )
                try:
                    assert(subsys in self.trd)
                except:
                    self.append_trd_obj(subsys)
                    self.trd[subsys][self.ZIP_DTYPE] = info[self.ZIP_DTYPE]

                for pmalias, data in info[self.ZIP_DATA].items():
                    if self.rawinfo:
                        pm_model = self.rawinfo[tool_idx][pmalias]
                    else:
                        pm_model = None
                    pmalias = f'{tool_idx}_{pmalias}'
                    try:
                        assert(pmalias in self.db)
                    except:
                        self.db[pmalias] = {}
                    self.db[pmalias][subsys] = data
                    self.db[pmalias][self.ATTR_MODULE] = pm_model
                    self.trd[subsys][self.ATTR_PMALIAS].append(pmalias)
                    self.trd[subsys][self.ZIP_DATA].update({pmalias:data})
        self.rawlog = rawlog

    def __query_svn(self):
        '''
        __query_svn identifies the software version of each platform
        '''
        key = 'SW_Configopt'
        self.svn_info = {}
        for tool_idx, rawdata in self.rawdata.items():
            try:
                assert(len(rawdata[key][self.ZIP_DATA])>0)
                for data in rawdata[key][self.ZIP_DATA].values():
                    self.svn_info[tool_idx] = data.loc[self.ATTR_SVN, 'value_000']
                    break
            except (AssertionError, KeyError):
                self.svn_info[tool_idx] = 'unknown'

    def __query_gasbox_interlock(self):
        '''
        __query
        '''
        key = 'GasBox_Interlock'
        self.gasbox_interlock = []
        for tool_idx, rawdata in self.rawdata.items():
            try:
                for pmalias, data in rawdata[key][self.ZIP_DATA].items():
                    data_new = data.copy()
                    data_new[ f'{tool_idx}_{pmalias}' ] = 'O'
                    data_new.set_index(['GasName','InterlockedGases'], inplace=True)                    
                    self.gasbox_interlock.append( data_new )
            except KeyError:
                pass

    def __query_gasbox_config(self):
        '''
        query_gasbox_config identifies all unique gasbox configs, and 
        create list-type attribute, self.gasbox_config.
        '''
        key = 'GasBox_Config'
        self.gasbox_config = []
        
        def transfer_gas(x): 
            tmp = 'Gas{:02}'.format( int(x[-1])) if re.search(r'^gas(\d)$',x, re.IGNORECASE) else x
            return tmp
        try:
            for rawdata in self.rawdata.values():
                if key not in rawdata:
                    self.append_trd_obj(key, rawdata)
                if all([pmalias in rawdata[key][self.ZIP_DATA].keys()
                    for pmalias in rawdata['PM_ConfigOpt'][self.ZIP_DATA].keys()]):
                    for data in rawdata[key][self.ZIP_DATA].values():
                        # PS  transfer multi-index for every df in self.gasbox_config
                        data.index = pd.MultiIndex.from_tuples([ ( transfer_gas(_elem[0]) , _elem[1] ) 
                                                                for _elem in data.index], names = ['Gas_Value','Gas']) 
                        data.sort_index(inplace=True)
                        flag=True
                        for gasbox in self.gasbox_config:
                            if gasbox.equals(data):
                                flag=False
                        if flag: 
                            self.gasbox_config.append(data) # 
                else:
                    raise AssertionError('Missing GasBox Error')            
        except (KeyError, AssertionError) as err:
            logger.warning(f'{err}, Executing the gasbox-recovery protocol')
            for rawdata in self.rawdata.values():
                if key not in rawdata: 
                    rawdata[key]={self.ZIP_DATA:{}}
                for pm_alias, data in rawdata['PM_ConfigOpt'][self.ZIP_DATA].items():
                    if pm_alias in rawdata[key][self.ZIP_DATA]: continue
                    gasbox = {
                        'Gas_Value':'Unknown',
                        'Gas':'Unknown',
                    }
                    try: 
                        gasbox['MaxFlow'] = data.loc['GasboxVersion'][0]
                    except KeyError:
                        gasbox['MaxFlow'] = 'No GasBox'
                    rawdata[key][self.ZIP_DATA][pm_alias] = pd.DataFrame(gasbox, index=[0]).set_index(['Gas_Value','Gas'])
            self.__create_db()          
            self.__query_gasbox_config()
          
    def __query_pm_config(self):
        '''
        query_pm_config creates a dataframe object, i.e., self.pm_config, that
        contains the module_info and gasbox_idx for each pm in this analysis
        '''
        self.module_info=[]
        self.pm_config = []
        self.gasbox_idx = {}
        for pmalias, data in self.db.items():
            if re.search(r'pm\d', pmalias, re.IGNORECASE): # ?  golden same tool Z4
                try:
                    module_info = self.getmodelnames(
                        attrs = ['ModelName','ModuleSpec','ModuleType'], 
                        data = data['PM_ConfigOpt'], 
                        concat=True
                    )
                    #remove the subsystem health checks that dont belong to the current model
                    module_alias = data[self.ATTR_MODULE] if data[self.ATTR_MODULE] else module_info
                except KeyError as err:
                    logger.debug(err)
                try:
                    module_config, module_alias = self.get_autotrd_config(module_alias)
                    err = [ _elem  for _elem in data.keys()
                            if _elem not in module_config]
                    for subsys in err:
                        if self.trd[subsys][self.ZIP_DTYPE] == 'xml':
                            pass
                        else:
                            logger.debug(f'{pmalias}.{subsys} does not exist in {module_alias} config')
                            del data[subsys]
                except KeyError as err:
                    logger.debug(err)
                
                try:
                    #identify the gasbox config of this module
                    gasbox_idx = [idx for idx, gasbox in enumerate(self.gasbox_config) 
                                  if data['GasBox_Config'].equals(gasbox)][0]
                    #identify the software version number of this module
                    svn = [svn for toolid, svn in self.svn_info.items()
                            if re.search(f'{toolid}_', pmalias, re.IGNORECASE)][0]
                    #identify the zipfile of this module
                    zipfile = [filename for toolid, filename in self.rawlog.items()
                            if re.search(f'{toolid}_', pmalias, re.IGNORECASE)][0]
                    data[self.ATTR_GASBOX] = gasbox_idx
                    data[self.SPEC_CONFIG] = module_info
                    if module_info not in self.module_info: self.module_info.append(module_info)
                    self.pm_config.append({
                        self.ATTR_PMALIAS:pmalias, 
                        self.SPEC_CONFIG:module_info, 
                        self.ATTR_GASBOX:gasbox_idx,
                        self.ATTR_SVN:svn,
                        self.ZIP_FNAME:zipfile
                        })
                except KeyError as err:
                    logger.debug(err)                    
                    
                try:
                    self.gasbox_idx[module_info].add(gasbox_idx)
                except KeyError:
                    self.gasbox_idx[module_info]=set([gasbox_idx])
        self.pm_config = pd.DataFrame(self.pm_config)

    def __update_gasbox_config(self):
        '''
        __update_gasbox_config adds an attribute,i.e., pmalias, to the gas table
        so that we can know which pm uses which gasbox
        '''
        df = self.pm_config.groupby(self.ATTR_GASBOX)
        for gasbox_idx, data in df:            
            self.gasbox_config[gasbox_idx].loc[(self.ATTR_PMALIAS),:] = ', '.join(data[self.ATTR_PMALIAS])

    def __query_pm_alias(self):
        '''
        query_pm_alias is used when there are multiple gasbox configs in
        the analysis. It creates additional elements in the trd object to
        allow an opportunity for pms of same config to be compared
        '''
        def __update_trd_obj(dict_data, subsys,**kwargs):
            '''
            Parameter:
            kwargs
              - pmalias (str)
              - module_alias (str)
            '''
            if 'pmalias' in kwargs:
                pmalias = kwargs['pmalias']
                module_alias = self.db[pmalias][self.ATTR_MODULE]
            elif 'module_alias' in kwargs:
                module_alias = kwargs['module_alias']
            module_config, module_alias = self.get_autotrd_config(module_alias)
            if subsys in module_config:
                module_config = module_config[subsys]
            else:
                if self.trd[subsys][self.ZIP_DTYPE] == 'xml':
                    module_config = copy.deepcopy(self.trd[subsys])
                    del module_config[self.ZIP_DATA]
                else:
                    module_config = {}
            dict_data.update(copy.deepcopy(module_config))
            return module_config, module_alias
        def __apply_groupby(groupby=True):
            # <flag> is a flow-control parameter
            flag = False
            #IF THERE IS ONLY ONE GASBOX CONFIG, IGNORE data[self.ATTR_GROUPBY]
            if len(gasbox_ids)==1:
                groupby=False
            for gasbox_idx in gasbox_ids:
                usrcmd = [f"{self.SPEC_CONFIG}=='{module_info}'"]
                # define <attr_trd>
                if data[self.ATTR_GROUPBY] and groupby:
                    usrcmd.append(f"{self.ATTR_GASBOX}=={gasbox_idx}")
                    attr_trd = f'{key}_{module_alias}_{gasbox_idx:02d}'
                else:
                    attr_trd = f'{key}_{module_alias}'
                    flag =True
                usrcmd = ' and '.join(usrcmd)
                pmalias = (self.pm_config.query(usrcmd)
                            .loc[:,self.ATTR_PMALIAS]
                            .values.tolist()) 
                pmalias = [_elem for _elem in pmalias 
                            if _elem in data[self.ATTR_PMALIAS]]
                if any([key in self.db[_elem] for _elem in pmalias]):
                    self.clone = data 
                    tmp[attr_trd] = copy.deepcopy(data)
                    tmp[attr_trd][self.ATTR_PMALIAS] = pmalias
                if flag: break
            if flag:
                pass
            else:
                if key in self.GOLDEN_SUBSYS:
                    __apply_groupby(groupby=False)
        tmp={}
        for key, data in self.trd.items():
            # if a subsystem doesnt contain any modules (i.e., pmalias),
            # it is removed from the self.trd
            if len(data[self.ATTR_PMALIAS])==0: 
                continue
            else: 
                if all([_elem not in self.pm_config[self.ATTR_PMALIAS].tolist() 
                    for _elem in data[self.ATTR_PMALIAS]]):
                    pmalias = data[self.ATTR_PMALIAS][0]
                    _ = __update_trd_obj(data, key, pmalias=pmalias)
                    tmp[key] = data
                else:
                    for module_info, gasbox_ids in self.gasbox_idx.items():
                        module_config, _ = __update_trd_obj(data, key, module_alias=module_info)
                        if module_config:
                            module_alias = module_info
                            module_alias = module_alias[:8]
                            __apply_groupby()
        self.trd = tmp
        self.module_alias = module_alias # PS
        del tmp

    def __run_golden_compare(self):
        '''
        __run_golden_compare is executed when a golden chamber is defined.
        it removes all subsystem reports that do not include the golden_pm,
        which effectively removes all pms with different configs. Then, it
        runs pairwise comparisons between the golden_pm vs all remaining pms
        '''
        tmp={}
        for key, data in self.trd.items():
            flag = True
            if np.any([True if re.search(_elem, key, re.IGNORECASE) else False
                for _elem in self.GOLDEN_SUBSYS]): 
                try:
                    pmaliases = copy.copy(data[self.ATTR_PMALIAS])
                    pmaliases.remove(self.golden_pm)
                    for pmalias in pmaliases:
                        tmp[f'{data[self.ATTR_KEY]}_{pmalias}'] = copy.deepcopy(data) 
                        tmp[f'{data[self.ATTR_KEY]}_{pmalias}'][self.ATTR_PMALIAS] = [self.golden_pm, pmalias]
                except ValueError as err:
                    flag = False
            if flag:
                tmp[key] = self.trd[key]
        self.trd = tmp
        del tmp

    def __ingest_spec(self, sheet_name= 'New_Spec_Format_Setpoint_Calc'):
        '''
        ingest_spec reads the specsheet and creates a pltf_spec object
        '''
        logger.info(f"Parsing {self.specfile}")
        try: 
            speclib = (pd.read_excel(self.specfile, sheet_name= sheet_name, engine = 'openpyxl')
                            .rename(columns={'GasLine_S':self.SPEC_GASIDX})
                            .fillna(''))
            #speclib = (pd.read_excel(self.specfile, sheet_name= sheet_name) # PS EIDH xls 
            #                .rename(columns={'GasLine_S':self.SPEC_GASIDX})
            #                .fillna('')) 
            speclib = speclib[speclib[self.SPEC_APPLLICATION] == self.application ] # filtering selected application
            #logger.info(f'speclib-{speclib}')
            speclib[self.ATTR_AO] = speclib[self.ATTR_AO].apply(lambda _elem: re.sub(r'\s','',f'{_elem}').lower())
            speclib.set_index([self.SPEC_CONFIG, self.SPEC_NAME, self.SPEC_APPLLICATION, self.ATTR_AO], inplace=True) # PS
            self.pltf_spec = speclib.groupby(level=[self.SPEC_CONFIG, self.SPEC_NAME, self.SPEC_APPLLICATION]) # PS
            # logger.info(f"self.pltf_spec content {self.pltf_spec.groups.keys()}")
            #speclib.to_excel('pltf_spec.xlsx') # PS
        except Exception as err:
            logger.error(f"{err}")
            
        logger.info(f"Parsing {self.specfile_A3}")
        try: 
            speclib_A3 = pd.read_excel(self.specfile_A3, sheet_name= 0, engine = 'openpyxl').fillna('')
            #speclib_A3 = pd.read_excel(self.specfile_A3, sheet_name= 0).fillna('') # PS EIDH xls 
            speclib_A3 = speclib_A3[speclib_A3[self.SPEC_APPLLICATION] == self.application ] # filtering selected application
            attrs = ['ModelName','ModelSpec','ModelType']
            self.speclib_A3 = speclib_A3.drop( attrs, axis = 1 )
            
            # logger.info(f"self.speclib_A3 content {self.speclib_A3}")
            # self.speclib_A3.to_excel("speclib_A3.xlsx")
        except Exception as err:
            logger.error(f"{err}")            

    def __query_last_entry(self):
        '''
        query_last_entry removes all duplicated records of 
        a specific parameter except the latest entry.
        Note:
        This function operates on self.df
        '''  
        dtfmt = ['year', 'month', 'day', 'hour', 'minute', 'second']
        #logger.info( f'self.df: {self.df}' )
        # PS Drop self.ATTR_TIME if NA for Kiyo GX Bias_Voltage_Probe
        try:
            if any( pd.isna( self.df[ self.ATTR_TIME ] ) ) : 
                self.df.drop( [ self.ATTR_TIME ], axis = 1, inplace = True )
        except:
            pass
        if self.ATTR_TIME not in self.df:
            logger.debug( f' not: {self.df}' )
            attr_dt = [ _elem for _elem in self.df.columns 
                        if re.search(self.ATTR_TIME,_elem, re.IGNORECASE)]
            self.df[self.ATTR_TIME] = self.df[attr_dt[0]]
            # Remove NA time related columns
            for attr in attr_dt: 
                if any( pd.isna( self.df[ attr ] ) ) : 
                    attr_dt.remove(attr)
            self.df[self.ATTR_TIME] = self.df[attr_dt[0]] 
        # Filter out empty time values                          
        self.df[self.ATTR_TIME] = self.df[self.ATTR_TIME].apply(lambda _elem:\
            datetime.datetime(**dict(list(zip( dtfmt, _elem)))))    
        self.df = (self.df.sort_values(by=self.ATTR_TIME)
                .groupby(level=[_elem for _elem in range(self.df.index.nlevels)])
                .tail(1))   

    def __is_iter_instance(self, pyobj):
        return hasattr(pyobj, '__iter__') and not isinstance(pyobj, str) 

    def __run_preprocess(self):
        '''
        run_preprocess (1) creates a multi-index dataframe to organize
        data according to the pm_config, subsystem, pmidx & other labels
        (2) keep the latest entry for each index
        '''
        self.df = self.df.T
        self.df[self.SPEC_NAME] = self.subsys
        reObj = re.compile(r'^(\S+?)_\d{3}$')
        self.df.index = [reObj.search(_elem).group(1) if reObj.search(_elem) else _elem for _elem in self.df.index ]
        
        self.df.index.name = self.ATTR_PMALIAS
        #logger.info( f'df_runpreprocess: {self.df}' )
        if not re.search('pm', self.df.index[0], re.IGNORECASE): # PS change pmalias in self.pm_config to merge with self.df
            self.pm_config_tmp = copy.deepcopy(self.pm_config)
            if self.pm_config_tmp.ModInfo_Concat.str.contains('pm').any(): # PS
                # filter out ModInfo_Concat 不包含'Strip'
                self.pm_config_tmp = self.pm_config_tmp[ ~self.pm_config_tmp.ModInfo_Concat.str.contains('Strip') ]  
            # check if row of self.pm_config_tmp > self.df; if not, duplicate itself.
            if self.pm_config_tmp.shape[0] < self.df.shape[0]:
                times = np.ceil( self.df.shape[0]/self.pm_config_tmp.shape[0] ) 
                self.pm_config_tmp = self.pm_config_tmp.loc[ self.pm_config_tmp.index.repeat( [times] ) ].reset_index()
            self.pm_config_tmp = self.pm_config_tmp.iloc[ :self.df.shape[0], : ]
            self.pm_config_tmp[self.ATTR_PMALIAS] = self.df.index        
            self.df = self.df.merge(self.pm_config_tmp, left_index=True, right_on=self.ATTR_PMALIAS)
        else:
            self.df = self.df.merge(self.pm_config, left_index=True, right_on=self.ATTR_PMALIAS) 
        # TO MAKE THIS CODE MORE FLEXIBLE, WE CREATE AUGMENTED FEATURES 
        if self.SPEC_GASIDX in self.subsys_obj[self.SPEC_INFO].keys():
            self.df[self.SPEC_GASIDX] = ['UnknownGas' if pd.isna(_elem)
                else 'Gas{:02}'.format(int(_elem[-1]))
                if re.search(r'^gas(\d)$',_elem, re.IGNORECASE)
                else _elem for _elem in self.df[self.SPEC_GASIDX]]
            self.df = self.df[self.df[self.SPEC_GASIDX] != 'UnknownGas']    
        self.__query_misg_info()
        self.df.set_index([self.SPEC_CONFIG, self.SPEC_NAME, self.ATTR_PMALIAS], inplace=True)                 
        self.spec_info = []
        for attr in self.subsys_obj[self.SPEC_INFO].keys():
            try:
                self.df.set_index(attr, append=True, inplace=True)
            except TypeError :
                self.spec_info.append(attr)                     
        # IF THERE ARE MULTIPLE RECORDS OF THE SAME MEASUREMENTS, KEEP THE LAST RECORD
        self.__query_last_entry()

    def __run_A3_config(self, key): # PS
        '''
        run_ 
        '''
        if re.search('PM_ConfigOpt', key, re.IGNORECASE):  # Calcualte CV mistach numbers
            if re.search( '[A-Z]$', key, re.IGNORECASE) : # PM_ConfigOpt_KiyoFX_K
                diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL
                for pm in self.pmalias_new:
                    if pm in self.cv_mismatch.columns:
                        self.cv_mismatch[pm].update( self.cv_mismatch[pm] + diff )
                    else:
                        self.cv_mismatch[pm] = diff 
                self.df_A3 = self.cv_mismatch
                self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3   
                
        elif re.search('PM_CV', key, re.IGNORECASE): # 
            if re.search( '[A-Z]$', key, re.IGNORECASE) : # PM_ConfigOpt_KiyoFX_K
                tmp = copy.deepcopy(self.df)
                tmp = tmp[ tmp.columns[ tmp.columns.str.contains('PM') ]  ]  
                
                tmp = tmp.filter(regex='WaferTransferPressure|BiasElectrodeHousingTempSetpoint', axis=0)
                tmp = tmp.reset_index( drop = False )
                tmp.rename( columns = {'id00': 'Variable'}, inplace = True )
                # Calcualte CV mistach numbers
                #if self.golden_pm:
                diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL
                for pm in self.pmalias_new:
                    if pm in self.cv_mismatch.columns:
                        self.cv_mismatch[pm].update( self.cv_mismatch[pm] + diff )
                    else:
                        self.cv_mismatch[pm] = diff
                #else:
                #    self.cv_mismatch = pd.DataFrame()
                self.df_A3 = pd.concat( [tmp, self.cv_mismatch], axis = 0 )
                self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3     

        elif re.search('Port', key, re.IGNORECASE): # Determine number of load port for AtmArm
            tmp = copy.deepcopy(self.df)
            tmp = tmp[ tmp.columns[ tmp.columns.str.contains('Port') ]  ]  
            self.number_port = tmp.shape[1]
            # Calcualte CV mistach numbers
            diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL
            for pm in self.pmalias_new:
                if pm in self.cv_mismatch.columns:
                    self.cv_mismatch[pm].update( self.cv_mismatch[pm] + diff )
                else:
                    self.cv_mismatch[pm] = diff 
            self.df_A3 = self.cv_mismatch
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3   
            #logger.info( f'self.df_A3-{self.df_A3}' )  # PS 

        elif re.search('AtmArm', key, re.IGNORECASE):
            tmp = copy.deepcopy(self.df)
            tmp = tmp[ tmp.columns[ tmp.columns.str.contains('AtmArm') ]  ]  
            tmp = tmp.filter(regex='AirLock[12]PostPosition$', axis=0)
            # Change index name based on number of load port
            tmp.index = tmp.index + f'_P{self.number_port}'
            #logger.info(f'tmp-{tmp}; tmp.index')            
            tmp = tmp.reset_index( drop = False )
            tmp.rename( columns = {'id00': 'Variable'}, inplace = True )
            #extend columns to all chambers
            needed_cols = list( tmp.columns[ tmp.columns.str.contains('AtmArm') ] )
            tmp = tmp[ ['Variable'] + needed_cols ]    
            if self.golden_pm:   # self.golden_pm
                golden_tool = self.golden_pm.split('_')[0]
                col_name = tmp.columns[ tmp.columns.str.contains(golden_tool) ][0]
                tmp.rename( columns = { col_name: self.golden_pm }, inplace = True )
                for pm in self.pmalias_new:
                    tmp[pm] = tmp[ needed_cols[-1] ]
                tmp.drop( columns = needed_cols[-1], inplace=True )
            else: 
                for pm in self.pmalias_new:
                    tmp[pm] = tmp[ needed_cols[-1] ]
                tmp.drop( columns = needed_cols[-1], inplace=True )
            # Calcualte CV mistach numbers
            diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL
            for pm in self.pmalias_new:
                if pm in self.cv_mismatch.columns:
                    self.cv_mismatch[pm].update( self.cv_mismatch[pm] + diff )
                else:
                    self.cv_mismatch[pm] = diff      
            #logger.info( f'self.cv_mismatch-{self.cv_mismatch}' )     
            #logger.info( f'tmp-{tmp} ' )      
            self.df_A3 = pd.concat( [tmp, self.cv_mismatch], axis = 0 )                
            #logger.info( f'self.df_A3-{self.df_A3}' )  # PS  
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('TM_CV$', key, re.IGNORECASE): # TM_CV for Kiyo GX
            tmp = copy.deepcopy(self.df)
            tmp = tmp[ tmp.columns[ tmp.columns.str.contains('TransferChamber') ]  ]  
            tmp = tmp.filter(regex='LocalDynamicAlignmentPM', axis=0)
            # Add new index 'LocalDynamicAlignment' after checking all TRUE for LocalDynamicAlignmentPMx
            tmp.loc['LocalDynamicAlignment'] = tmp.apply(lambda col: 'TRUE' if all(col == True)  else 'FALSE' )
            # Change index name
            tmp = tmp.reset_index( drop = False )
            tmp.rename( columns = {'id00': 'Variable'}, inplace = True )
            #extend columns to all chambers
            needed_cols = list( tmp.columns[ tmp.columns.str.contains('TransferChamber') ] )
            tmp = tmp[ ['Variable'] + needed_cols ]    
            if self.golden_pm:   # self.golden_pm
                golden_tool = self.golden_pm.split('_')[0]
                col_name = tmp.columns[ tmp.columns.str.contains(golden_tool) ][0]
                tmp.rename( columns = { col_name: self.golden_pm }, inplace = True )
                for pm in self.pmalias_new:
                    tmp[pm] = tmp[ needed_cols[-1] ]
                tmp.drop( columns = needed_cols[-1], inplace=True )
            else: 
                for pm in self.pmalias_new:
                    tmp[pm] = tmp[ needed_cols[-1] ]
                tmp.drop( columns = needed_cols[-1], inplace=True )
            # Calcualte CV mistach numbers
            diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL
            for pm in self.pmalias_new:
                if pm in self.cv_mismatch.columns:
                    self.cv_mismatch[pm].update( self.cv_mismatch[pm] + diff )
                else:
                    self.cv_mismatch[pm] = diff            
            self.df_A3 = pd.concat( [tmp, self.cv_mismatch], axis = 0 )                
            #logger.info( f'self.df_A3-{self.df_A3}' )  # PS  
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('_CV', key, re.IGNORECASE):  # Calcualte CV mistach numbers
            diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL         
            for pm in self.pmalias_new:
                if pm in self.cv_mismatch.columns:
                    self.cv_mismatch[pm].update( self.cv_mismatch[pm] + diff )
                else:
                    self.cv_mismatch[pm] = diff 
            self.df_A3 = self.cv_mismatch
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3   
            #logger.info( f'self.df_A3-{self.df_A3}' )  # PS                 
            
        elif re.search('GasBox_Config', key, re.IGNORECASE):
            if self.golden_pm:
                tmp = {'Variable': ['Gas_diff'], self.golden_pm: '' } # 'GasBox': [ '' ]
                tmp = pd.DataFrame( tmp ) 
                diff = sum( self.df[ self.ATTR_MATCH ] == False )  # self.ATTR_ISNULL
                for pm in self.pmalias_new:
                    tmp[pm] = diff       
                self.df_A3 = tmp
                self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3   
                #logger.info( f'self.df_A3-{tmp}' )  # PS
        elif re.search('AbsoluteFlowVerifier', key, re.IGNORECASE):  # PS add for AFV
            # test PS
            tmp = copy.deepcopy(self.df)            
            #logger.info( f'tmp1-{tmp}' )   
            tmp = tmp.filter(regex='Pass', axis=0)    
            tmp = tmp.reset_index( drop = False )
            tmp.rename( columns = {'id00': 'Variable'}, inplace = True )
            needed_cols = list( tmp.columns[ tmp.columns.str.contains('PM') ] )
            #logger.info( f'tmp2-{tmp}' )
            self.df_A3 = tmp[ ['Variable'] + needed_cols ]            
            #logger.info( f'self.df_A3-{self.df_A3}' )  # PS  
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3
        else:
            pass
        logger.info(f'df_A3: {self.df_A3}')
            
    def __run_A3_SSHC(self, key): # PS
        '''
        run_ 
        '''
        if re.search('Ten_PT_Cal|AFV_Ten_PT', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS, self.SPEC_GASIDX ]   
            needed_cols = [ '_10', '_50', '_90' , '_100', '_10_abs', '_50_abs', '_90_abs' , '_100_abs' , '_MaxError' , '_20' , '_30' , '_40' , '_60' , '_70' , '_80'] # PS add for F20 Kiyo FX
            try: 
                if re.search( 'Micro', key, re.IGNORECASE): #Ten_PT_Cal_Microwav
                # change some column names to add_S # Gas02_50_S
                    needed_cols = [ '_10', '_50', '_90' , '_100', '_10_abs', '_50_abs', '_90_abs' , '_100_abs' , '_MaxError' ]
                    df1 = tmp.drop(columns = needed_cols)
                    df2 = tmp.filter(needed_cols).add_suffix('_S')
                    tmp = pd.concat( [df1, df2], axis = 1 )
                    needed_cols = [ col+'_S'  for col in needed_cols ]
                # tmp1 
                tmp1 = tmp[ columns + needed_cols ]
                tmp1 = pd.melt(tmp1, id_vars= columns , value_vars= needed_cols  )
                tmp1['Variable'] = tmp1[self.SPEC_GASIDX] + tmp1['variable'] 
                tmp1 = tmp1.drop( [ self.SPEC_GASIDX, 'variable' ], axis = 1 )
                # tmp2 AllGas_MaxError
                def AbsMax_list(x):
                    return max( [ abs(_elem) for _elem in x ] )
                tmp['value'] = tmp['Error'].apply( AbsMax_list ).copy()
                tmp2 = tmp.groupby([self.SPEC_NAME, self.ATTR_PMALIAS]).max( 'value' ).reset_index()
                tmp2['Variable'] = 'AllGas_MaxError'
                # tmp3 Gas08Gas10Gas13Gas14_MaxError for KiyoFXE CMG-ME
                tmp3 = tmp[ tmp[self.SPEC_GASIDX].isin(['Gas08', 'Gas10', 'Gas13', 'Gas14']) ].copy()
                tmp3['value'] = tmp3['Error'].apply( AbsMax_list )
                tmp3 = tmp3.groupby([self.SPEC_NAME, self.ATTR_PMALIAS]).max( 'value' ).reset_index()
                tmp3['Variable'] = 'Gas08Gas10Gas13Gas14_MaxError'            
                # tmp4 Gas12Gas13_MaxError for Flex FL MD1
                tmp4 = tmp[ tmp[self.SPEC_GASIDX].isin(['Gas12', 'Gas13']) ].copy()
                tmp4['value'] = tmp4['Error'].apply( AbsMax_list )
                tmp4 = tmp4.groupby([self.SPEC_NAME, self.ATTR_PMALIAS]).max( 'value' ).reset_index()
                tmp4['Variable'] = 'Gas12Gas13_MaxError'              
                # tmp5 Gas03Gas15_MaxError for Flex FL MD1
                tmp5 = tmp[ tmp[self.SPEC_GASIDX].isin(['Gas03', 'Gas15']) ].copy()
                tmp5['value'] = tmp5['Error'].apply( AbsMax_list )
                tmp5 = tmp5.groupby([self.SPEC_NAME, self.ATTR_PMALIAS]).max( 'value' ).reset_index()
                tmp5['Variable'] = 'Gas03Gas15_MaxError'  
                # tmp6 Gas08Gas13Gas14Gas15_MaxError for KiyoFXE HMO
                tmp6 = tmp[ tmp[self.SPEC_GASIDX].isin(['Gas08', 'Gas13', 'Gas14', 'Gas15']) ].copy()
                tmp6['value'] = tmp6['Error'].apply( AbsMax_list )
                tmp6 = tmp6.groupby([self.SPEC_NAME, self.ATTR_PMALIAS]).max( 'value' ).reset_index()
                tmp6['Variable'] = 'Gas08Gas13Gas14Gas15_MaxError'     
                
                tmp = pd.concat( [tmp1, tmp2, tmp3, tmp4, tmp5, tmp6], axis = 0 )
                tmp = tmp.pivot( [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('AFV2_Compensation', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS, self.SPEC_GASIDX ]   
            try: 
                # tmp1 AllGas_MaxError
                def AbsMax_list(x):
                    return max( [ abs(_elem) for _elem in x ] )
                tmp['value'] = tmp['AcceptedCompensationError'].apply( AbsMax_list ).copy()
                tmp1 = tmp.groupby([self.SPEC_NAME, self.ATTR_PMALIAS]).max( 'value' ).reset_index()
                tmp1['Variable'] = 'AllGas_MaxError'
                
                tmp = pd.concat( [tmp1], axis = 0 )
                tmp = tmp.pivot( [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('Linearity', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            if re.search('TCP', key, re.IGNORECASE):
                needed_cols = ['MaxError_TCPRF', 'Slope', 'TCP_100W', 'TCP_200W', 'TCP_300W', 'TCP_400W', 'TCP_500W', 'TCP_600W', 'TCP_700W', 'TCP_800W', 'TCP_900W', 'TCP_1000W', 'TCP_1100W', 'TCP_1200W', 'TCP_1300W', 'TCP_1400W'] 
                #if re.search('Outer', key, re.IGNORECASE): # Solo
                #    needed_cols.append( 'OuterSlope_TCPRF' )
                #else:
                #    needed_cols.append( 'InnerSlope_TCPRF' ) 
            elif re.search('Bias_Linearity', key, re.IGNORECASE):
                needed_cols = ['MaxError_BiasRF', 'MaxError_6_25_50_80_BiasRF', 'MeterReadings_6_BiasRF', 'Pass_BiasRF', 'Bias_100W', 'Bias_200W', 'Bias_300W', 'Bias_400W', 'Bias_500W', 'Bias_600W', 'Bias_700W', 'Bias_800W', 'Bias_900W', 'Bias_1000W', 'Bias_1100W', 'Bias_1200W', 'Bias_1300W', 'Bias_1400W' ]
            elif re.search('Bias2_Linearity', key, re.IGNORECASE):
                needed_cols = ['MaxError_Bias2RF', 'Pass_Bias2RF']
            elif re.search('[\d]+MHzRFLinearity', key, re.IGNORECASE):     
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('MaxError') ] ) 
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
                #logger.info( f'self.df_A3-{tmp}' )  # PS
            except KeyError: 
                tmp = pd.DataFrame()   
            # PS rename "One-Shot-Macro Name" column 
            tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)  
            self.df_A3 = tmp   
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3
            
        elif re.search('Valve_Conductance', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 

            needed_cols = []
            if any( tmp.columns.str.contains('ConductancesAt.*_') ): # Felx FL
                needed_cols.extend( [ 'ConductancesAtUp_1000', 'ConductancesAtDown_1000', 'ConductancesAtUp_400', 'ConductancesAtDown_400' ] )
            if any( tmp.columns.str.contains('Conductance_slope') ):  # KiyoFX, KiyoCX, KiyoEX, KiyoGX, Solo
                tmp_len = len( tmp.loc[0, 'Slope'] ) 
                needed_cols.extend( [ 'Conductance_slope'+str(i+1) for i in range(tmp_len) ] )
            logger.info(f'needed_cols-{needed_cols}')  # PS
            try: 
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()      
            logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp               
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('WAP_Conductance', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            needed_cols = [ 'Conductance_Knee', 'Conductance_Knee_Poistion', 'CurrentConductance.CalculatedOffset', 'Conductance_Comment' ]
            try: 
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()    
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp               
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3
            
        elif re.search('VC_At_Up', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            needed_cols = [ 'VC_At_Up_65' ]
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                  
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp               
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('VC_At_Down', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            needed_cols = [ 'VC_At_Down_65' ]
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                  
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp               
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3
            
        elif re.search('TCCT_NoPlasmaTest', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            tmp_len = len( set(tmp.loc[0, 'Setpoints'] ) )
            needed_cols = [ 'TCCT_slope'+str(i+1) for i in range(tmp_len) ]
            tmp_col = list( tmp.columns[ tmp.columns.str.contains('TCCT_SP') ] )
            needed_cols.extend( tmp_col)
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                  
            logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('Fixed.*GasOrifice', key, re.IGNORECASE): # FixedOrificeCurrent  => FixedTriGasOrifice
            tmp = self.df.reset_index()
            logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            tmp_len = len( tmp.loc[0, 'TestNumber']  )
            attr = key.split('_')[0]
            needed_cols = [ f'{attr}{str(i+1)}' for i in range(tmp_len) ]
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                  
            logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3            

        elif re.search('GapDriveCalibration', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            needed_cols = [ 'Average', 'GapOffset', 'Pitch', 'Roll', 'PosAtCenter', 'PosAt3', 'PosAt6', 'PosAt9', 'PosAt12' ] # Use Roll for "PosAt9-PosAt3" and Pitch for "PosAt6-PosAt12"
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()              
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.df_A3 = tmp               
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('Bias.*Voltage_Probe', key, re.IGNORECASE):
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS]
            if 'Slope' in tmp.columns: 
                needed_cols = [ 'Slope', 'Rsquared' ]
            else: # Solo; Kiyo GX
                needed_cols =['cwDict.Slope', 'cwDict.Rsquared', 'pulseDict.Slope' , 'pulseDict.Rsquared' ]
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                
            # PS rename "One-Shot-Macro Name" column 
            tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)                  
            self.df_A3 = tmp       
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

            
        elif re.search('TCP.*VProbe', key, re.IGNORECASE): # Solo
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS]
            needed_cols =['rSeriesDict.slope' ]
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()
            tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)  # PS                                    
            self.df_A3 = tmp       
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('TCP_JunctionBoxTune', key, re.IGNORECASE): # Solo
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS]
            needed_cols =['TCPInnerFineTuneAI', 'TCPOuterFineTuneAI' ]
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                  
            self.df_A3 = tmp       
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('CorvusCalibration', key, re.IGNORECASE): 
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS] 
            tmp_len = len( set(tmp.loc[0, 'verification.name'] ) )            
            needed_cols = [ 'Verification'+ str(i+1)+'_range_calculated' for i in range(tmp_len) ] # 'Verification'+ str(i+1)+'_range_calculated'
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()              
            self.df_A3 = tmp 
            #logger.info( f'self.df_A3-{tmp}' )  # PS
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3
            
        elif re.search('^PartialPressure', key, re.IGNORECASE):  
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS           
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS, 'GasLine', 'GasName' ] # PS
            needed_cols = [ 'R2' ] 
            try:
                '''
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols  )
                tmp['Variable'] = 'Pressure_' + tmp['variable'] + '_' + tmp['GasLine']  +  '_' + tmp['GasName']  # PS  Pressure_R2_NF3-500 => Pressure_R2_Gas13_NF3-500
                tmp = tmp.drop( [ 'variable' ], axis = 1 )
                #logger.info( f'tmp2-{tmp}' )  # PS  
                tmp = tmp.pivot( [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
                '''
                #tmp1
                tmp1 = tmp[ columns + needed_cols ]
                tmp1 = pd.melt(tmp, id_vars= columns , value_vars= needed_cols  )
                tmp1['Variable'] = 'Pressure_' + tmp1['variable'] + '_' + tmp1['GasLine']  +  '_' + tmp1['GasName']  # PS  Pressure_R2_NF3-500 => Pressure_R2_Gas13_NF3-500
                tmp1 = tmp1.drop( [ 'variable' ], axis = 1 )
                logger.info(f'tmp1-{tmp1}')
                #tmp2
                tmp2 = tmp.groupby([self.SPEC_NAME, self.ATTR_PMALIAS])['R2'].min().reset_index()
                tmp2['value'] = tmp2['R2'].apply(lambda x: 'Pass' if x > 0.9 else 'Failed')
                tmp2['Variable'] = 'All_R2'
                logger.info(f'tmp2-{tmp2}')                

                tmp = pd.concat( [tmp1, tmp2], axis = 0 )
                tmp = tmp.pivot( [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()      
            self.df_A3 = tmp 
            #logger.info( f'self.df_A3_new-{tmp}' )  # PS           
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        elif re.search('VoDMPartialPressure', key, re.IGNORECASE):  
            tmp = self.df.reset_index()
            #logger.info( f'self.df_A3-{tmp}' )  # PS           
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS, 'GasLine', 'GasName' ] # PS
            needed_cols = [ 'state' ] 
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()     
            self.df_A3 = tmp       
            #logger.info( f'self.df_A3_new-{tmp}' )  # PS 
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3               

        elif re.search('VATAutoLearn', key, re.IGNORECASE):  # Add for F20 Kiyo FX
            tmp = self.df.reset_index()
            logger.info(f'tmp-{tmp}')
            columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ] # PS
            needed_cols = ['Ar_check', 'Ar_flow_check']
            try:
                tmp = tmp[ columns + needed_cols ]
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
            except KeyError: 
                tmp = pd.DataFrame()                  
            self.df_A3 = tmp       
            #logger.info( f'self.df_A3_new-{tmp}' )  # PS 
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3

        if re.search('^ZTM|Solo_TM', key, re.IGNORECASE): 
            
            tmp = self.df.reset_index() 
            if re.search('ZTM22_|ZTM23_|ZTM24_|ZTM82_|ZTM68_|ZTM_N2_22_|ZTM_N2_23_|ZTM_N2_24_', key, re.IGNORECASE):     
                if re.search("PressureCheck", key, re.IGNORECASE):      # for F22 solo
                    # needed_cols = ['GasVacuumSystemPressureReaderManagerPressure']
                    needed_cols = ['VTM_Press_AI_TransferChamber', 'Press_AI_AirLock1', 'Press_AI_AirLock2']
                elif re.search("40mt", key, re.IGNORECASE): #20250919Debug for ZTM23
                    needed_cols = list(tmp.columns[tmp.columns.str.contains('Press')])    
                elif re.search("solo", key, re.IGNORECASE):
                    needed_cols = ['GasVacuumSystemPressureReaderManagerPressure_TransferChamber', 'Press_AI_AirLock1', 'Press_AI_AirLock2']
                else:
                    needed_cols = list(tmp.columns[tmp.columns.str.contains('PressureReading')])
                    # needed_cols = [ 'PressureReading', 'AL1_PressureReading', 'AL2_PressureReading' ]
                # needed_cols = [ 'PressureReading', 'AL1_PressureReading', 'AL2_PressureReading',
                #                 'GasVacuumSystemPressureReaderManagerPressure', 'Press_AI_AirLock1', 'Press_AI_AirLock2']
                logger.info( f'self.df_A3-{tmp}' )  # PS   
                columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                try:
                    tmp = tmp[ columns + needed_cols ].copy()
                    tmp_new = pd.DataFrame()
                    if self.golden_pm:  # 如果有golden, golden update pmalias, non-golden產生n rows
                        # Gloden
                        tmp1 = tmp[ tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        tmp1[self.ATTR_PMALIAS ] = self.golden_pm
                        # Others
                        tmp2 = tmp[ ~tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp2[ tmp2[self.ATTR_PMALIAS].str.contains( tool+'_' )  ].copy() # f'({tool})_'
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )                              
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                         
                        tmp_new = pd.concat( [tmp1, tmp_new], axis = 0 ) # 
                        #logger.info( f'tmp_new0-{tmp_new}' ) 
                    else: # non-golden case
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp[ tmp[self.ATTR_PMALIAS].str.contains(tool+'_')  ]
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )   
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                      
                    logger.info( f'tmp_new-{tmp_new}' )  # PS 
                    tmp = tmp_new
                    tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                    tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                    tmp = tmp.reset_index() 
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)
                except KeyError as err:
                    logger.info(f'err-{err}')
                    tmp = pd.DataFrame()                     
                    
            elif re.search('ZTM25_|ZTM_N2_25_', key, re.IGNORECASE):     
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('LeakRate') ] ) 
                # needed_cols = [ 'ChamberLeakRate_TransferChamber', 'ChamberLeakRate_AirLock1', 'ChamberLeakRate_AirLock2', 'LeakRatePolicyLastLeakRate_TransferChamber', 'LeakRatePolicyLastLeakRate_AirLock1' ]
                #logger.info( f'self.df_A3-{tmp}' )  # PS   
                try: 
                    columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                    tmp = tmp[ columns + needed_cols ].copy()
                    tmp_new = pd.DataFrame()                    
                    if self.golden_pm: # 如果有golden, golden update pmalias, non-golden產生n rows
                        # Gloden
                        tmp1 = tmp[ tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        tmp1[self.ATTR_PMALIAS ] = self.golden_pm 
                        # Others
                        tmp2 = tmp[ ~tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp2[ tmp2[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )                              
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                             
                        tmp_new = pd.concat( [tmp1, tmp_new], axis = 0 ) # 
                    else:
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp[ tmp[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )   
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                            
                    #logger.info( f'tmp_new-{tmp}' )  # PS                 
                    tmp = tmp_new
                    tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                    tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                    tmp = tmp.reset_index() 
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)                
                except KeyError as err:
                    logger.info(f'err-{err}')
                    tmp = pd.DataFrame() 

            elif re.search('ZTM27_|ZTM_N2_27_', key, re.IGNORECASE):     
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('Time') ] ) 
                # needed_cols = [ 'PumpDownElapsedTime_AirLock1', 'VentElapsedTime_AirLock1', 'PumpDownElapsedTime_AirLock2', 'VentElapsedTime_AirLock2' ]
                columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                logger.info( f'self.df_A3-{tmp}' )  # PS 
                try:                                 
                    tmp = tmp[ columns + needed_cols ].copy()
                    tmp_new = pd.DataFrame()
                    if self.golden_pm:  # 如果有golden, golden update pmalias, non-golden產生n rows
                        # Gloden
                        tmp1 = tmp[ tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        tmp1[self.ATTR_PMALIAS ] = self.golden_pm 
                        # Others
                        tmp2 = tmp[ ~tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp2[ tmp2[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )                              
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #    
                        tmp_new = pd.concat( [tmp1, tmp_new], axis = 0 )
                    else:
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp[ tmp[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )   
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                        
                    logger.info( f'tmp_new-{tmp}' )  # PS                 

                    tmp = tmp_new
                    tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                    tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                    tmp = tmp.reset_index() 
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)                
                except KeyError as err:
                    logger.info(f'err-{err}')
                    tmp = pd.DataFrame() 

            elif re.search('ZTM90_', key, re.IGNORECASE):     
                needed_cols = ['mainframe_base_pressure', 'AL1_vent_time', 'AL1_pump_time', 'AL2_vent_time', 'AL2_pump_time']
                columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                logger.info( f'self.df_A3-{tmp}' )  # PS 
                try:                                 
                    tmp = tmp[ columns + needed_cols ].copy()
                    tmp_new = pd.DataFrame()
                    if self.golden_pm:  # 如果有golden, golden update pmalias, non-golden產生n rows
                        # Gloden
                        tmp1 = tmp[ tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        tmp1[self.ATTR_PMALIAS ] = self.golden_pm 
                        # Others
                        tmp2 = tmp[ ~tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp2[ tmp2[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )                              
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #    
                        tmp_new = pd.concat( [tmp1, tmp_new], axis = 0 )
                    else:
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp[ tmp[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )   
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                        
                    logger.info( f'tmp_new-{tmp}' )  # PS                 

                    tmp = tmp_new
                    tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                    tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                    tmp = tmp.reset_index() 
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)                
                except KeyError as err:
                    logger.info(f'err-{err}')
                    tmp = pd.DataFrame() 

            elif re.search('ZTM26_|ZTM28_|ZTM43_|ZTM50_|ZTM51_|ZTM101_|ZTM58_|ZTM60_|ZTM69_|ZTM78_|ZTM87_|ZTM88_|ZTM89_|ZTM505_|ZTM507_|ZTM518_|ZTM517_|ZTM522_|ZTM523_|ZTM539_|ZTM540_|ZTM541_|ZTM542_|Solo_TM_N2_CDA|Solo_TM_AL_leak|Solo_TM_AL_PumpVent|Solo_TM_AL_PressureDelta|ZTM_N2_58_|ZTM_N2_59_', key, re.IGNORECASE):  # comments
                attr_key = key.split('_')[1] 
                needed_cols = list( tmp.columns[ tmp.columns.str.contains(attr_key) ] ) 
                logger.info( f'self.df_A3-{tmp}' )  # PS   
                columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                try:
                    tmp = tmp[ columns + needed_cols ].copy()
                    tmp_new = pd.DataFrame()
                    if self.golden_pm:  # 如果有golden, golden update pmalias, non-golden產生n rows
                        # Gloden
                        tmp1 = tmp[ tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        logger.info( f'tmp1-{tmp1}; self.golden_pm-{self.golden_pm}' )
                        tmp1[self.ATTR_PMALIAS ] = self.golden_pm
                        # Others
                        tmp2 = tmp[ ~tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp2[ tmp2[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )                              
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #                                  
                        tmp_new = pd.concat( [tmp1, tmp_new], axis = 0 )
                    else:
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp[ tmp[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            pm_list = [ i for i in self.pmalias_new if i.startswith(tool+'_') ]
                            df_tmp_new= pd.concat( [df_tmp]*len(pm_list), axis = 0 ).reset_index( drop = True ) # pmalias extend to pm_list
                            df_tmp_new[self.ATTR_PMALIAS ].update( pm_list )   
                            if tmp_new.empty :
                                tmp_new = df_tmp_new
                            else:
                                tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #
                    logger.info( f'tmp_new-{tmp_new}' )  # PS 
                    tmp = tmp_new
                    tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                    tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                    tmp = tmp.reset_index() 
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True) 
                except KeyError as err:
                    logger.info(f'err-{err}')
                    tmp = pd.DataFrame()   
    
            elif re.search('ZTM59_|ZTM91_|ZTM129_|ZTM130_|ZTM140_|ZTM157_|ZTM329_|ZTM506_', key, re.IGNORECASE): # Generic Module error?
                attr_key = '_'.join( key.split('_')[1:] )  # key.split('_')[1] 
                needed_cols = list( tmp.columns[ tmp.columns.str.contains( attr_key ) ] ) # TMandChamberPressureDiffWhenSlotDoorOpen_PMx
                logger.info( f'self.df_A3-{tmp}' )  # PS   
                columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                try:
                    tmp = tmp[ columns + needed_cols ].copy() 
                    tmp_new = pd.DataFrame()
                    if self.golden_pm:  # 如果有golden, golden update pmalias, non-golden產生n rows
                        # Golden         
                        tmp1 = tmp[ tmp[self.ATTR_PMALIAS].str.contains( self.golden_pm.split('_')[0]+'_' ) ].copy()
                        pm = self.golden_pm.split('_')[-1].replace( '(golden)', '' ) # PM1
                        column_pm =  list( tmp1.columns[ tmp1.columns.str.contains( pm )] )
                        tmp1 = tmp1[ columns + column_pm ]  
                        tmp1[self.ATTR_PMALIAS ] = self.golden_pm 
                        tmp1.rename( columns = { column_pm[0] : f'{attr_key}1'}, inplace = True )                        
                        # Others
                        tmp2 = tmp[ ~tmp[self.ATTR_PMALIAS].str.contains('golden') ]
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp2[ tmp2[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            for pm in chambers:
                                col_pm = [col for col in needed_cols if col.endswith(pm) ]                                            
                                df_tmp_new = df_tmp[ columns + col_pm ].copy()
                                pmalias = tool + '_' + pm
                                df_tmp_new[self.ATTR_PMALIAS] = pmalias # update
                                df_tmp_new.rename( columns = { col_pm[0] : f'{attr_key}1'}, inplace = True )
                                if tmp_new.empty :
                                    tmp_new = df_tmp_new
                                else:
                                    tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #
                        tmp_new = pd.concat( [tmp1, tmp_new], axis = 0 ).reset_index( drop = True ) #   
                        logger.info(f'tmp_new-{tmp_new}')  
                    else: 
                        for tool, chambers in self.tool_pm.items(): 
                            df_tmp = tmp[ tmp[self.ATTR_PMALIAS].str.contains(tool+'_')  ].copy()
                            for pm in chambers:
                                col_pm = [col for col in needed_cols if col.endswith(pm) ]                                            
                                df_tmp_new = df_tmp[ columns + col_pm ].copy()
                                pmalias = tool + '_' + pm
                                df_tmp_new[self.ATTR_PMALIAS] = pmalias
                                logger.info(f'df-tmp-new: {df_tmp_new}')
                                df_tmp_new.rename( columns = { col_pm[0] : f'{attr_key}1'}, inplace = True )
                                if tmp_new.empty :
                                    tmp_new = df_tmp_new
                                else:
                                    tmp_new = pd.concat( [tmp_new, df_tmp_new], axis = 0).reset_index( drop = True ) #
                    #logger.info( f'tmp_new-{tmp_new}' )  # PS 
                    tmp = tmp_new
                    tmp = pd.melt(tmp, id_vars= columns , value_vars= [ f'{attr_key}1' ], var_name='Variable'  )
                    tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                    tmp = tmp.reset_index() 
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True)                 
                except KeyError as err:
                    logger.info(f'err-{err}')
                    tmp = pd.DataFrame()  

            self.df_A3 = tmp 
            logger.info( f'self.df_A3_new-{tmp}' )  # PS 
            self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3   
            
        elif re.search('^Z|^Solo', key, re.IGNORECASE):  
            tmp = self.df.reset_index()  
            # comment
            if re.search('Z4_|Z6_|Z9_|Z17_|Z18_|Z19_|Z20_|Z30_|Z31_|Z32_|Z33_|Z39_|Z42_|Z46_|Z61_|Z62_|Z64_|Z66_|Z70_|Z71_|Z72_|Z74_|Z77_|Z80_|Z81_|Z83_|Z84_|Z86_|Z89_|Z103_|Z109_|Z142_|Z145_|Z149_|Z160_|Z162_|Z164_|Z178_|Z182_|Z202_|Z203_|Z209_|Z210_|Z528_|Z529_|Z540_|Z541_|Z542_|Z543_|Z544_|ZS156_|ZS163_|ZR44_|ZR45_|ZR47_|ZR48_|ZR49_|ZR52_|ZR53_|ZR54_|ZR55_|ZR56_|ZR57_|ZR60_|ZR73_|ZR87_|ZR90_|ZR115_|ZR150_|ZR151_|ZR165_|ZF102_|ZFR103_|ZFR106_|ZFR111_|ZFR112_|ZFR113_|ZFR313_|ZFR115_|ZFR116_|ZFR135_|ZFR136_|ZFR137_|ZFR139_|ZFR141_|ZFR309_|ZFR301_|ZFR315|ZF119_|ZF120_|ZF200_|ZF201_|ZF319_|ZF320_|ZF122_|ZF123_|ZF127_|ZF128_|ZF131_|ZF132_|ZF133_|ZF142_|ZF182_|ZF340_|ZF619_|ZF620_|ZFR613_|ZS143_|ZS161_|ZSR152_|ZSR153_|ZSR154_|ZSR155_|ZSR159_|ZSR166_|Solo_Capacitor|Solo_PinLifter|Solo_CorvusPin|Solo_ESCTempPerformance|Solo_ESCHeaterDutyCycle|Solo_ESCHeliumWithWaferNew|Solo_FI_CDA|Solo_PM_N2_CDA|Solo_PM_exhaust|Solo_PM_He|Solo_PM_PCW|Solo_ESCIdleTemp|Solo_PM_Leakback|Solo_Turbo_N2_PCW|Solo_Gasbox_exhaust_CDA|Solo_AL_N2|Solo_Drypump_PCW|Solo_Chiller_PCW|Solo_Bcl3|Solo_PM_V8|Solo_PM_Conductance|Solo_PM_GasLineLeak|Solo_PM_BasePressure|Solo_PM_BH_Leak|Solo_PM_DD_slope|Solo_PM_DD_NoPlasma|Solo_PM_ESC_Resistance|Z191_|Z206_|Z519_|Z520_|Z521_|Z524_|Z525_|Z526_|Z527_|Z528_|Z529_|Z530_|Z531_|Z532_|Z534_|Z535_|Z536_|Z537_|Z538_|Z545_|Z546_|Z547_|Z563_|Z564_|Z565_|Z566_|Z567_|Z568_|Z569_|Z570_|Z571_|ZFR501_|ZF502_|ZF504_|ZF507_|ZF508_|ZFR510_|ZF512_|ZF513_|ZF514_|ZF515_|ZF516_|ZF517_|ZF518_|ZF519_|ZF521_|ZF522_|ZF523_|ZF524_|ZF525_|ZF526_|ZF527_|ZF528_|ZF529_|ZF530_|ZF531_|ZF532_|ZF708_|Z550_|Z551_|Z552_|Z553_|Z554_|Z555_|Z557_|Z558_|Z559_|Z560_|Z561_|Z562_|Z_N2_1_|Z_N2_4_|Z_N2_5_|Z_N2_7_|Z_N2_8_|Z_N2_10_|Z_N2_11_|Z_N2_12_|Z_N2_13_', key, re.IGNORECASE):  # all steps  
                attr_key = key.split('_')[1] 
                needed_cols = list( tmp.columns[ tmp.columns.str.contains(attr_key) ] ) 
                logger.info(f'tmp data {tmp}')
            # io_point_name
            elif re.search('Z5_|Z38_|ZF121_', key, re.IGNORECASE):
                needed_cols = list(tmp.columns[tmp.columns.str.contains('WaferLifterMotionElapsedTime')])
                # needed_cols = [ 'WaferLifterMotionElapsedTimeUpToDown', 'WaferLifterMotionElapsedTimeDownToUp' ]                 
                
            elif re.search('Z8_', key, re.IGNORECASE):    
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('HeaterResistanceCheck') ] )
                # needed_cols = [ 'TCR_TCW_HeaterResistanceCheck1' ]                      
            elif re.search('Z7_', key, re.IGNORECASE):            
                needed_cols = [ 'LTC_208V_LineVoltageMonitor' ]
            elif re.search('Z10_|Z105_', key, re.IGNORECASE):            
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('Temperature_AI') ] )
                # needed_cols = [ 'GasLineHeater1Temperature_AI' ] 
                # if 'GasLineHeater2Temperature_AI' in tmp.columns:  # 2 for Metal NXP        
                #     needed_cols = [ 'GasLineHeater1Temperature_AI',  'GasLineHeater2Temperature_AI' ] 
            elif re.search('Z11_', key, re.IGNORECASE):     
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('Temp') ] ) 
                # needed_cols = [ 'ChamberFrontRightTemperatureMonitor_AI', 'ChamberFrontLeftTemperatureMonitor_AI', 
                #                'ChamberRearRightTemperatureMonitor_AI', 'ChamberRearLeftTemperatureMonitor_AI',
                #                'BiasElectrodeHousingTemperatureMonitor_AI', 'ThrottleValveTemperatureMonitor_AI' ] # Kiyo CX       
            elif re.search('Z12_|ZF101_|ZS148_|Solo_Leakrate|ZF503_', key, re.IGNORECASE):            
                needed_cols = [ 'LeakbackRate' ]             
            elif re.search('Z13_|ZS146_|ZF108_', key, re.IGNORECASE):     
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('ProcessManometer') ] ) 
                # needed_cols = [ 'ProcessManometer_RawAI', 'ProcessManometerZeroOffset', 'ProcessManometerAdjustedPressure' ] 
            elif re.search('Z14_|ZS147_', key, re.IGNORECASE):            
                needed_cols = list(tmp.columns[tmp.columns.str.contains('ChamberManometer')])
                # needed_cols = [ 'ChamberManometer_RawAI', 'ChamberManometerZeroOffset' ]
            elif re.search('Z15_', key, re.IGNORECASE):            
                # needed_cols = [ 'ForelineManometer_RawAI' ]  
                needed_cols = list(tmp.columns[tmp.columns.str.contains('Manometer')])
            elif re.search('Z16_|ZF134_|ZF181_|ZS158_', key, re.IGNORECASE):            
                needed_cols = [ 'ProcessManometerAdjustedPressure' ]    
            elif re.search('Z21_', key, re.IGNORECASE):     
                needed_cols = list(tmp.columns[ tmp.columns.str.contains('AI') ])
                # if re.search('Z21_TCP_TCR_Temperature80C', key, re.IGNORECASE):     # for F22 solo
                #     needed_cols = [ 'TCPWindowMidInnerTemperature_AI', 'TCPWindowHeaterOutputValue_AI', 'TCRTemperature_AI', 'TCRHeaterOutputValue_AI' ]                
                # else:
                #     needed_cols = list(tmp.columns[ tmp.columns.str.contains('AI') ])
            elif re.search('Z27_|Z28_|Z35_', key, re.IGNORECASE):     
                needed_cols = [ 'TCUCh1TempMonitor_AI' ] 
                if needed_cols[0] not in tmp.columns: # Z28 for Kiyo CX; Kiyo EX Z28 & Z35: F21 Kiyo EX (single chiller)
                    needed_cols = [ 'TCUMonitor_AI' ]
            elif re.search('Z34_|Z40_', key, re.IGNORECASE):     
                needed_cols = [ 'GasPumpPurgeCycle' ] 
            elif re.search('Z37_', key, re.IGNORECASE):     
                needed_cols = [ 'VCIZeroingValue' ]         
            elif re.search('Z63_', key, re.IGNORECASE):     
                needed_cols = [ 'TCUMonitor_AI' ]
                if needed_cols[0] not in tmp.columns: # Z63 for Kiyo FXE CMG-ME
                    needed_cols = [ 'TCUCh1TempMonitor_AI' ]
            elif re.search('Z67_', key, re.IGNORECASE): # for Kiyo FXE CMG-ME; HMO
                needed_cols = [ 'TCUCh1TempMonitor_AI' ]
            elif re.search('Z75_', key, re.IGNORECASE):     
                needed_cols = list(tmp.columns[tmp.columns.str.contains('_AI')])
                # needed_cols = [ 'InnerESCTemperature_AI', 'MidInnerESCTemperature_AI', 
                #                'MidOuterESCTemperature_AI', 'OuterESCTemperature_AI' ] 
            elif re.search('Z100_', key, re.IGNORECASE):     
                needed_cols = list(tmp.columns[tmp.columns.str.contains('_AI')]) #2025/11 Darren for HIR
                # needed_cols = [ 'TCUMonitor_AI', 'ESCCoolantFlow_AI',      
            elif re.search('Z101_', key, re.IGNORECASE):     
                needed_cols = list(tmp.columns[tmp.columns.str.contains('Step2_OutputValue')]) #2025/11 Darren for HIR
                # needed_cols = [ 'Inner_Step2_OutputValue', 'MidInner_Step2_OutputValue','MidOuter_Step2_OutputValue','Outer_Step2_OutputValue',]      
            elif re.search('Z76_|Solo_PM_TCU_Temp_Flow|Z_N2_9_|Z_N2_14_', key, re.IGNORECASE):     
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('Monitor_AI') ] ) 
                # needed_cols = [ 'TCUCh1TempMonitor_AI', 'TCUCh2TempMonitor_AI', 
                #                'TCUCh1OutletFlowMonitor_AI', 'TCUCh2OutletFlowMonitor_AI' ]  
            elif re.search('Z104_|Z171_|Z172_|Z173_|Z175_|Z176_|ZF174_|ZF177_|ZF310_|ZF311_|ZF372_|ZF509_|ZF520_|ZF533_', key, re.IGNORECASE): # for Kiyo EXP OD0
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('TimeDelta') ] ) 
                # needed_cols = [ 'TCURampUpTimeDelta', 'TCURampDownTimeDelta' ]  #  Single TCU
                # if needed_cols[0] not in tmp.columns:  
                #     needed_cols = [ 'TCUCH1RampUpTimeDelta', 'TCUCH1RampDownTimeDelta' ] # Dual TCU             
            # elif re.search('Z172_', key, re.IGNORECASE): # for Kiyo FXE CMG-HMO, CMG-ME
            #     needed_cols = [ 'TCUCH1RampUpTimeDelta', 'TCUCH1RampDownTimeDelta' ]
            # elif re.search('Z173_', key, re.IGNORECASE): # for Metal NXP
            #     needed_cols = [ 'MaxTCURampDownTimeDelta' ] 
            # elif re.search('Z175_|ZF174_|ZF177_|ZF509_|ZF520_|ZF533_', key, re.IGNORECASE): # for Kiyo FXE CMG-ME, Flex FL MD1, VG
            #     needed_cols = [ 'TCURampUpTimeDelta', 'TCURampDownTimeDelta' ]   
            # elif re.search('Z176_', key, re.IGNORECASE): # for Metal M-LB
            #     needed_cols = [ 'MaxTCURampDownTimeDelta' ]                   
            elif re.search('Z183_|Z201_', key, re.IGNORECASE): # for solo
                needed_cols = list(tmp.columns[tmp.columns.str.contains('ForelineHeater')])
                # needed_cols = [ 'ForelineHeater1_TempMonitor_AI', 'ForelineHeater1_OutputValue_AI',
                #                'ForelineHeater2_TempMonitor_AI', 'ForelineHeater2_OutputValue_AI',
                #                 'ForelineHeater3_TempMonitor_AI', 'ForelineHeater3_OutputValue_AI',
                #                 'ForelineHeater4_TempMonitor_AI', 'ForelineHeater4_OutputValue_AI',
                #                 'ForelineHeater5_TempMonitor_AI', 'ForelineHeater5_OutputValue_AI']
            elif re.search('Z190_', key, re.IGNORECASE):
                needed_cols = list(tmp.columns[tmp.columns.str.contains('ESCCoolantFlow')])
            # elif re.search('Z201_', key, re.IGNORECASE):
            #     needed_cols = ['ForelineHeater2to5SetpointActuator']
            elif re.search('ZF104_|ZF105_|ZF107_|ZF109_|ZF124_|ZF210_|ZF307_|ZF308_|ZF309_|ZF433_|ZF607_', key, re.IGNORECASE):  
                needed_cols = list(tmp.columns[tmp.columns.str.contains('Manometer')])
                # needed_cols = [ 'ProcessManometer2AdjustedPressure' ]      
            # elif re.search('ZF307_', key, re.IGNORECASE):  
            #     needed_cols = list( tmp.columns[ tmp.columns.str.contains('Manometer') ] ) 
            #     # needed_cols = [ 'ProcessManometer2ZeroOffset', 'ProcessManometer2AdjustedPressure' ]  
            # elif re.search('ZF308_', key, re.IGNORECASE):  
            #     needed_cols = [ 'ProcessManometerZeroOffset' ]
            # elif re.search('ZF309_', key, re.IGNORECASE):  
            #     needed_cols = [ 'ChamberManometerZeroOffset' ]   
            # elif re.search('ZF210_', key, re.IGNORECASE):  
            #     needed_cols = [ 'ForelineManometerZeroManometer' ]                 
            # elif re.search('ZF607_',key, re.IGNORECASE):  
            #     needed_cols = list( tmp.columns[ tmp.columns.str.contains('Manometer') ] ) 
                # needed_cols = [ 'ProcessManometer_AI','ProcessManometerZeroOffset','ProcessManometer_RawAI',
                #                'ProcessManometer2_AI','ProcessManometer2_RawAI','ProcessManometer2ZeroOffset',
                #                'ChamberManometer_RawAI' ,'ChamberPressureManometer_AI','ForelinePressureManometer_AI',
                #                'ForelineManometer_RawAI']  
            # elif re.search('ZF372_', key, re.IGNORECASE): 
            #     needed_cols = list( tmp.columns[ tmp.columns.str.contains('TimeDelta') ] ) 
            # elif re.search('ZF109_', key, re.IGNORECASE):            
            #     needed_cols = [ 'ChamberManometerAdjustedPressure' ] 
            elif re.search('ZF110_', key, re.IGNORECASE):            
                needed_cols = [ 'ForelinePressureManometer_AI' , 'ForelineManometer_RawAI' ]                 
            elif re.search('ZF117_|ZF203_|ZF125_|ZF511_', key, re.IGNORECASE):  
                needed_cols = list( tmp.columns[ tmp.columns.str.contains('TER_') ] )               
                logger.info(f'ZF511: {needed_cols}')
            elif re.search('ZF118_|ZF318_|ZF126_|ZF304_|ZF534_|ZF618_', key, re.IGNORECASE):  
                needed_cols = list( tmp.columns[ tmp.columns.str.endswith('OutputValue') ] )
            elif re.search('Z167_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas1_255', 'Gas2_123' ]
            elif re.search('ZF401_', key, re.IGNORECASE):   #2025/11 Darren for GL MD1  
                needed_cols = [ 'Gas2_5.8', 'Gas3_4.6' ,'Gas6_25','Gas9_26','Gas10_247','Gas11_400','Gas12_1560','Gas13_1599','Gas14_450','Gas15_2000' ]     
            elif re.search('Z168_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas8_5_abs', 'Gas8_95_abs' ]                  
            elif re.search('Z169_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas6_95_abs' ] 
            elif re.search('ZF170_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas5_8_abs', 'Gas2Gas15_9_abs' ]
            elif re.search('ZF399_', key, re.IGNORECASE): #2025/11 Darren for FL VG    
                needed_cols = [ 'Gas14_6_abs' ] 
            elif re.search('Z102_', key, re.IGNORECASE):     ##2025/11 Darren for MetalM and NXP [GasX_X] 
                # Make selection robust to MultiIndex or non-string column names , needed_cols = tmp.filter(regex=r'^Gas\d+_\d+(?:\.\d+)?$').columns.tolist()
                needed_cols = [col for col in tmp.columns 
                               if re.match(r'^Gas\d+_\d+(?:\.\d+)?$', str(col))]
            elif re.search('Z179_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas8_55', 'Gas9_55', 'Gas10_23' ]                  
            elif re.search('ZF180_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas2_9s', 'Gas8_70s', 'Gas7_60s', 'Gas12_5s' ]                  
                # if the parameter not in columns, remove
                for i in needed_cols:
                    if i not in tmp.columns:
                        needed_cols.remove( i )
            elif re.search('Z181_', key, re.IGNORECASE):     
                needed_cols = [ 'Gas7_5', 'Gas8_5', 'Gas10_5' ]   
            elif re.search('Z533_ChillerFlowAlign', key, re.IGNORECASE):     
                needed_cols = [ 'ESCCoolantFlow_AI', 'TCUCh1FlowSetpoint_AO', 'TCUCh2FlowSetpoint_AO' ]   
            elif re.search('Z556_', key, re.IGNORECASE):     
                needed_cols = [ 'TCPWindowHeaterTemperatureMonitor_AI', 'TCRHeaterTemperatureMonitor_AI', 'TCPWindowHeaterOutputValue_AI', 'TCRHeaterOutputValue_AI' ]    
            elif re.search('Z_N2_3_', key, re.IGNORECASE):
                needed_cols = list(tmp.columns[tmp.columns.str.contains('Flow_AI')])
            elif re.search('Z_N2_6_', key, re.IGNORECASE):
                needed_cols = ['PMPressureDelta']
            #elif re.search('Solo_Temperature', key, re.IGNORECASE):   
            #    attr_key = 'AI'
            #    needed_cols = list( tmp.columns[ tmp.columns.str.contains( attr_key ) ] )              
            elif re.search('Solo_ESCHelium_202-A52722-025|Solo_Temperature', key, re.IGNORECASE):   
                attr_key = 'Solo_A3'
                needed_cols = list( tmp.columns[ tmp.columns.str.contains( attr_key ) ] )
                #logger.info(f'key-{key}; needed_cols-{needed_cols}')
            try:  
                columns = [ self.SPEC_NAME, self.ATTR_PMALIAS ]
                logger.info( f'self.df_A3-{tmp}' )  # PS   
                tmp = tmp[ columns + needed_cols ]
                                
                tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
                tmp = tmp.pivot( index = [ self.SPEC_NAME, 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
                tmp = tmp.reset_index()
                # non-unique variable name; need to rename "One-Shot-Macro Name" column to merge A3 form
                if re.search('Z27_|Z28_|Z35_|Z63_|Z4_|Z12_|Z13_|Z14_|Z16_|ZS146_|ZS147_|ZS148_|ZS158_|ZS163_|ZF105_|ZF107_|ZF124_|ZF310_|ZF311_', key, re.IGNORECASE): # join with Variable and One-Shot-Macro Name
                    tmp.rename( columns = { self.SPEC_NAME:'One-Shot-Macro Name' }, inplace = True) 
                if tmp.empty: # sshc column for 0 rows with 3 columns
                    tmp = None
                self.df_A3 = tmp       
                self.subsys_obj[self.EXCEL_DATA_A3] = self.df_A3               
                logger.info( f'self.df_A3_new-{tmp}' )  # PS 
            except (UnboundLocalError, KeyError) as err: # Z3_
                logger.info( f'Error occurred for {key}; err-{err}' )
                pass
        else:
            self.df_A3= None

    def __query_misg_info(self):
        '''
        query_misg_info is run when the self.df doesnt contain one or more requested attrs 
        in spec_info. This function uses the PM_ConfigOpt of the respective PM to search 
        for the missing attributes.
        '''
        info_value ={}
        str2bool={'true':True, 'false':False}
        for attr in self.subsys_obj[self.SPEC_INFO].keys():
            if attr in self.df:
                continue
            for pmalias in self.df[self.ATTR_PMALIAS].unique():
                try:
                    assert(pmalias in info_value)
                except AssertionError:
                    info_value[pmalias] ={}
                try:
                    info_value[pmalias][attr]=self.db[pmalias]['PM_ConfigOpt'].loc[attr,'value_000']
                except KeyError:
                    info_value[pmalias][attr]=self.subsys_obj[self.SPEC_INFO][attr]

        info_value =  pd.DataFrame.from_dict(info_value, orient='index')
        # info_value.replace(str2bool,inplace=True)
        self.df = self.df.merge(info_value, how='left', left_on=self.ATTR_PMALIAS,right_index=True)

    def __unlist_df_elem(self, data=None):
        '''
        Parameters:
        data (pd.DataFrame): a dataframe in which attrs are <list> dtype
        __unlist_df_elem is a private function of the autotrd class
        (1) it explodes each list element in the dataframe into multiple rows
        (2) it only works on the self.df 
        '''
        #1[=] float, which will be consider as new indices, 0 [=] list, pd.Series.explode
        if not isinstance(data, pd.DataFrame):
            data = self.df
        iter_attr = [] # iterable-datatype attributes
        noniter_attr = [] # noniterable-datatype attributes
        for attr in data.columns:
            if self.__is_iter_instance(data.iloc[0][attr]):
                iter_attr.append(attr)
            else:
                data.set_index(attr, append=True, inplace=True)
                noniter_attr.append(attr)                        
        #transform a list element of a dataframe into rows 
        data = (data[iter_attr].apply(pd.Series.explode)
                                    .reset_index(level=noniter_attr, drop=False))
        logger.info(f' data : {data}')
        if len(self.ltype_info)!=0: 
            data.set_index(self.ltype_info,append=True, inplace=True)
        return data

    def __generate_subsys_data(self, data=None):
        if not isinstance(data, pd.DataFrame):
            data = self.df
        ai_value = data.iloc[0,0]
        try:
            ao_level = self.subsys_obj[self.SPEC_AO]
            ao_vec = self.subsys_obj[self.EXCEL_DATA][ao_level]
            ao_value = ao_vec.iloc[0]
            # if the ai_value is a list-type element
            if self.__is_iter_instance(ai_value):
                if self.__is_iter_instance(ao_value):
                    if len(ai_value) == len(ao_value):
                        data = data.merge(ao_vec, left_index=True, right_index=True)
                data = self.__unlist_df_elem(data)
            else:
                if not self.__is_iter_instance(ao_value):
                    data = data.merge(ao_vec, left_index=True, right_index=True)
            if ao_level in data:
                data[ao_level] = data[ao_level].astype(str)
                data = (data.rename(columns={ao_level:self.ATTR_AO})
                                .set_index(self.ATTR_AO, append=True))
        except:
            if self.__is_iter_instance(ai_value):
                data = self.__unlist_df_elem(data)
        return data

    def __unstack_dataframe(self, data=[]):
        data.append(self.df[~self.df.index.duplicated()].unstack(self.ATTR_PMALIAS))
        self.df = self.df[self.df.index.duplicated()]
        if self.df.empty:
            return pd.concat(data, axis=0, sort=False)
        else:
            return self.__unstack_dataframe(data)

    def __run_postprocess(self, spec_attr):
        '''
        run_postprocess modifies EXCEL_RPT to create a human-readable dataframe
        '''
        flag=False
        try:
            self.df.set_index([self.SPEC_MIN,self.SPEC_MAX], append=True, inplace=True)
            # reorder the index column 
            index=self.df.index.names.copy()
            #logger.info( f'before ordered index-{index};\n' ) 
            if self.subsys_obj[self.ZIP_DTYPE] == 'xml':
                index.remove('UCL')
                index.remove('LCL')
                if('Application' in index):
                    index.remove('Application')
                    index.insert(2,'Application')
                    index.insert(3,'LCL')
                    index.insert(4,'UCL')
                else:
                    index.insert(2,'LCL')
                    index.insert(3,'UCL')
                #logger.info( f'after ordered index-{index};\n' ) 
                self.df=self.df.reorder_levels(index)
        except:
            pass
        self.df = self.df[spec_attr].to_frame()
        self.df = self.__unstack_dataframe(data=[])
        #**** THIS IS USEFUL TO FIND THE IDX LEVEL GIVEN THE INDEX NAME
        # data.index.set_levels(data.index.levels[data.index.names.index(self.SPEC_AO)].astype(float),
        #     level=self.SPEC_AO,inplace=True)
        index_names = [ _elem for _elem in self.df.index.names 
            if  (_elem not in self.subsys_obj[self.SPEC_INFO].keys()) and\
                (_elem != self.ATTR_AO) ]
        index_names += [ _elem for _elem in self.subsys_obj[self.SPEC_INFO].keys()
                    if  _elem in self.df.index.names]
        if self.ATTR_AO in self.df.index.names: 
            index_names.append(self.ATTR_AO)
            flag=True
        self.df = (self.df.reorder_levels(index_names)) 
        if flag:
            try:
                self.df.index = self.df.index.set_levels(self.df.index.levels[-1].astype(float),
                        level=self.ATTR_AO)
                self.df.sort_index(level=[self.ATTR_AO], inplace=True)
            except Exception as err:
                pass
            if self.SPEC_GASIDX in self.df.index.names: 
                self.df.sort_index(level=[self.SPEC_GASIDX, self.ATTR_AO], inplace=True)
        elif self.SPEC_GASIDX in self.df.index.names:
            self.df.sort_index(level=[self.SPEC_GASIDX], inplace=True)
        self.subsys_obj[self.TOCSV_DATA][spec_attr] = self.df 

    def _augm_gasbox_interlock(self, key):
        try: 
            gasbox_interlock_new = functools.reduce(lambda left, right: 
                pd.merge(left,right, how='outer', left_index= True, right_index=True), self.gasbox_interlock) 
        except TypeError:
            gasbox_interlock_new = pd.DataFrame()
        return gasbox_interlock_new 

    def _augm_gasbox_config(self, key):
        module_info = re.search(r'GasBox_Config_(.*?)_?$', key, re.IGNORECASE).group(1)
        if len(module_info) < 8:
            module_info = f'({module_info}_|{module_info}$)'
        module_info = [_elem for _elem in self.module_info
                if re.search(f'^{module_info}', _elem, re.IGNORECASE)][0]
        gasboxes = [self.gasbox_config[idx].rename(columns={'MaxFlow':f'Gasbox{idx}'}) 
            for idx in self.gasbox_idx.get(module_info)]
        
        # PS Change to columns with every chamber 
        gasboxes_new = []
        for df in gasboxes:
            col_tmp = list( df.columns[ df.columns.str.contains('Gasbox') ] )
            pmalias = df.loc[(self.ATTR_PMALIAS), col_tmp[0] ].values[0]
            if ',' in pmalias:
                pmalias_tmp = pmalias.replace(' ', '').split(',') # ['AESDL1_PM1', 'AESDL1_PM2', 'AESDL1_PM3']
                for i in pmalias_tmp:
                    df_tmp = copy.deepcopy(df)
                    df_tmp = df_tmp.drop('pmalias', level=0, axis=0 )
                    df_tmp = df_tmp.rename(columns={ col_tmp[0] : i } )
                    gasboxes_new.append( df_tmp )
            else:
                df_tmp = copy.deepcopy(df)                
                df_tmp = df_tmp.drop('pmalias', level=0, axis=0 )
                df_tmp = df_tmp.rename(columns={ col_tmp[0]: pmalias })
                gasboxes_new.append( df_tmp )
        gasboxes = functools.reduce(lambda left, right: 
            pd.merge(left,right, how='outer', left_index= True, right_index=True), gasboxes)
        gasboxes_new = functools.reduce(lambda left, right: 
            pd.merge(left,right, how='outer', left_index= True, right_index=True), gasboxes_new)    
            
        return gasboxes_new # gasboxes

    def __query_subsys_data(self, key):
        '''
        __query_subsys_data aggregates sshc data from all modules  
        ''' 
        if re.search('GasBox_Config', key, re.IGNORECASE):
            self.df = self._augm_gasbox_config(key=key) # PS
        elif re.search('GasBox_Interlock', key, re.IGNORECASE):
            self.df = self._augm_gasbox_interlock(key=key) # PS
        else:
            self.df = []
            for pmalias in self.pmalias:
                try:
                    data = copy.deepcopy(self.db[pmalias][self.subsys])
                    #logger.info( f'data-{data}' )
                    attr = [pmalias] if data.shape[1] == 1\
                            else [re.sub('value', pmalias, _elem) for _elem in data.columns] 
                    data.columns = attr
                    self.df.append(data)
                except KeyError as err:
                    logger.debug(f'{pmalias} does not contain {self.subsys}, {err}')
            try:
                self.df = functools.reduce(lambda left, right: 
                    pd.merge(left,right, how='outer', left_index= True, right_index=True), self.df)
            except:
                self.df = pd.DataFrame() 
        #logger.info( f'key-{key}; self.df-{self.df}' ) # PS
    def __compare_config(self, key):
        '''
        compare_config (1) compares the config options among the modules; (2) records
        the number of different settings for each config option; and (3) reports whether 
        all modules have the same setting for each config option 
        Note:
        this function makes the following assumptions
        1. each column corresponds to module
        2. each row corresponds to a config option 
        3. all modules are of the same type
        '''
        def format_numeric(value):
            if isinstance(value, (int,float)) and re.search('[^a-zA-Z]',f'{value}'):
                if value  == 0:
                    int(value)
                elif abs(value) < 1e-2:
                    value = np.format_float_scientific(value, precision=3, exp_digits=1)
                else:
                    value = float(value)
            return value
        self.df = self.df.applymap(format_numeric)
        self.df.fillna('', inplace = True)
        self.df[self.ATTR_NUMCONFIG] = self.df.apply(lambda _elem: 
                    len(np.unique([f'{_value}' for _value in _elem.dropna()]))
                    + any(_elem.isna()), axis=1)
        self.df[self.ATTR_MATCH] = (self.df[self.ATTR_NUMCONFIG] == 1)
        self.df[self.ATTR_ISNULL] = self.df.applymap(lambda value: 1 if value =='' else 0).sum(axis=1)
        nlevels = self.df.index.nlevels
        self.df.index.names = [name if name else f'id{idx:02d}'
            for idx, name in zip(range(nlevels), self.df.index.names)]
        if re.search('GasBox_Config', self.subsys, re.IGNORECASE):
            self.df.loc[self.ATTR_PMALIAS, self.ATTR_MATCH] = True
        else:
            self.df.sort_values([self.ATTR_MATCH, self.ATTR_ISNULL] + self.df.index.names, 
                ascending=[True, True] + [True]*nlevels, inplace=True)
        self.df[self.ATTR_COMMENT]=''
        if re.search('AbsoluteFlowVerifier', key, re.IGNORECASE):  # PS
            needed_cols = list( self.df.columns[ self.df.columns.str.contains('PM') ] )
            self.df = self.df[ needed_cols ]
            #logger.info(f'self.df-{self.df}')
        self.subsys_obj[self.TOCSV_DATA][self.subsys] = self.df

    def __run_pm_spec_augm(self):
        attr_name = 'ConfigOptionName'
        attr_value = 'ConfigOptionValue'
        delimiter = ':'
        self.pm_spec[attr_value].replace(regex=r'\*', value='%', inplace=True)
        attr_config = self.pm_spec.loc[:,attr_name].unique().tolist()
        if all([True if len(_elem) == 0 else False 
            for _elem in attr_config]):
            return
        index_names = self.pm_spec.index.names        
        self.pm_spec = (self.pm_spec.reindex(columns=self.pm_spec.columns.tolist()+attr_config)
                                    .reset_index(drop=False))
        for idx, data in self.pm_spec.iterrows():
            name, value = data.loc[[attr_name,attr_value]]
            self.pm_spec.loc[idx,name]=value
        tmp = []
        for org_attr in  attr_config:
            new_attrs = re.split(delimiter, org_attr)
            tmp+=new_attrs
            if len(new_attrs) == 1:
                self.pm_spec[org_attr] = self.pm_spec[org_attr].astype(str)
            else:
                self.pm_spec[new_attrs] = self.pm_spec[org_attr].str.split(pat=delimiter, expand=True)
        attr_config = tmp
        self.pm_spec.set_index(index_names+attr_config, drop=True, inplace=True)

    def __query_pm_spec(self, spec_attr):
        '''query_pm_spec queries the dataframe containing the SSHC spec
        Parameter:
        spec_attr (str): attribute in EXCEL_DATA to be compared against the SSHC specs
        Note:
        This function is executed after the self.run_preprocess(.), which creates
        its two input parameters, i.e., self.subsys and self.pm_config
        '''
        try:
            sql_pmalias = ', '.join([f"'{_elem}'" for _elem in self.pmalias])
            #logger.debug(f'check variable data sql_pmalias:\n{sql_pmalias}')
            usrcmd = f"{self.ATTR_PMALIAS} in [{sql_pmalias}]"
            if not re.search('pm', self.pmalias[0], re.IGNORECASE): # PS 
                pm_config = self.pm_config_tmp.query(usrcmd).loc[:,self.SPEC_CONFIG].unique()
                
            else:
                pm_config = self.pm_config.query(usrcmd).loc[:,self.SPEC_CONFIG].unique()
                
            self.pm_spec = [self.pltf_spec.get_group((_elem, self.subsys, self.application))\
                                for _elem in pm_config] # PS
            self.pm_spec = pd.concat(self.pm_spec, sort = False)  
            
            #*** THIS IS USEFUL TO REMEMBER WHEN WORKING WITH MULTIINDEX DATAFRAME           
            tmp = self.pm_spec.query(f"{self.ATTR_AO} == '{spec_attr}'")
            
            if not tmp.empty:
                self.pm_spec = tmp
            
            self.__run_pm_spec_augm()
           
            if len(self.pm_spec.index.unique(level=self.ATTR_AO)) == 1: # PS for one spec items, check if spec_attr is the same as attr in spec
                if self.pm_spec.index.unique(level=self.ATTR_AO) != spec_attr : # len( self.pm_spec.index.get_level_values(level = self.ATTR_AO) ) == 1
                    pass
                else:  
                    self.pm_spec.reset_index(level=self.ATTR_AO, inplace =True)  
            self.pm_spec = self.pm_spec[[self.SPEC_MIN, self.SPEC_MAX]]
            #logger.debug(f'check variable data new pm_spec:{type(self.pm_spec[self.SPEC_MIN].values[0])}\n{self.pm_spec}')
        except KeyError as err:
            if self.subsys_obj[self.ZIP_DTYPE] == 'xml':
                # costumor macro use A3 spec to build the spec  
                try:
                    self.pm_spec=self.speclib_A3.copy()
                    #logger.info(f'self.pm_spec-{self.pm_spec}') # PS
                    #  make the dataframe formate same as pervious define
                    self.pm_spec.rename(columns={'One-Shot-Macro Name': 'SSHC','Variable':'io_point_name'}, inplace=True)
                    self.pm_spec.set_index(['ModInfo_Concat', 'SSHC', 'Application', 'io_point_name'], inplace=True) # PS
                    self.pm_spec = self.pm_spec.groupby(level=['ModInfo_Concat', 'SSHC', 'Application']) # PS

                    self.pm_spec = [self.pm_spec.get_group((_elem, self.subsys, self.application))\
                                    for _elem in pm_config]
                    self.pm_spec = pd.concat(self.pm_spec, sort = False)
                    self.SPEC_SHOW='ShowSpec'
                    self.DATATYPE='dataType'
                    #logger.debug(f'check spec data \n:{self.pm_spec[[self.SPEC_MIN, self.SPEC_MAX,self.SPEC_SHOW,self.DATATYPE]] }')
                    if(self.pm_spec[self.SPEC_SHOW].iloc[0]==1):
                        #logger.debug(f'check DATATYPE {type(self.pm_spec[self.DATATYPE]) } \n:{self.pm_spec[self.DATATYPE].values }')
                        tmp = self.pm_spec.query(f"{self.DATATYPE} == '{spec_attr}'")
                        if not tmp.empty:
                            # specify the spec with the spacial value
                            self.pm_spec = tmp
                        elif (self.pm_spec[self.DATATYPE].values[0]!=''):
                            # this indicate the spec with the specific value but not the one we calculae now
                            raise KeyError
                        #logger.debug(f'check second spec data \n:{self.pm_spec[[self.SPEC_MIN, self.SPEC_MAX,self.SPEC_SHOW,self.DATATYPE]] }')    
                        self.pm_spec = self.pm_spec[[self.SPEC_MIN, self.SPEC_MAX]] 
                    else:
                        raise KeyError
                    #logger.debug(f'before sort pm_spec:{type(self.pm_spec)}\n{self.pm_spec}\n') 
                    try:
                        specIndex2Df =self.pm_spec.index.to_frame(index=False)
                        for i in specIndex2Df['io_point_name']:
                            float(i[-1])
                        from natsort import index_natsorted, order_by_index
                        # self.pm_spec.reindex(natsorted(self.pm_spec.index.tolist(), alg=ns.IGNORECASE))
                        self.pm_spec=self.pm_spec.reindex(index=order_by_index(self.pm_spec.index, index_natsorted(specIndex2Df['io_point_name'])))
                        # self.pm_spec.sort_index(by='io_point_name',key=natsort_keygen(),inplace=True)
                    except Exception as e:
                        #logger.debug(f'error:{e}\n')
                        pass
                    # delete repeat spec remain the first spec
                    self.pm_spec=self.pm_spec[~self.pm_spec.index.duplicated(keep='first')]
                    #logger.debug(f'after sort pm_spec:{type(self.pm_spec)}\n{self.pm_spec}') 
                except KeyError as err:
                    logger.debug(f'There is no SSHC spec for {err}')
                    self.pm_spec = pd.DataFrame()
                    raise TypeError
            else:
                logger.debug(f'There is no SSHC spec for {err}')
                self.pm_spec = pd.DataFrame()

    def _query_xml_spec(self, spec_attr, idx):
        excel_data = self.subsys_obj[self.EXCEL_DATA].copy()
        info_attrs = list(self.subsys_obj[self.SPEC_INFO].keys())
        attrs = [self.SPEC_MIN, self.SPEC_MAX]
        try:     
            attrs += info_attrs        
            for attr in attrs:
                try: 
                    alias = self.subsys_obj[attr][idx]
                except KeyError:
                    # attr belongs to info_attrs 
                    alias = attr
                if alias == None: 
                    raise TypeError
                else:
                    if alias in  excel_data:
                        excel_data.rename(columns={alias:attr},inplace=True)
                        # for attr belongs to info_attrs, check if they are the same
                        # type and the same length as those of the spec attribute
                        if alias == attr:
                            for func in [type, len]:
                                if not all(excel_data[spec_attr].apply(func) == excel_data[attr].apply(func)):
                                    excel_data = excel_data.drop(columns=attr)
                                    break
                    else:
                        try:
                            alias  = float(alias)
                            excel_data[attr] = excel_data[spec_attr].apply(
                                lambda value:  [alias]*len(value) if isinstance(value, list) else alias)
                        except ValueError:
                            pass
            attrs = [attr for attr in attrs if attr in excel_data]
            data = self.__generate_subsys_data(excel_data.loc[:,attrs])
            self.pm_spec = data.astype(float)
        except TypeError:
            # triggered when the self.subsys_obj[attr] is set to None
            self.pm_spec = pd.DataFrame()

    def __query_dt(self):
        '''
        query_dt creates a dataframe containing the date & time, on
        which the corresponding sshc data was recorded
        '''
        self.df = (self.subsys_obj[self.EXCEL_DATA].loc[:, self.ATTR_TIME]
                                    .to_frame())
        self.df = self.df.unstack(self.ATTR_PMALIAS)
        self.subsys_obj[self.TOCSV_DATA][self.ATTR_TIME] = self.df

    def __run_sql_merge(self):
        tb_data = 'df'
        tb_spec = 'pm_spec'
        def build_join_cmd(attrs, mode='wildcard'):
            attr_equiv = list(set(attr_join).difference(attrs))
            n = len(join_cmd)
            for idx, attr in enumerate(attrs):
                alias = f'alias{(idx+n):02d}'
                equiv = attr_equiv +[_elem for _elem in attrs if _elem != attr]
                if mode == 'wildcard':
                    cte_cmd[alias] = f'''{alias} AS (SELECT * FROM {tb_spec} WHERE {attr} LIKE '%\%%' ESCAPE '\\')'''
                    clause00 = [f'{tb_data}.{attr} LIKE {alias}.{attr}']
                if mode == 'noteq':
                    cte_cmd[alias] = f'''{alias} AS (SELECT * FROM {tb_spec} WHERE {attr} LIKE '%!%')'''
                    clause00 = [f'{tb_data}.{attr} <> SUBSTRING({alias}.{attr},2)']
                clause01 = [f'(LOWER({tb_data}.{_elem}) = LOWER({alias}.{_elem}))' for _elem in equiv]
                if ao_cmd:
                    clause01.append(ao_cmd)
                clause01 = ' AND '.join(clause00 + clause01)
                join_cmd[alias] = f'LEFT JOIN {alias} ON ({clause01})'
        #create connection object then ingest self.df, & self.pm_spec into memory
        cnx = sqlite3.connect(':memory:')
        attr_join = list(set(self.df.index.names).intersection(self.pm_spec.index.names))
        self.df.to_sql(tb_data, cnx, index=True, index_label=self.df.index.names, if_exists='replace')
        #logger.info(f'merge-pm_sepc:{self.pm_spec}; index: {self.pm_spec.index.names}')
        self.pm_spec.to_sql(tb_spec, cnx, index=True, index_label=self.pm_spec.index.names, if_exists='replace')
        # find the attributes that will be used for joining two tables 
        
        #CREATE THE HIERARCHICAL JOIN STATEMENT
        join_cmd = {}
        cte_cmd = {}
        if self.ATTR_AO in attr_join:
            ao_cmd = f'(CAST({tb_data}.{self.ATTR_AO} AS REAL)  = CAST({tb_spec}.{self.ATTR_AO} AS REAL))'
            attr_join.remove(self.ATTR_AO)
            clause01 = [f'(LOWER({tb_data}.{_elem}) = LOWER({tb_spec}.{_elem}))' for _elem in attr_join]
            clause01.append(ao_cmd)
            clause01 = ' AND '.join(clause01)
            #remove all specs that contain any alpha characters
            cursor_obj = cnx.cursor()
            sqlcmd = f'DELETE FROM {tb_spec} WHERE {self.ATTR_AO} GLOB "*[A-Za-z]*"'
            cursor_obj.execute(sqlcmd)
            cursor_obj.close()
        else:
            ao_cmd = False
            clause01 = ' AND '.join([f'(LOWER({tb_data}.{_elem}) = LOWER({tb_spec}.{_elem}))' for _elem in attr_join])
        join_cmd[tb_spec] = f'LEFT JOIN {tb_spec} ON {clause01}'
        # Isolate the attributes that contain at least one wildcard character
        attr_wildcard = [_elem for _elem in attr_join if 
                        self.pm_spec.index.get_level_values(level=_elem).str.contains("%", case=False).any()]
        if len(attr_wildcard) > 0:
            build_join_cmd(attr_wildcard, mode='wildcard')
        # Isolate the attributes that contain at least one noteq character
        attr_noteq = [_elem for _elem in attr_join if 
                    self.pm_spec.index.get_level_values(level=_elem).str.contains(r"!", case=False).any()]
        if len(attr_noteq) > 0:
            build_join_cmd(attr_noteq, mode='noteq')
        sql_funct = 'COALESCE' if len(join_cmd) > 1 else ''

        #CREATE THE COALESCE STATEMENT
        query_cmd = ['{}({}) AS {}'.format(sql_funct, ', '.join([f'{alias}.{_elem}' for alias in join_cmd.keys()]), _elem)
                    for _elem in [self.SPEC_MAX,self.SPEC_MIN]]
        query_cmd = ', '.join(query_cmd)
        join_cmd = '\n'.join(join_cmd.values())
        if cte_cmd:
            cte_cmd =  f"WITH {','.join(cte_cmd.values())}"
        else:
            cte_cmd = ''
        #CREATE THE SQL STATEMENT
        sqlcmd = f'''
            {cte_cmd}
            SELECT {tb_data}.*, {query_cmd}
            from {tb_data}
            {join_cmd}
        '''
        self.sqlcmd = sqlcmd
        self.metadata = pd.read_sql_query(sqlcmd, cnx)
        if self.ATTR_AO in self.metadata:
            self.metadata[self.ATTR_AO] = self.metadata[self.ATTR_AO].astype(float) 
        self.metadata.set_index(self.df.index.names, inplace=True)
        cnx.close()

    def __query_ltype_info(self, sample, attr):
        value_attr = sample[attr]
        self.ltype_info=[]
        if self.__is_iter_instance(value_attr):
            for _elem in self.spec_info: 
                value_info = sample[_elem]
                if len(value_info) == len(value_attr):
                   self.ltype_info.append(_elem) 
        logger.info(f'ltype_info : {self.ltype_info}')

    def __create_trd(self, key): 
        '''
        create_trd is the most important function of the autotrd class.
        (2) It runs the preprocessing and data cleansing function, and stores
        the data in self.EXCEL_DATA. 
        (3a) It takes an attribute of interest from EXCEL_DATA, merges with
        the corresponding specfication data, and checks for out-of-spec data
        (3b) It compares the options among all like modules to check for 
        inconsistent settings
        '''
        if re.search('toolID', key, re.IGNORECASE): 
            self.trd[key][self.TOCSV_DATA][key] = self.pm_config
            # A3 items for SoftwareVersion
            tmp = self.pm_config.copy()
            columns = [ self.ATTR_PMALIAS ] 
            needed_cols = [ self.ATTR_SVN ]
            tmp = tmp[ columns + needed_cols ]
            tmp = pd.melt(tmp, id_vars= columns , value_vars= needed_cols, var_name='Variable'  )
            tmp = tmp.pivot( index = [ 'Variable'], columns = self.ATTR_PMALIAS, values = 'value' )
            tmp = tmp.reset_index() 
            # re-index columns
            col1 = list( tmp.columns[ tmp.columns.str.contains('golden') ] )
            col2 = list( tmp.columns[ tmp.columns.str.contains('PM\d$') ] )                                
            new_cols = ['Variable'] + col1 + col2
            self.df_A3 = tmp.reindex( columns = new_cols )
            self.trd[key][self.EXCEL_DATA_A3] = self.df_A3
        else:
            self.subsys_obj = self.trd[key]
            self.subsys = self.subsys_obj[self.ATTR_KEY]
            self.pmalias = self.subsys_obj[self.ATTR_PMALIAS]
            #logger.info( f'key-{key};creat_trd: {self.subsys_obj} ' ) 
            self.__query_subsys_data(key)
            if not self.df.empty:
                if self.SPEC_ATTR in self.subsys_obj:
                    self.__run_preprocess()      
                    #logger.info( f'key-{key};df_pre: {self.df}' ) # PS debug
                    self.__run_A3_SSHC(key) # P3
                    
                    self.subsys_obj[self.EXCEL_DATA] = self.df
                    for spec_idx, spec_attr in enumerate(self.subsys_obj[self.SPEC_ATTR]):
                        logger.debug(f'Evaluating {self.subsys} SSHC via the {spec_attr} metric')
                        #logger.info( f'key-{key};\ncreat_trd: \n{self.subsys_obj} ' ) 
                        self.__query_ltype_info(
                            sample=self.subsys_obj[self.EXCEL_DATA].iloc[0], 
                            attr=spec_attr)
                        self.df = (self.subsys_obj[self.EXCEL_DATA].loc[:, [spec_attr]+self.ltype_info])
                       
                        # PS; Modify self.df for Forwards in Bias_Voltage_Power and set Forwards to be the same length as Setpoint
                        if spec_attr == 'Forwards' and self.subsys_obj[self.SPEC_AO] in self.subsys_obj[self.EXCEL_DATA]:   
                            len_setpoint = [ len(i) for i in self.subsys_obj[self.EXCEL_DATA][ self.subsys_obj[self.SPEC_AO] ] ]
                            for i in range( len(len_setpoint) ):
                                self.df[spec_attr][i] = self.df[spec_attr][i][:len_setpoint[i]]                        
                        
                        self.df = self.__generate_subsys_data()
                        try:
                            # check the spec and save the spec in self.pm_spec
                            self.__query_pm_spec(spec_attr.lower())
                        except TypeError:
                            pass
                            #self._query_xml_spec(spec_attr, spec_idx) # comment it is the original version which is no use for tsmc
                            
                        if not hasattr( self, 'pm_spec'): # PS for pure config # 'pm_spec' not in self.__dict__.keys()
                            self.pm_spec = pd.DataFrame()
                        if  (self.pm_spec.empty) \
                            or ((self.ATTR_AO not in self.df.index.names) 
                                and (self.ATTR_AO in self.pm_spec.index.names)):
                            pass
                        else:                            
                            try:
                                #logger.info( f'spec-\n{self.pm_spec[[self.SPEC_MIN,self.SPEC_MAX]]}\nself.df: \n{self.df} ' )  
                                # merge the data with spec
                                # there are three situation in this merge action
                                # 1. spec find the specific row to add
                                # 2. spec cannot find the specific create more data row than origin, need to remove 
                                # 3. after merge some data been removed, data smaller than origin, need to add   
                                self.pm_spec = self.pm_spec[ self.pm_spec['LCL'] != '' ] # PS remove empty string in spec 
                                # PS Rename variable name to io_point_name in self.df for SOLO macro using variable name (Solo_ESCHelium) before merging with self.pm_spec
                                if 'variable_name' in self.df.index.names:
                                    tmp = self.df.index.names
                                    self.df = self.df.reset_index()
                                    self.df.rename( columns = {'io_point_name': 'io_point_name_old', 'variable_name': 'io_point_name'}, inplace =True)
                                    self.df.set_index( list( self.df.columns[:-1] ) , inplace = True )
                                    logger.info( f'self.df-{self.df}' )
                                self.metadata = self.df.merge(self.pm_spec[[self.SPEC_MIN,self.SPEC_MAX]],\
                                            left_index=True, right_index=True)
                                #logger.info( f'self.metadata -\n{self.metadata }\n' ) 
                                #logger.info(f'self.metadata.shape-{self.metadata.shape}; self.df.shape-{self.df.shape}')
                                #logger.info(f'self.metadata-{self.metadata}; self.df-{self.df}; self.pm_spec[[self.SPEC_MIN,self.SPEC_MAX]]-{self.pm_spec[[self.SPEC_MIN,self.SPEC_MAX]]}' ) # PSS
                                # pop exception if the amunt of output row didn't match the original row lines 
                                if self.metadata.shape[0] != self.df.shape[0]: raise ValueError

                                # an exception when the spec is according to step and the spec is only for step 1,
                                # although it create the data is as big as the origin, we still need to delete follow the rule bellow 
                                if self.subsys_obj[self.ZIP_DTYPE] == 'xml' :
                                    logger.info(f"self.pm_spec.index.to_frame(index=False)['io_point_name'][0]-{ self.pm_spec.index.to_frame(index=False)['io_point_name'][0][-5:] } ")
                                if (self.subsys_obj[self.ZIP_DTYPE] == 'xml' and self.pm_spec.index.to_frame(index=False)['io_point_name'][0][-1]=='1' 
                                    and self.pm_spec.index.to_frame(index=False)['io_point_name'][0][-5:] !='item1' # Solo
                                    and not 'TMandChamberPressureDiffWhenSlotDoorOpen' in self.pm_spec.index.to_frame(index=False)['io_point_name'][0]  #  ZTM59, ZTM91
                                    and not 'WaferTransferStabilityMorethan2000' in self.pm_spec.index.to_frame(index=False)['io_point_name'][0]): 
                                    raise TypeError

                            except (TypeError, ValueError):
                                if self.subsys_obj[self.ZIP_DTYPE] == 'xml':
                                    # for a3 spec
                                    try:
                                        if(self.metadata.shape[0] < self.df.shape[0]):
                                            try:
                                                tempIndex2Df =self.pm_spec.index.to_frame(index=False)
                                                #logger.info( f'check tempIndex2Df -\n{tempIndex2Df}\n' )
                                                pecName=tempIndex2Df['io_point_name']
                                                #logger.info( f'check pecName -\n{pecName}\n' )
                                                dfIndex2Df =self.df.index.to_frame(index=False)
                                                resource=dfIndex2Df['resource']
                                                dfIOpointName=dfIndex2Df['io_point_name']
                                                #logger.info( f'check resource -\n{resource}\n' )
                                                tempUCL=[None] * self.df.shape[0]
                                                tempLCL=[None] * self.df.shape[0]
                                                for i in range(self.df.shape[0]):
                                                        for j in range(len(pecName)):
                                                            if resource[i] in pecName[j] and dfIOpointName[i] in pecName[j] :
                                                                tempUCL[i]=self.pm_spec['UCL'].values[j]
                                                                tempLCL[i]=self.pm_spec['LCL'].values[j]
                                                                break
                                                #logger.info( f'check up down -\n{tempUCL}\n{tempLCL}\n' )
                                                self.metadata=self.df.copy()
                                                self.metadata['UCL']=tempUCL
                                                self.metadata['LCL']=tempLCL
                                                logger.info( f'self error build new self.metadata -\n{self.metadata }\n' )
                                            except Exception as err:
                                                #logger.info( f'no rerender -\n{err}\n' )
                                                self.__run_sql_merge()
                                                #logger.info( f'error build new self.metadata -\n{self.metadata }\n' )
                                                pass
                                        else : # self.metadata.shape[0] >= self.df.shape[0]
                                            # to deal with the a3 form data if the spec cannot find their correspond data row they will be wrong
                                            # our expect: item spec(LCL,UCL)  |  the wrong data merge: item spec(LCL,UCL)
                                            #              A1 spec_1          |                         A1  spec_1
                                            #              A2 spec_2          |                         A1  spec_2         (we want to erase)
                                            #              A3 spec_3          |                         A3  spec_3         (we want to erase)
                                            #                                 |                         A2  spec_1         (we want to erase)
                                            #                                 |                         A2  spec_2
                                            #                                 |                         A2  spec_3         (we want to erase)
                                            #                                 |                         A3  spec_1         (we want to erase)
                                            #                                 |                         A3  spec_2         (we want to erase)
                                            #                                 |                         A3  spec_3
                                            copylen=len(self.pm_spec) # the amount of spec type * pm size
                                            tempIndex2Df = self.df.index.to_frame(index=False) # the temp to save df's index to find the amount of pm
                                            # Fix pm size
                                            pmSize= len(tempIndex2Df['pmalias'].drop_duplicates())
                                            #logger.info(f'tempIndex2Df-{tempIndex2Df}; self.df-{self.df} ')
                                            #pmSize=len(self.df)/len(tempIndex2Df['pmalias'].drop_duplicates())# get the amount of spec type
                                            # Total metadata per pm
                                            metadata_pm  = int(self.metadata.shape[0]/pmSize)
                                            #logger.info(f"copylen-{copylen}; pmSize-{pmSize}; self.metadata.shape[0]-{self.metadata.shape[0]} ")
                                            
                                            count_delete=0 #simplle count
                                            count_add=0 # count te rows which have been added
                                            for i in self.metadata.index:
                                                #logger.info(f'i-{i}; count_delete-{count_delete}' )
                                                #logger.info( f'show index - {count_delete%copylen }  {int(count_delete/copylen) }\n' )
                                                if count_add != copylen : # adding items 
                                                    if(count_delete % copylen!=int(count_delete/copylen)) :
                                                        #logger.info(f'count_delete-{count_delete}')
                                                        #logger.info( f'delete index - \n{i }\n' )
                                                        self.metadata=self.metadata.drop(i, axis='index') # delete the no need rows
                                                    else: 
                                                        #logger.info( f'add index - \n{i }\n' )
                                                        count_add=count_add+1
                                                else: # add enough items, delete other items
                                                    #logger.info(f'count_delete-{count_delete}')
                                                    #logger.info( f'delete index - \n{i }\n' )
                                                    self.metadata=self.metadata.drop(i, axis='index') # delete the no need rows
                                                    
                                                if (count_delete+1) == metadata_pm  : # (count_add%pmSize==0 )
                                                    # restart when one pm data has insert done
                                                    count_add=0
                                                    count_delete=0
                                                else: 
                                                    count_delete=count_delete+1 
                                                
                                            logger.info( f'too long error build new self.metadata -\n{self.metadata }\n' )
                                            if (self.metadata.shape[0] < self.df.shape[0]):
                                                #logger.info( f'A3 spec smaller than the previous data -\n' )
                                                #logger.info( f'self.metadata index-\n{self.metadata.index }\n' )
                                                temp=self.metadata.index.names
                                                # PS define step
                                                if 'step' in self.metadata.index.names:
                                                    step_col = 'step' # choose step first
                                                else:
                                                    step_col = 'step_no' # Z164
                                                # Use step to check if miss or not; Add missing row to self.metadata
                                                #logger.info(f'self.metadata-{self.metadata}; self.df-{self.df}' ) # PS
                                                #logger.info(f'self.metadata.index.unique("step")-{ list(self.metadata.index.unique("step")) }')
                                                metadata_step = list(self.metadata.index.unique(step_col))
                                                
                                                for i in range( self.df.shape[0]):
                                                    df_step = list( self.df.iloc[[i]].index.get_level_values(step_col) )[0]
                                                    #logger.info( f'df_step-{df_step}; metadata_step-{metadata_step}' )
                                                    if df_step not in metadata_step: # (i>=self.metadata.shape[0])
                                                        #logger.info( f'self.df index and value-\n{self.df.index.values[i] }\n  '  )
                                                        #logger.info( f'{self.df.loc[self.df.index.values[i]]}\n '  )
                                                        # self.metadata=pd.concat([ self.metadata, self.df.loc[self.df.index.values[i]]],ignore_index=False)
                                                        self.metadata=self.metadata.append(self.df.loc[self.df.index.values[i]], ignore_index=False)
                                                self.metadata.index = pd.MultiIndex.from_tuples(list(self.metadata.index.values))
                                                self.metadata.index.names=temp
                                                index=self.metadata.index.to_frame(index=False)
                                                #logger.info( f'add apps to index - {index}\n' )
                                                # add the application name
                                                index.loc[index["Application"].isna(), "Application"] = index["Application"][0]
                                                self.metadata.index = pd.MultiIndex.from_frame(index)
                                                logger.info( f'too short error build new self.metadata -\n{self.metadata }\n' )
                                        # reorder the index column 
                                        # index=self.metadata.index.names.copy()
                                        # logger.info( f'before ordered index-{index};\n' ) 
                                        # index.remove('UCL')
                                        # index.remove('LCL')
                                        # index.insert(2,'LCL')
                                        # index.insert(3,'UCL')
                                        # logger.info( f'after ordered index-{index};\n' ) 
                                        # self.metadata.reorder_levels(index)
                                    except Exception as err:
                                        # if no spec
                                        logger.info( f'data no spec; {err}\n' )
                                        pass
                                elif(self.metadata.shape[0] < self.df.shape[0]):
                                    # for normal spec and spec is smaller than data reinput the data 
                                    logger.info( f'no rerender -\n' )
                                    self.__run_sql_merge()
                                    logger.info( f'error build new self.metadata -\n{self.metadata }\n' )
                                    pass

                            if not self.metadata.dropna().empty:
                                self.df = self.metadata
                                
                                def to_numeric_same(value):
                                    try:
                                        value = float(value)
                                    except:
                                        logger.info( f'{value} in self.df cannot be translate to float' )
                                        pass
                                    return value
                                def check_spec(dfRow):
                                    try:
                                        if ((dfRow[spec_attr] > dfRow[self.SPEC_MAX]) or (dfRow[spec_attr] < dfRow[self.SPEC_MIN])) :
                                            return 1
                                        else:
                                            return 0
                                    except TypeError: # string
                                        return np.NaN # eturn 0
                                # translate value to float
                                # PS change for removing float for each; string stay the same
                                #self.df[spec_attr] = pd.to_numeric( self.df[spec_attr], errors = 'ignore' )  # self.df[spec_attr].astype(float)
                                self.df[spec_attr] = self.df[spec_attr].apply( lambda _elem: to_numeric_same(_elem) )
                                # for i in range(len(self.df[spec_attr])):
                                #     try:# some value can not be transform type to float
                                #         # PS change for slice 
                                #         self.df[spec_attr].iloc[i] = float(df[spec_attr].iloc[i])
                                #     except:
                                #         logger.info( f'the values {self.df[spec_attr].iloc[i]}cannot be translate to float \n' )
                                
                                # to check the value is in the spec or not
                                self.df[self.ATTR_OOS] =  self.df.apply(lambda _elem: check_spec(_elem) , axis=1)
                                #logger.info(f'self.df-{self.df}')
                                #logger.info( f'df \n\{self.df[[self.SPEC_MIN,spec_attr,self.SPEC_MAX]]}\nout of spec nums\n{self.df[self.ATTR_OOS]}\n' )
                        self.subsys_obj[self.EXCEL_RPT][spec_attr] = self.df
                        self.__run_postprocess(spec_attr)
                    self.__query_dt()
                else:            
                    self.__compare_config(key) # PS                   
                    self.__run_A3_config(key)


    def __sort_trd_sheet(self):
        '''
        __sort_trd_sheet sorts the SSHC reports in the alphabetical order
        Procedure:
        pulls the content of the <GenericModule>
        identifies items that dont belong to the <GenericModule>
        sorts those items in the alphabetical order
        '''
        module_config, module_alias = self.get_autotrd_config()
        config_data = {}
        sshc_data = {}
        for key, value in self.trd.items():
            if value[self.ATTR_KEY] in module_config.keys():
                config_data.update(**{key: value})
            else:
                sshc_data.update(**{key: value})
        sshc_data = {key: value for key, value in sorted(sshc_data.items(), key=lambda item: item[1][self.ATTR_KEY])}
        config_data.update(**sshc_data)
        self.trd = config_data

    def __call__(self):
        self._make_xmllog()
        self.load_autotrd_config()
        if self.autopm_auth:
            config_lib = self.load_autotrd_config(inplace=False, **self.autopm_auth)
            for key, data in config_lib.items():
                data = copy.deepcopy(data)
                for item in data.values():
                    if self.SPEC_AUGM in item:
                        item[self.SPEC_ATTR] += item[self.SPEC_AUGM]
                try:
                    self.config_lib[key].update(data)
                except KeyError:
                    pass
                    self.config_lib[key] = data
        self.__ingest_spec()
        self.__create_db()
        self.__query_gasbox_config()
        self.__query_gasbox_interlock() # PS
        self.__query_svn()
        self.__query_pm_config()
        self.__update_gasbox_config()
        self.__query_pm_alias()
        if isinstance(self.golden_pm,str):
            self.__run_golden_compare()
        if self.workers == 1:
            # Add self.pmalias_new for tool chambers for macro results
            self.pmalias_new  = [ key for key in self.db.keys() if 'PM' in key ] # ['AESDL2_PM1(golden)', 'AESDL1_PM1', 'AESDL1_PM2', 'AESDL1_PM3']         
            if self.golden_pm:
                self.pmalias_new = [ i for i in self.pmalias_new if 'golden' not in i ] # AESDL1-1_PM1(golden) ; AESDL1_PM1(golden) # i not in self.golden_pm
            # dict for tools and chambers #  {'tool1': ['PM2', 'PM1'], 'tool2': ['PM2', 'PM3', 'PM4', 'PM1', 'PM5']}
            self.tool_pm = {}
            for i in self.pmalias_new:
                tmp1, tmp2 = i.split('_')[0], i.split('_')[1]
                if tmp1 not in self.tool_pm:        
                    self.tool_pm[tmp1] = [tmp2]
                else:
                    self.tool_pm[tmp1].append( tmp2 ) 
            logger.info(f'self.pmalias_new-{self.pmalias_new} ')  
            logger.info(f'self.tool_pm-{self.tool_pm} ')  
            # Calcualte cv mistach numbers
            cv_mismatch = {'Variable': ['CV_mismatch']  } 
            self.cv_mismatch = pd.DataFrame( cv_mismatch )
            # Generate UCL and LCL
            def LCL(x):
                result=''                    
                try:
                    result = float(x)
                except ValueError:
                    x = str(x).replace(' ', '').replace('[', '').replace(']', '').replace(':', '').replace('小於', '<')
                    result=''
                    if re.search(r'^\d+(\.\d+)?$', x): #Solve when spec value: [100] cannot be detected
                        result = float(x)
                    elif re.search( '<=', x ):
                        delimiter = '<='
                        result = -1 * float(x.split( delimiter )[-1] ) # 0 
                    elif re.search( '<±', x ) :    
                        delimiter = '<±'
                        result = -1 * float(x.split( delimiter )[-1] )   
                    elif re.search( '<', x ) :    
                        delimiter = '<'
                        result = -1 * float(x.split( delimiter )[-1] ) # 0                                             
                    elif re.search( '=', x ):   
                        delimiter = '='
                        result = float(x.split( delimiter )[-1] )                    
                    elif re.search( '~', x ):   
                        delimiter = '~'
                        result = float(x.split( delimiter )[0] )  
                    elif re.search( '>=', x ):
                        delimiter = '>='
                        result = float(x.split( delimiter )[-1] ) 
                    elif re.search( '>', x ):    
                        delimiter = '>'
                        result = float(x.split( delimiter )[-1] )
                    elif re.search( '±', x ):
                        delimiter  = '±'
                        A = float(x.split( delimiter )[0])
                        B = float(x.split( delimiter )[1])
                        result = A-B
                    elif re.search( '\+/-', x ) :
                        delimiter  = '+/-'
                        A = float(x.split( delimiter )[0])
                        B = float(x.split( delimiter )[1])
                        result = A-B
                return result                 
            def UCL(x):
                result=''                    
                #logger.info(f'x-{x}') # PS
                try:
                    result = float(x)
                except ValueError:
                    x = str(x).replace(' ', '').replace('[', '').replace(']', '').replace(':', '').replace('小於', '<')
                    if re.search(r'^\d+(\.\d+)?$', x): # Solve when spec value: [100] cannot be detected
                        result = float(x)                    
                    elif re.search( '<=', x ):    
                        delimiter = '<='
                        result = float(x.split( delimiter )[-1] )
                    elif re.search( '<±', x ):
                        delimiter = '<±'
                        result = float(x.split( delimiter )[-1] )                        
                    elif re.search( '<', x ) :
                        delimiter = '<'
                        result = float(x.split( delimiter )[-1] )
                    elif re.search( '=', x ):   
                        delimiter = '='
                        result = float(x.split( delimiter )[-1] )                    
                    elif re.search( '~', x ):   
                        delimiter = '~'
                        result = float(x.split( delimiter )[1] )  
                    elif re.search( '>=', x ):
                        delimiter = '>='
                        result = float(x.split( delimiter )[-1] )+30000 # 1000
                    elif re.search( '>', x ):    
                        delimiter = '>'
                        result = float(x.split( delimiter )[-1] )+30000 # 1000
                    elif re.search( '±', x ):
                        delimiter  = '±'
                        A = float(x.split( delimiter )[0])
                        B = float(x.split( delimiter )[1])
                        result = A+B
                    elif re.search( '\+/-', x ) :
                        delimiter  = '+/-'
                        A = float(x.split( delimiter )[0])
                        B = float(x.split( delimiter )[1])
                        result = A+B
                return result   
            self.speclib_A3['LCL'] = self.speclib_A3['value'].apply( LCL )                
            self.speclib_A3['UCL'] = self.speclib_A3['value'].apply( UCL )    
            #self.speclib_A3.to_excel("speclib_A3.xlsx")  # PS
            for key in self.trd.keys():  
                self.__create_trd(key)
                
            #Merge self.speclib_A3
            speclib_A3 = copy.deepcopy(self.speclib_A3)
            
            for key in self.trd.keys(): # Merge A3 form: speclib_A3
                try:
                    if bool( re.search( '^PM_', key, re.IGNORECASE) ) & bool( re.search( '\d$', key, re.IGNORECASE) ) : # self.module_alias  :
                        pass # # PM_ConfigOpt_KiyoFX_K_00 or # PM_ConfigOpt_KiyoFX_K_PM1
                        
                    else:  # PM_ConfigOpt_KiyoFX_K or PM_CV_KiyoFX_K 
                        tmp = self.trd[key][self.EXCEL_DATA_A3]
                        #logger.info( f'key-{key}; tmp-{tmp.columns}' ) 
                        common_cols = set(tmp.columns).intersection(speclib_A3.columns)
                        logger.info( f'common_cols-{common_cols}' ) 
                        # non-unique variable name; need to rename "One-Shot-Macro Name" column to merge A3 form
                        if re.search( 'ZTM|Z27_|Z28_|Z35_|Z63_|Z4_|Z12_|Z13_|Z14_|Z16_|ZS146_|ZS147_|ZS148_|ZS158_|ZS163_|ZF105_|ZF107_|ZF124_|ZF310_|ZF311_|TCP.*Linearity|TCP.*VProbe|Bias.*Voltage_Probe', key, re.IGNORECASE): # same variable
                            if len( common_cols ) == 2: # 'Variable', 'One-Shot-MacroName'
                                speclib_A3 = speclib_A3.merge( tmp, how = 'left', on = ['Variable', 'One-Shot-Macro Name'] )
                            else: 
                                df_update = speclib_A3.merge( tmp, how = 'left', on = ['Variable', 'One-Shot-Macro Name'], suffixes=(None, '_x') )
                                cols = df_update.columns[ df_update.columns.str.contains('_x') ]
                                for i in cols:
                                    df_update_target = df_update[[i]].rename( columns={ i: i.replace('_x', '') })  
                                    speclib_A3.update( df_update_target  )
                                
                        else: #Others      PM_ConfigOpt_KiyoFX_K or PM_CV_KiyoFX_K    
                            if len( common_cols ) == 1: # 'Variable' common
                                speclib_A3 = speclib_A3.merge( tmp, how = 'left', on = 'Variable' )
                            else:
                                df_update = speclib_A3.merge( tmp, how = 'left', on = 'Variable', suffixes=(None, '_x') )
                                cols = df_update.columns[ df_update.columns.str.contains('_x') ]
                                for i in cols:
                                    df_update_target = df_update[[i]].rename( columns={ i: i.replace('_x', '') })  
                                    speclib_A3.update( df_update_target  )
                            #logger.info( f'After{speclib_A3}' ) 
                except (KeyError, AttributeError) as err: # PS
                    #logger.info(f'{err}')
                    pass
            self.speclib_A3 = speclib_A3
            
        else:
            kwargs = {}
            blank_obj = autotrd()
            for key in self.__dict__.keys():
                if key not in blank_obj.__dict__.keys():
                    kwargs[key] = getattr(self,key)
            func = getattr(MultiProcessHelper, 'create_trd')
            if kwargs:
                func = functools.partial(func, **kwargs)
            pool_obj = mp.Pool(processes=self.workers)
            pool_output = pool_obj.map(func, self.trd.keys())
            for data in pool_output:
                self.trd.update(data)
        self.__sort_trd_sheet()

class py2excel(Lam2300):
    STR_TOC = 'Table_of_Contents'
    STR_FORM = 'A3_Form'
    '''
    This class generates an excel trd
    Parameter:
    filename(str): filename of the excel trd
    trdObj (dict): a dictionary generated by kiyof.create_trd()
    Note:
    wsObj.dim_rowmax return the last row in the current sheet
    '''
    def __init__(self, filename, trdObj, A3Obj, ziplog=[], xmllog=[], **kwargs):
        self.trd = trdObj
        self.A3 = A3Obj
        self.filename = f'{self.HDFS_PATH}/{filename}' if 'hdfs' in kwargs\
            else filename
        #self.filename = f'{self.HDFS_PATH}/{filename}' # PS EIDH            
        self.ziplog = ziplog
        self.xmllog = xmllog
        self.writerObj = pd.ExcelWriter(self.filename, engine='xlsxwriter')
        self.wbObj  = self.writerObj.book
        # https://xlsxwriter.readthedocs.io/example_conditional_format.html
        # Add a format. Light red fill with dark red text.
        self.redcell_fmt = self.wbObj.add_format({'bg_color': '#FFC7CE',
                                        'font_color': '#9C0006'})
        # Add a format. Green fill with dark green text.
        self.grncell_fmt = self.wbObj.add_format({'bg_color': '#C6EFCE',
                                             'font_color': '#006100'})
        # Add a format. Yellow fill with dark yellow text.
        self.yelcell_fmt = self.wbObj.add_format({'bg_color': '#FFEB9C',
                                             'font_color': '#9C6500'})
        # Add a format. Blue fill with dark blue text.
        self.bluecell_fmt = self.wbObj.add_format({'bg_color': '#DAEEF3',
                                             'font_color': '#27697B'})   
        # Add a format. Grey fill with black text.        
        self.greycell_fmt = self.wbObj.add_format({'bg_color': '#F2F2F2',
                                             'font_color': '#000000'})         
        self.blkcell_fmt = self.wbObj.add_format()
        self.url_fmt = self.wbObj.get_default_url_format()
        self.header_fmt = self.wbObj.add_format({'border': 2,
            'bold': True, 'font_color': 'black', 'underline':  0,
            'center_across' :False, 'bg_color' : '#D3D3D3'})
        logger.info(f'Generating {self.filename}')
        
    def create_A3(self): # PS
        self.wsObj_A3 = self.wbObj.add_worksheet(self.STR_FORM)
        self.writerObj.sheets[f'{self.STR_FORM}'] = self.wsObj_A3
        # remove the Unnamed columns
        self.A3 = self.A3.loc[:, ~self.A3.columns.str.contains('^Unnamed')]
        # change 'Item' to the position that before 'chkItemName'
        self.A3.insert( 5, 'Item', self.A3.pop('Item') )
        self.A3.to_excel(self.writerObj, sheet_name=self.STR_FORM, startrow = 1 , header = True, index = False)        
        
        #DYNAMICALLY RE-SIZE EACH COLUMN
        tmp = self.A3 # self.A3.reset_index()
        for idx, attr in enumerate(tmp.columns):
            #content_width = tmp.loc[:,attr].astype(str).str.len().max() #content_width = 20
            content_width = tmp.loc[:,attr].apply(lambda _elem: len(f'{_elem}')).max()
            if isinstance(attr, tuple):
                header_width = np.max([len(_elem) for _elem in attr])
            else:
                header_width = len(f'{attr}')
            column_len = np.min([np.max([content_width, header_width, 10])+2,60])
            self.wsObj_A3.set_column(idx, idx, column_len)
        # spec color
        col_start = list( self.A3.columns[ self.A3.columns.str.contains('golden') ])[0] if self.A3.columns.str.contains('golden').any() else 'UCL' # tool_pm(golden) or UCL
        n1, m1 = self.A3.shape
        number, item_pre = 0,  ''
        for (idx_info, sample),jrow in zip(self.A3.iterrows(),range(2, 2+n1)) :
            #self.A3.columns.str.contains('golden')            
            excel_loc = self.query_excel_idx( jrow, self.A3.columns.get_loc(col_start)+1, jrow, self.wsObj_A3.dim_colmax )
            ucl = sample['UCL']
            lcl =  sample['LCL']
            self.wsObj_A3.conditional_format(f'{excel_loc}', 
                    {'type': 'blanks','stop_if_true': True,
                    'format': self.blkcell_fmt})
            self.wsObj_A3.conditional_format(f'{excel_loc}',
                    {'type': 'cell', 'criteria': 'between',
                    'minimum': lcl, 'maximum': ucl,
                    'format': self.grncell_fmt})
            self.wsObj_A3.conditional_format(f'{excel_loc}',
                    {'type': 'cell', 'criteria': 'not between',
                    'minimum': lcl, 'maximum': ucl,
                    'format': self.redcell_fmt})  
            # grey and write color by items
            item = sample['One-Shot-Macro Name']
            excel_loc = self.query_excel_idx( jrow, 0, jrow, self.A3.columns.get_loc('UCL') ) 
            if item != item_pre: 
                number += 1
            item_pre = item    
            if number%2 == 1:  
                #logger.info(f'jrow-{jrow};excel_loc-{excel_loc}')
                self.wsObj_A3.conditional_format(f'{excel_loc}', 
                        {'type': 'blanks', 'stop_if_true': True,
                         'format': self.greycell_fmt})    
                # PS text grey
                self.wsObj_A3.conditional_format(f'{excel_loc}', 
                       {'type': 'text', 'criteria': 'not containing', 'value':  'xyzzyx',
                        'format': self.greycell_fmt})                    
                #self.wsObj_A3.conditional_format(f'{excel_loc}', # failed in M
                #       {'type': 'cell','criteria': '>=', 'value':  -9999999,
                #        'format': self.greycell_fmt})                            
            # golden column light blue
            if 'golden' in col_start : 
                excel_loc = self.query_excel_idx( jrow, self.A3.columns.get_loc(col_start), jrow, self.A3.columns.get_loc(col_start) )
                self.wsObj_A3.conditional_format(f'{excel_loc}', 
                        {'type': 'blanks','stop_if_true': True,
                        'format': self.bluecell_fmt})   
                self.wsObj_A3.conditional_format(f'{excel_loc}', 
                       {'type': 'cell','criteria': '>=', 'value':  -9999999,
                        'format': self.bluecell_fmt})                                  
        # hide
        self.wsObj_A3.set_column('A:A', None, None, {'hidden': True})
        self.wsObj_A3.set_column('B:B', None, None, {'hidden': True})
        self.wsObj_A3.set_column('C:C', None, None, {'hidden': True})
        # hide ShowSpec and dataType
        location = self.A3.columns.get_loc('ShowSpec')
        self.wsObj_A3.set_column(location, location, None, None, {'hidden': True})
        self.wsObj_A3.set_column(location+1, location+1, None, None, {'hidden': True})

    def create_table_of_contents(self):
        self.wsObj_info = self.wbObj.add_worksheet(self.STR_TOC)
        header_info =[               
            (['__version__', __status__],self.blkcell_fmt),
            (['__date__', __date__], self.blkcell_fmt), 
            (["Content","Out of Spec/Mismatch Count"],self.header_fmt)
        ]
        for idx, (item,fmt) in enumerate(header_info):
            for idy, _elem in enumerate(item):          
                self.wsObj_info.write(idx, idy, _elem, fmt)
                self.wsObj_info.set_column(idx, idy, len(_elem)+2)
                if idx == 0: self.info_width = len(_elem)
    def update_table_of_contents(self):
        n0_info = self.wsObj_info.dim_rowmax+1
        self.wsObj_info.write(n0_info, 0, self.sshc_sheet ) # self.sshc
        self.wsObj_info.write(n0_info, 1, self.num_err)
        self.wsObj_info.write_url(n0_info, 0, f'internal:{self.sshc_sheet}!A1', self.url_fmt) # self.sshc
        self.wsObj_info.write(n0_info, 0, self.sshc_sheet, self.url_fmt) # self.sshc
        self.wsObj_info.conditional_format(n0_info, 1, n0_info, 1, 
                {'type': 'blanks','stop_if_true': True,
                 'format': self.blkcell_fmt})
        self.wsObj_info.conditional_format(n0_info, 1, n0_info, 1, 
                {'type': 'cell', 'criteria': '=', 'value':    0,
                'format': self.grncell_fmt})
        self.wsObj_info.conditional_format(n0_info, 1, n0_info, 1, 
                {'type': 'cell', 'criteria': '>=', 'value':    0,
                'format': self.redcell_fmt})
        if self.info_width < len(self.sshc):
            self.info_width = len(self.sshc)
            self.wsObj_info.set_column(0, 0, self.info_width+2)
    def create_sshc_report(self, attr_sshc):
        self.sshc = attr_sshc
        self.num_err = 0
        # Get the xlsxwriter workbook and worksheet objects.
        
        if len(self.sshc)>31: # Adjust sheet name # PS
            self.sshc_sheet = self.sshc[:31]
        else:
            self.sshc_sheet = self.sshc
        self.wsObj = self.wbObj.add_worksheet(f'{self.sshc_sheet}')
        self.writerObj.sheets[f'{self.sshc_sheet}'] = self.wsObj
        self.wsObj.write_url('A1',  f'internal:{self.STR_TOC}!A1')
        self.wsObj.write('A1',  self.STR_TOC, self.url_fmt)
        for attr, data in self.trd[self.sshc][self.TOCSV_DATA].items():
            try:
                self.num_err += self.trd[self.sshc][self.EXCEL_RPT][attr][self.ATTR_OOS].sum()
            except KeyError:
                pass
            if self.ATTR_MATCH in data:
                self.num_err += (data.shape[0] - data[self.ATTR_MATCH].sum())
            self.update_sshc_report(data)    
        self.update_table_of_contents()           
        self.wsObj.write('B1', f"{'Out of Spec' if self.SPEC_ATTR in self.trd[self.sshc] else 'Mismatch Count'}={self.num_err}")
        if self.num_err > 0:
            self.wsObj.set_tab_color('#FFC7CE')
        else:
            self.wsObj.set_tab_color('#C6EFCE')
        try:
            for y, plot_info  in self.trd[self.sshc]['plot'].items():
                self.plot(y=y, **plot_info)
        except Exception as err:
            logger.debug(f'{self.sshc}, {err}')
            pass

        if self.EXCEL_DATA in self.trd[self.sshc]:
            self.export_rawdata()
    def query_excel_idx(self, idx0, idy0, idx1, idy1):
        '''
        Parameter:
        idx0, idx1, idy0, idy1 (int): pd.iloc
        in excel, the starting row index = 1, 
        and the starting column index is 0
        '''

        idy0 = xlsxwriter.utility.xl_col_to_name(idy0)
        idy1 = xlsxwriter.utility.xl_col_to_name(idy1)
        idx0+=1
        idx1+=1
        return f'{idy0}{idx0}:{idy1}{idx1}'

    def plot(self, sheet, x, y, hue, facet, plot_funct, title, unit, yref='', plotloc = 'A', **kwargs):
        data = self.trd[self.sshc][sheet][y].reset_index()
        plt_obj = Plot(data, x, y, hue, facet, plot_funct, yref)
        plt_obj()
        plt_obj.format_plot(title, ylabel=(y if unit == '' else f'{y} {unit}'))
        idx0 = self.wsObj.dim_rowmax + 3
        #insert image into excel
        image = io.BytesIO()
        plt_obj.fig.savefig(image,format='jpeg',bbox_inches='tight',dpi=75)
        self.wsObj.insert_image(plotloc + str(idx0), '', {'image_data': image})
        plt.close(plt_obj.fig)

    def update_sshc_report(self, data):
        #wsObj.dim_rowmax return the last excel_idx (pyidx+1)
        # (+1) START APPENDING FROM THE NEXT ROW
        idx0 = self.wsObj.dim_rowmax + 1
        # Convert the dataframe to an XlsxWriter Excel object.
        data.to_excel(self.writerObj, sheet_name=self.sshc_sheet, # PS
            startrow = idx0, header = True, index = True)
        n1, m1 = data.shape
        # THIS IS BASED ON THE EXCEL FILE, THERE IS A BLANK LINE
        # SEPARATNG THE HEADER AND THE DATA IN MULTIINDEX DATAFRAME
        idx0 += (data.columns.nlevels+1) if data.columns.nlevels>1 else 1
        # THE (idy0-1) COLUMNS CONTAIN THE INDEX INFORMATION
        idy0 = data.index.nlevels 
        # THERE ARE m1+1 ELEMENTS BETWEEN dy0 AND dy1
        # (-1) IS USED TO SIMULATE THE EFFECT OF ARANGE(N)
        idy1 = idy0 + m1 -1
        # Apply a conditional format to the cell range.
        if  self.SPEC_MIN in data.index.names: 
            idx_min = data.index.names.index(self.SPEC_MIN)
            idx_max = data.index.names.index(self.SPEC_MAX)
            
            for (idx_info, sample),jrow in zip(data.iterrows(),range(n1)):
                ucl = idx_info[idx_max]
                lcl = idx_info[idx_min]
                excel_loc = self.query_excel_idx(idx0+jrow, idy0, idx0+jrow, idy1)
                self.wsObj.conditional_format(f'{excel_loc}', 
                    {'type': 'blanks','stop_if_true': True,
                    'format': self.blkcell_fmt})
                self.wsObj.conditional_format(f'{excel_loc}',
                    {'type': 'cell', 'criteria': 'between',
                    'minimum': lcl, 'maximum': ucl,
                    'format': self.grncell_fmt})
                self.wsObj.conditional_format(f'{excel_loc}',
                    {'type': 'cell', 'criteria': 'not between',
                    'minimum': lcl, 'maximum': ucl,
                    'format': self.redcell_fmt})
        elif self.ATTR_MATCH in data.columns:  
            match_idy = xlsxwriter.utility.xl_col_to_name(data.columns.get_loc(self.ATTR_MATCH)+idy0)
            isnull_idy = xlsxwriter.utility.xl_col_to_name(data.columns.get_loc(self.ATTR_ISNULL)+idy0)
            excel_loc = self.query_excel_idx(idx0, idy0, idx0+n1-1, idy1)
            #logger.info( f'{idx0}' )
            xlsx_idx = idx0+1
            # raise Exception
            self.wsObj.conditional_format(excel_loc,
                {"type": "formula",
                  "criteria": f'=AND(${isnull_idy}{xlsx_idx}=0, ${match_idy}{xlsx_idx}=FALSE)',
                  "format": self.redcell_fmt
                })
            self.wsObj.conditional_format(excel_loc,
                {"type": "formula",
                  "criteria": f'=AND(${isnull_idy}{xlsx_idx}>0, ${match_idy}{xlsx_idx}=FALSE)',
                  "format": self.yelcell_fmt
                })
            self.wsObj.conditional_format(excel_loc,
                {"type": "formula",
                  "criteria": f'=${match_idy}{xlsx_idx}=TRUE',
                  "format": self.grncell_fmt
                })
        #DYNAMICALLY RE-SIZE EACH COLUMN
        tmp = data.reset_index()
        for idx, attr in enumerate(tmp.columns):
            #content_width = 20
            content_width = tmp.loc[:,attr].apply(lambda _elem: len(f'{_elem}')).max()
            if isinstance(attr, tuple):
                header_width = np.max([len(_elem) for _elem in attr])
            else:
                header_width = len(f'{attr}')
            column_len = np.min([np.max([content_width, header_width, 10])+2,60])
            self.wsObj.set_column(idx, idx, column_len)
    def export_rawdata(self):
        self.trd[self.sshc][self.EXCEL_DATA].to_excel(self.writerObj, sheet_name=self.sshc_sheet, # PS
            startrow=0, startcol= 100, header=True, index=True)
    def generate_report(self):
        
        self.create_A3()
        self.create_table_of_contents()
        for key in self.trd.keys():
            logger.debug(f'Adding {key} report into {self.filename}')
            self.create_sshc_report(key)
        self.writerObj.save()

class Plot():
    AX_HEIGHT = 4
    AX_WIDTH = 3

    def __init__(self, data, x, y, hue, facet=None, plot_funct='lineplot', yref='', **kwargs):
        self.data = data.copy() #
        self.x = x
        self.y = y
        self.hue = hue
        self.yref = yref
        self.facet = facet
        self.lineplot_config = {
            'markersize':15,
            'alpha':1
        }
        self.plot_funct = plot_funct
        self.fontsize = kwargs['fontsize'] if 'fontsize' in kwargs else 12
        self.ax_height, self.ax_width = kwargs['figsize'] if 'figsize' in kwargs else (self.AX_HEIGHT,self.AX_WIDTH)
        self.markers = {}
        self.bars = {}
        self.boxes = {}

    def format_marker(self):
        markers = ['o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X',',', '.']
        linestyles = ['solid', 'dashed', 'dashdot', 'dotted']
        symbols = list(itertools.product(linestyles, markers))
        idx = len(self.markers)%len(symbols)
        ls, marker = symbols[idx]   
        ftm = {
            'marker':marker,
            'linestyle':ls
        }
        return ftm

    def __run_groupby(self, data, attr, default_key):
        '''
        Parameter:
        data (pd.dataframe):
        attr (str): an attribute in data
        default_key (str): 
        '''
        flag = 0
        try:
            data = data.groupby(attr)
            partition = {}
            for param, sample in data:
                partition.update({param:sample})
            flag = 1
        except (ValueError,TypeError):
            partition = {default_key:data}
        return partition, flag

    def __make_subplots(self, ncols, **kwargs):
        #separate data by facet options
        self.facet_df, _ = self.__run_groupby(
                            data=self.data, 
                            attr=self.facet, 
                            default_key=self.y)

        if len(self.facet_df) < ncols:
            ncols = len(self.facet_df)
            nrows = 1
        else:
            nrows = round(len(self.facet_df)/ncols+0.49)
        
        # if ncols == 1:
        #     figsize = (self.ax_height*2, self.ax_width*2)
        if nrows == 1:
            figsize = (self.ax_height*2*ncols, self.ax_width*2)
        else:
            figsize = (self.ax_height*ncols, self.ax_width*nrows)
            self.lineplot_config['markersize'] = 10

        pyplot_opts={    
            'nrows':nrows,
            'ncols':ncols,
            'sharex':False,
            'sharey':False,
            'figsize':figsize
        }   
        self.pyplot_opts = pyplot_opts
        self.fig, self.axs = plt.subplots(**pyplot_opts)

        try:
            self.axs = self.axs.flatten()
        except AttributeError:
            self.axs = [self.axs]

    def format_plot(self, title, xlabel=None, ylabel=None, **kwargs):
        
        if self.pyplot_opts['ncols'] == 1:
            self.axs[0].set_xlabel(xlabel if xlabel else self.x)
            self.axs[0].set_ylabel(ylabel if ylabel else self.y)
            self.axs[0].set_title(title)
        else:
            self.fig.add_subplot(111, frameon=False)
            # hide tick and tick label of the big axes
            plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)
            plt.grid(False)
            plt.xlabel(xlabel if xlabel else self.x)
            plt.ylabel(ylabel if ylabel else self.y)
            self.fig.suptitle(title)
            
        self.fig.legend(self.legend.values(), self.legend.keys(),loc='center left', bbox_to_anchor=(1.04, 0.5))
        self.fig.tight_layout()

    def lineplot(self, ax, key, data, **kwargs):
        if key not in self.markers:
            self.markers[key] = self.format_marker()
        self.lineplot_config.update(self.markers[key])
        try:
            data.sort_values(self.x,inplace=True)
            ax.plot(data[self.x], data[self.y], label=key, **self.lineplot_config)
            ax.tick_params(axis='x', labelrotation = 45, labelsize = 6) # PS   # ax.tick_params(axis='x')         
        except KeyError:
            data.reset_index(inplace=True, drop=True)
            ax.plot(data[self.y], label=key, **self.lineplot_config)

    def barplot(self, ax, key, data, idx, facet_key, facet_data, **kwargs):
        width = 0.8
        if self.bars[facet_key]:
            data[self.x] = data[self.x].map(self.bars[facet_key]['xlabel'])
            data[self.x] += (self.bars[facet_key]['dx']*idx -  width/2)
            ax.bar(self.x, self.y, bottom=None, width=self.bars[facet_key]['dx'], align='edge', data=data, label=key)
            ax.set_xticks(list(self.bars[facet_key]['xlabel'].values()))
            ax.set_xticklabels( list(self.bars[facet_key]['xlabel'].keys()))
            ax.tick_params(axis='x', labelrotation = 45)
        else:            
            xaxis = facet_data[self.x].unique()
            xaxis.sort()
            self.bars[facet_key]['xlabel'] = {_elem:_idx  for _idx, _elem in enumerate(xaxis)}
            hue = facet_data[self.hue].unique()
            self.bars[facet_key]['dx'] = width if self.hue == self.x else width/len(hue)
            self.barplot(ax, key, data, idx, facet_key, facet_data, **kwargs)

    def boxplot(self, ax, key, data, idx, facet_key, facet_data, **kwargs):
        '''
        Parameters:
        data (pd.DataFrame): trace-data to be used for generating a trace 
        facet_data (pd.DataFrame): data to be used for generating the whole plot
        '''
        width = 0.8
        if self.boxes[facet_key]:
            positions = [ 
                self.boxes[facet_key]['xlabel'].get(x_value) \
                    + self.boxes[facet_key]['dx']*idx\
                    -  width/2*(self.hue != self.x)
                for x_value in data[self.x].unique()
            ]            
            axObj = ax.boxplot(
                x=self.y, 
                positions=positions, 
                widths=self.boxes[facet_key]['dx'], 
                data=data, 
                patch_artist=True
            )
            for patch in axObj['boxes']:
                patch.set_facecolor(self.boxes['color'].get(key))
            ax.set_xticks(list(self.boxes[facet_key]['xlabel'].values()))
            ax.set_xticklabels( list(self.boxes[facet_key]['xlabel'].keys()))
            ax.tick_params(axis='x', labelrotation = 45)
        else:            
            xaxis = facet_data[self.x].unique()
            xaxis.sort()
            self.boxes[facet_key]['xlabel'] = {_elem:_idx  for _idx, _elem in enumerate(xaxis)}
            hue = facet_data[self.hue].unique()
            self.boxes[facet_key]['dx'] = width if self.hue == self.x else width/len(hue)
            self.boxplot(ax, key, data, idx, facet_key, facet_data, **kwargs)

    @staticmethod
    def _to_numeric(value):
        try:
            value = float(value)
        except ValueError:
            value = np.NaN
        return value


    def _get_colors(self):
        if self.hue:
            hue = self.data[self.hue].unique()
            cm = plt.cm.get_cmap('rainbow')
            num_hue = len(hue)
            self.boxes['color'] = {key:cm(val/num_hue) for val, key in enumerate(hue)}

    def _augm_plot(self, data, attrs, ax, ls='--', color ='r'):
        if isinstance(attrs, str):
            attrs = [attrs]
        if attrs[0] in data: # 
            data = data[data[attrs[0]].notnull()]
            op_window = data[[self.x]+ attrs].groupby(self.x).mean()
            if op_window.shape[0] == 1:
                _ = [ax.axhline(op_window.iloc[0][attr], ls=ls, color=color, label=attr)
                for attr in attrs]
            else:
                op_window.sort_index(inplace=True)
                _ = [ax.plot(op_window[attr], ls=ls, color=color, label=attr)
                for attr in attrs]

    def __call__(self, ncols=3, **kwargs):
        SPEC_MIN = 'LCL'
        SPEC_MAX = 'UCL'
        plt.rcParams.update({'font.size': self.fontsize})
        self.data.dropna(axis= 1, how='all', inplace=True)
        if self.facet:
            self.data[self.facet] = self.data[self.facet].astype(str)
        try:
            self.data[[self.x, self.y]] = self.data[[self.x, self.y]].astype(float)
        except ValueError:
            tmp = self.data[self.x].apply(Plot._to_numeric).dropna()
            if tmp.shape[0] == 0:
                self.data[self.y] = self.data[self.y].astype(float)
            else:
                self.data = self.data.loc[tmp.index]
                self.data[[self.x, self.y]] = self.data[[self.x, self.y]].astype(float)
        except (KeyError,TypeError):
            self.data[self.y] = self.data[self.y].astype(float)
        self._get_colors()
        self.__make_subplots(ncols, **kwargs)
        self.legend = {}
        for idx, ((facet_key, facet_data), ax) \
            in enumerate(zip(self.facet_df.items(),self.axs)): 
            self.bars[facet_key] = {}
            self.boxes[facet_key] = {}
            ax.grid(b=True, which='major')
            rgb_df, _ = self.__run_groupby(
                                data=facet_data, 
                                attr=self.hue, 
                                default_key=facet_key)            
            for rgb_idx, (rgb_key, rgb_data) in enumerate(rgb_df.items()):
                getattr(self, self.plot_funct)(
                    ax=ax, key=rgb_key, 
                    data=rgb_data, 
                    idx=rgb_idx*(self.hue != self.x),
                    facet_key=facet_key,
                    facet_data=facet_data
                )
            if re.search( 'Fixed.*GasOrifice', facet_data['SSHC'].unique()[0] , re.IGNORECASE ): # PS add for Winbond Fixed.*GasOrifice
                facet_data[self.x] = facet_data[self.x] -1
                logger.info(f'facet_data-{facet_data}')
            self._augm_plot(  
                data=facet_data, 
                attrs=[SPEC_MIN, SPEC_MAX],
                ax=ax
            )
            #logger.info(f'y: {self.y}; sshc: {self.data["SSHC"].unique()} ')  # PS 
            # PS add 3 sigma line for Bias_Voltage_Probe [Winbond EIDH]
            # if re.search( 'Bias_Voltage_Probe', facet_data['SSHC'].unique()[0] , re.IGNORECASE ) and self.y == 'Slope': 
            #     SIGMA_MIN = 'LCL_3sigma'
            #     SIGMA_MAX = 'UCL_3sigma'
            #     facet_data[SIGMA_MIN] = np.mean(facet_data[self.y])-3*statistics.stdev(facet_data[self.y]) # self.data
            #     facet_data[SIGMA_MAX] = np.mean(facet_data[self.y])+3*statistics.stdev(facet_data[self.y])                
            #     #logger.info(f'facet_data: {facet_data}')  #psa
            #     self._augm_plot(  
            #          data=facet_data, 
            #          attrs=[SIGMA_MIN, SIGMA_MAX],
            #          ax=ax, color = 'blue'
            #        )              
            self._augm_plot( 
                data=facet_data, 
                attrs=self.yref,
                ax=ax,
                ls='-', 
                color ='green'
            )
            if self.pyplot_opts['ncols'] > 1:
                ax.set_title(facet_key if isinstance(facet_key, str) else f'{", ".join(facet_key)}')
            handles, labels = ax.get_legend_handles_labels()
            self.legend.update({label:handle  for handle, label in zip(handles,labels)
                    if label not in self.legend})         

class Logger(lamsqlite.LamSqliteUtil): # lamsqlite.LamSqlite
    KEY_LINKAGE = 'FOREIGN KEY'
    #KEY_PK = 'PRIMARY KEY' # 
    TB_REPORTS = 'REPORTS'
    DELETE_STATE = 'DELETE'
    ARCHIVE_STATE = 'ARCHIVE'
    
    TB_SCENARIOS = 'SCENARIOS'     # PS
    ATTR_SCENARIOS = {
        's_id':'INTEGER',
        'tooltype_id':'INTEGER',
        'tool_id':'INTEGER'
    }    
    CONSTR_SCENARIOS = {
        's_id' : 'PRIMARY KEY AUTOINCREMENT',
        'tooltype_id': KEY_LINKAGE,
        'tool_id': KEY_LINKAGE
    }
    ALIAS_SCENARIOS = {
    }
    
    TB_TOOLTYPE = 'TOOLTYPE'
    ATTR_TOOLTYPE = {
        'tooltype_id':'INTEGER',
        'tooltype_name' : 'TEXT'
    }    
    CONSTR_TOOLTYPE = {
        'tooltype_id' : 'PRIMARY KEY AUTOINCREMENT'
    }
    ALIAS_TOOLTYPE = {
    }    

    TB_TOOLS = 'TOOLS'
    ATTR_TOOLS = {
        'tool_id':'INTEGER',
        'tool_name' : 'TEXT',
        'tooltype_id': 'INTEGER',
        'application': 'TEXT'
    }    
    CONSTR_TOOLS = {
        'tool_id' : 'PRIMARY KEY AUTOINCREMENT',
         'tooltype_id' : KEY_LINKAGE
    }
    ALIAS_TOOLS = {
    }      # PS
    
    ATTR_REPORTS = {
        'report_id':'INTEGER',
        'filename' : 'TEXT',
        'status': 'TEXT',
        'created_date' : 'DATETIME'
    }    
    CONSTR_REPORTS = {
        'report_id' : 'PRIMARY KEY AUTOINCREMENT',
        'filename' : 'UNIQUE'
    } 
    ALIAS_REPORTS = { 
    } # PS

    TB_ZIPS = 'ZIPS'
    ATTR_ZIPS = {
        'zip_id':'INTEGER',
        'filename' : 'TEXT',
        'status' : 'TEXT',
        'created_date' : 'DATETIME',
        'accessed_date' : 'DATETIME',
    }    
    CONSTR_ZIPS = {
        'zip_id' : 'PRIMARY KEY AUTOINCREMENT',
        'filename' : 'UNIQUE'
    } 
    ALIAS_ZIPS = {
    } # PS

    TB_XMLS = 'XMLS'
    ATTR_XMLS = {
        'xml_id':'INTEGER',
        'filename' : 'TEXT',
        'status' : 'TEXT',
        'created_date' : 'DATETIME',
        'accessed_date' : 'DATETIME'
    }    
    CONSTR_XMLS = {
        'xml_id' : 'PRIMARY KEY AUTOINCREMENT',
        'filename' : 'UNIQUE'
    } 
    ALIAS_XMLS = {
    } # PS

    TB_LINK_XMLS = 'REPORT_XMLS'
    ATTR_LINK_XMLS = {
        'report_id':'INTEGER',
        'xml_id':'INTEGER',
    }    
    CONSTR_LINK_XMLS = {
    } 
    ALIAS_LINK_XMLS = {
    } # PS

    TB_LINK_ZIPS = 'REPORT_ZIPS'
    ATTR_LINK_ZIPS = {
        'report_id':'INTEGER',
        'zip_id':'INTEGER',
    }    
    CONSTR_LINK_ZIPS = {
    } 
    ALIAS_LINK_ZIPS = {
    } # PS    
    def __init__(self, filename='autotrd_logger_test', path='./tmp/'): # '/tmp' ; autotrd_logger; PS EIDH 
        path_copy = './'  # PS
        self.filename = filename # PS
        self.path = path + '/' # PS
        if self.filename+'.db' in os.listdir(path_copy): # check if './' has logger db  # PS
            shutil.copy( path_copy+self.filename+'.db', self.path+self.filename+'.db')
            logger.info( f'Copy db file from {path_copy+self.filename}.db' ) # autotrd_logger_test; /tmp/
        #logger.info( f'{self.filename}; {self.path}' ) # autotrd_logger_test; /tmp/
        filename = os.path.join(path, filename)
        super().__init__(filename)
        logger.info(f'Writing db file under {filename}') # /tmp/autotrd_logger_test; /tmp/
        self.error_code = 0
        self.SCHEMA = {
            self.TB_TOOLTYPE:{ # PS
                self.SCHEMA_ATTR:self.ATTR_TOOLTYPE,
                self.SCHEMA_CONSTR:self.CONSTR_TOOLTYPE,
                self.SCHEMA_CHILD:[self.TB_TOOLS, self.TB_SCENARIOS],
                self.SCHEMA_ALIAS:self.ALIAS_TOOLTYPE
            },            
            self.TB_TOOLS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_TOOLS,
                self.SCHEMA_CONSTR:self.CONSTR_TOOLS,
                self.SCHEMA_CHILD:[self.TB_SCENARIOS],
                self.SCHEMA_ALIAS:self.ALIAS_TOOLS
            },    
            self.TB_SCENARIOS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_SCENARIOS,
                self.SCHEMA_CONSTR:self.CONSTR_SCENARIOS,
                self.SCHEMA_CHILD:[],
                self.SCHEMA_ALIAS:self.ALIAS_SCENARIOS
            },              
            
            self.TB_REPORTS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_REPORTS,
                self.SCHEMA_CONSTR:self.CONSTR_REPORTS,
                self.SCHEMA_CHILD:[self.TB_LINK_ZIPS, self.TB_LINK_XMLS],
                self.SCHEMA_ALIAS:self.ALIAS_REPORTS
            },
            self.TB_ZIPS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_ZIPS,
                self.SCHEMA_CONSTR:self.CONSTR_ZIPS,
                self.SCHEMA_CHILD:[self.TB_LINK_ZIPS],
                self.SCHEMA_ALIAS:self.ALIAS_ZIPS
            },
            self.TB_XMLS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_XMLS,
                self.SCHEMA_CONSTR:self.CONSTR_XMLS,
                self.SCHEMA_CHILD:[self.TB_LINK_XMLS],
                self.SCHEMA_ALIAS:self.ALIAS_XMLS
            },
            self.TB_LINK_XMLS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_LINK_XMLS,
                self.SCHEMA_CONSTR:self.CONSTR_LINK_XMLS,
                self.SCHEMA_CHILD:[],
                self.SCHEMA_ALIAS:self.ALIAS_LINK_XMLS
            },
            self.TB_LINK_ZIPS:{ # PS
                self.SCHEMA_ATTR:self.ATTR_LINK_ZIPS,
                self.SCHEMA_CONSTR:self.CONSTR_LINK_ZIPS,
                self.SCHEMA_CHILD:[],
                self.SCHEMA_ALIAS:self.ALIAS_LINK_ZIPS
            }
        }

    def _build_schema(self):
        for key, value in self.SCHEMA.items():
            self._create_table(tb = key, **value)

    def __call__(self):
        self._augm_schema() # PS
        self._build_schema()

    def _update_v2(self, tb, filename, platform_id, dt): # update from new version # PS
        '''
        check if an file is in the database, 
        if yes, run an update command
        if no, run an insert command
        '''
        schema = self.SCHEMA.get(tb)
        primary_key = schema[self.SCHEMA_PK]
        cmd = f'SELECT {primary_key} FROM {tb} WHERE filename = "{filename}" AND status="{self.ARCHIVE_STATE}" LIMIT 1'
        self.cursor.execute(cmd)
        try:
            pk_value = self.cursor.fetchone()[0]
            attrs = ['accessed_date']
            constrs = [primary_key]
            data = (dt, pk_value)
            super()._update(tb, attrs, constrs, data)
        except TypeError:
            cmd = f'''
            SELECT {primary_key} 
            FROM {tb} 
            WHERE filename = "{filename}"
            '''
            self.cursor.execute(cmd)
            try:
                pk_value = self.cursor.fetchone()[0]
                attrs = ['accessed_date', 'status']
                constrs = [primary_key]
                data = (dt, self.ARCHIVE_STATE, pk_value)
                super()._update(tb, attrs, constrs, data)
            except TypeError:
                pk_value = self._insert(
                        tb=tb, 
                        sql_tuple=[filename, platform_id, self.ARCHIVE_STATE, dt, dt]
                )
        cmd = f'SELECT {primary_key} FROM {tb} WHERE filename = "{filename}" AND platform_id IS NULL LIMIT 1'
        self.cursor.execute(cmd)
        if self.cursor.fetchone():
            self._exec_backfill(tb, filename, platform_id)
        return pk_value
    
class Sandbox(Lam2300):
    '''
    Sandbox class is a autotrd UI. It allows user to interact
    with all this program
    Important attributes
    self.rawdata (dict): {toolid:Lam2300.trd}
    self.rawinfo (dict): {toolid:a list of module aliases}
    self.golden (dict): {toolid:golden_pm}
    self.sample (list): list of tool_ids
    self.err_code (int): 
        0 [=] no error
        1 [=] failed to connect to the local database
        2 [=] no archive platforms found
        3 [=] no platforms selected

    '''
    REGEX_ZIP = 'all|config'
    def __init__(self, specfile, specfile_A3, 
        archive_path=None, 
        enable_filter=False, 
        enable_golden=False, 
        workers = 1,
        hdfs_auth={},
        solr_auth={},
        autopm_auth={},
        local_query = False, # PS
        **kwargs):
        '''
        Parameter:
        specfile (str): absolute path to the spec file
        archive_path (str/None): absolute path to the local archiver
        hdfs_auth (dict): {'customer'->str, 'site'->str, 'hdfs_host'->str, 'hdfs_port'->int}
        solr_auth (dict): {'host':'ip address' , 'port':'8983', 'name':'EIM_SOLR'}
        autopm_auth (dict): {
            'config' (str): 'autopm_config.yaml'
            'xml' (list of strs): sshc tests expected to be identified            
            'basename' (str): keyword (solr param)
            'duration' (int): +/-number of days from a given date (solr param)
        }
        '''
        self.specfile = specfile
        self.specfile_A3 = specfile_A3 # PS
        self.archive_path = archive_path
        self.enable_filter = enable_filter 
        self.enable_golden = enable_golden 
        self.local_query = local_query # PS
        self.err_code = 0   
        try:
            self.hdfs_auth = hdfs_auth  
            self.hdfs = hdfs.connect(self.hdfs_auth['hdfs_host'], self.hdfs_auth['hdfs_port'])
            #self.hdfs = pyarrow.fs.HadoopFileSystem(self.hdfs_auth['hdfs_host'], self.hdfs_auth['hdfs_port']) # backup
            self.solr_auth = solr_auth
            self.sqlObj = Logger(path=self.HDFS_PATH) # PS EIDH keep
            worker = 1
            logger.info('Connecting to /LamData/{customer}/{site},'.format(**self.hdfs_auth))
        except FileNotFoundError:
            if isinstance(self.archive_path,str):
                self.hdfs_auth = {}
                self.solr_auth = {}
                self.sqlObj = Logger()
            else:
                logger.error('failed to connect to the local database.')
                self.err_code = 1
        except KeyError: # PS local file not connect to EIDH
            if isinstance(self.archive_path,str):
                self.hdfs_auth = {}
                self.solr_auth = {}
                self.sqlObj = Logger() # PS EIDH keep
        if self.err_code: 
            return
        self.sqlObj() # PS EIDH keep
        self.autopm_auth = autopm_auth
        self.rawdata = {}
        self.rawinfo = {}
        self.rawlog = {}
        self.golden = {}
        self.sample = []
        self.err_msg = None
        self.workers = mp.cpu_count() if (workers == -1) | (workers > mp.cpu_count()) else workers

    def __query_archive_files(self):
        '''
        __query_archive_files searches for the archive files in the local file system
        '''
        self.filenames = [os.path.join(self.archive_path,_elem) 
                        for _elem in os.listdir(self.archive_path) 
                        if re.search('zip$', _elem, re.IGNORECASE)]
        log_msg = 'Select all archive files to be used in the analysis'        
        tkObj = TkWrapper(self.filenames, log_msg)
        self.filenames = tkObj.items
        if len(self.filenames) == 0:
            logger.error('No archive files was selected.')
            self.__query_archive_files()

    def __query_golden_info(self):
        '''
        __query_golden_info allows user to (1) select 1x archive file to
        represent a golden platform, (2) select a golden chamber among the
        pms from the golden platform, and (3) updates self.sample, 
        self.golden, & self.rawdata
        '''
        self.sample = list(set(self.rawinfo.keys()))
        if len(self.filenames) > 1 and self.enable_golden:
            log_msg = f'''\
            Select all archive files that you wish to specify the golden chambers
            Select <Exit> to not specify any golden chamber chamber for this analysis
            '''
            tkObj = TkWrapper(self.rawinfo.keys(), log_msg)
            if len(tkObj.items) == 0: 
                logger.info('User opted to not specify any golden chambers for this analysis')
            else:
                golden_tool = tkObj.items[0]
                self.sample.remove(golden_tool)
                try:
                    pmaliases = [_elem for _elem in self.rawinfo[golden_tool]
                        if re.search(r'pm\d+', _elem, re.IGNORECASE)]
                    log_msg = f'Select 1x PM from {golden_tool} to be specified as the golden chamber'
                    tkObj = TkWrapper(pmaliases, log_msg)
                    golden_pm = tkObj.items[0]
                    self.golden[golden_tool]= golden_pm
                    # pmaliases.remove(golden_pm)
                    for subsys, info in self.rawdata[golden_tool].items():
                        for pmalias in pmaliases:
                            if pmalias in info[self.ZIP_DATA]:
                                #ONLY KEEP THE CV DATA FROM THE GOLDEN CHAMBER
                                if (pmalias == golden_pm) \
                                and re.search('txt|cv', info[self.ZIP_DTYPE], re.IGNORECASE):
                                    continue
                                if pmalias in info[self.ZIP_FNAME]:
                                    del info[self.ZIP_FNAME][pmalias]
                                del info[self.ZIP_DATA][pmalias]
                except IndexError:
                    logger.info(f'Input Error, no golden chamber was specified in {golden_tool}')
                    self.__query_golden_info()

    def __apply_pm_filter(self):
        '''
        __apply_pm_filter allows user to (1) select all pms
        to be used in this analysism and (2) updates self.rawdata
        '''
        delimiter = '_'
        pmaliases =  [[f'{toolid}{delimiter}{_elem}, {module_type}' 
                for _elem, module_type in self.rawinfo[toolid].items()
                if re.search(r'pm\d+', _elem, re.IGNORECASE)]
                for toolid in self.sample]
        pmaliases = list(itertools.chain.from_iterable(pmaliases))
        log_msg = f'Select all PMs to be used in the analysis'
        tkObj = TkWrapper(pmaliases, log_msg)
        try: 
            if len(tkObj.items) == 0: 
                raise ValueError('No PMs was selected in this analysis')
            pmaliases = [_elem for _elem in pmaliases if _elem not in tkObj.items]
            for pmalias in pmaliases:
                toolid, pmalias = re.split(f'\\{delimiter}', pmalias)
                for subsys, info in self.rawdata[toolid].items():
                    if pmalias in info[self.ZIP_DATA]:
                        if pmalias in info[self.ZIP_FNAME]:
                            del info[self.ZIP_FNAME][pmalias]
                        del info[self.ZIP_DATA][pmalias]
        except ValueError as err:
            logger.error(err)
            self.__apply_pm_filter()

    def _update_logger(self, wbObj, dtfmt):
        filename = os.path.basename(wbObj.filename)
        dt = re.search(r'\w+_(.*?)\.xlsx', filename, re.IGNORECASE).group(1)
        dt = dt.split('_')[-1]
        dt = datetime.datetime.strptime(dt, dtfmt)
        dt = dt.strftime('%Y-%m-%d %H:%M:%S.%f')
        
        report_id = self.sqlObj._insert(
            tb = self.sqlObj.TB_REPORTS, 
            tosql = (filename, self.sqlObj.ARCHIVE_STATE, dt)
        )
        inputs = {
            self.sqlObj.TB_ZIPS : (self.sqlObj.TB_LINK_ZIPS, wbObj.ziplog),
            self.sqlObj.TB_XMLS : (self.sqlObj.TB_LINK_XMLS, wbObj.xmllog)
        }

        for tb, (linkage_tb, data) in inputs.items():
            for filename in data:
                pk_value = self.sqlObj._update_v2(tb, filename, dt) # PS
                self.sqlObj._insert(
                    tb=linkage_tb, 
                    tosql= (report_id, pk_value)
                )
        self.sqlObj.cnx.commit()

    def main(self, **kwargs):
        if hasattr(self, 'hdfs'): 
            kwargs['hdfs'] = self.hdfs
        dtfmt = '%Y-%m-%d--%H-%M-%S'
        dt = datetime.datetime.now().strftime(dtfmt)
        self.out_files = []
        if len(self.golden) == 0:
            tool_names = '_'.join(self.rawdata.keys())
            filename = rf'TRD_{tool_names}_{dt}.xlsx'
            autotrd_obj = autotrd(
                rawdata=self.rawdata,
                specfile=self.specfile, 
                specfile_A3=self.specfile_A3, # PS
                rawinfo=self.rawinfo,
                rawlog=self.rawlog,
                autopm_auth=self.autopm_auth,
                application = self.application, #PS
                # workers=self.workers
            )
            autotrd_obj()
            wbObj = py2excel(
                filename=filename, 
                trdObj=autotrd_obj.trd,
                A3Obj = autotrd_obj.speclib_A3, # PS
                ziplog=list(autotrd_obj.rawlog.values()),
                xmllog=autotrd_obj.xmllog, 
                **kwargs
            )
            wbObj.generate_report()
            self.out_files.append(filename)
            self.A3 = wbObj.A3 # PS           
            #self._update_logger(wbObj, dtfmt) # PS EIDH comment
        else:
            for golden, pm in self.golden.items(): # all tools in one file
                self.rawdata_new = {key: value for key, value in self.rawdata.items() if key != golden }
                self.rawlog_new = {key: value for key, value in self.rawlog.items() if key != golden }
                tool_names = '_'.join(self.rawdata_new.keys())
                filename = f'TRD_{tool_names}_golden{golden}_{pm}_{dt}.xlsx' # filename = f'TRD_{dt}.xlsx'    
                
                rawdata = { golden:self.rawdata[golden] }
                rawdata.update( self.rawdata_new ) 
                rawlog = { golden:self.rawlog[golden] }
                rawlog.update( self.rawlog_new ) 
                autotrd_obj = autotrd(
                    rawdata=rawdata,
                    specfile=self.specfile, 
                    specfile_A3=self.specfile_A3, # PS
                    golden_pm=f'{golden}_{self.golden[golden]}',                    
                    rawlog=rawlog,
                    autopm_auth=self.autopm_auth,
                    application = self.application # PS
                )
                autotrd_obj()
                wbObj = py2excel(
                    filename=filename, 
                    trdObj=autotrd_obj.trd,
                    A3Obj = autotrd_obj.speclib_A3, # PS
                    ziplog=list(autotrd_obj.rawlog.values()),
                    xmllog=autotrd_obj.xmllog,
                    **kwargs
                )
                wbObj.generate_report()
                self.out_files.append(filename)
            # for golden, sample in list(itertools.product(self.golden.keys(), self.sample)): # seperate files for tools
            #     filename = f'TRD_{sample}_{dt}.xlsx'
            #     rawdata = {
            #         golden:self.rawdata[golden],
            #         sample:self.rawdata[sample]
            #     }
            #     rawlog = {
            #         golden:self.rawlog[golden],
            #         sample:self.rawlog[sample]
            #     }
            #     autotrd_obj = autotrd(
            #         rawdata=rawdata,
            #         specfile=self.specfile, 
            #         specfile_A3=self.specfile_A3, # PS
            #         golden_pm=f'{golden}_{self.golden[golden]}',                    
            #         rawlog=rawlog,
            #         autopm_auth=self.autopm_auth,
            #         application = self.application # PS
            #     )
            #     autotrd_obj()
            #     wbObj = py2excel(
            #         filename=filename, 
            #         trdObj=autotrd_obj.trd,
            #         A3Obj = autotrd_obj.speclib_A3, # PS
            #         ziplog=list(autotrd_obj.rawlog.values()),
            #         xmllog=autotrd_obj.xmllog,
            #         **kwargs
            #     )
            #     wbObj.generate_report()
            #     self.out_files.append(filename)
                #self._update_logger(wbObj, dtfmt) # PS EIDH comment
        #clear_output() # PS   
        self._archive_xlsx() # PS EIDH comment
        for out_file in self.out_files:
            if kwargs:
                if os.path.exists(out_file):
                    os.remove(out_file)
                shutil.copyfile(f'{self.HDFS_PATH}/{out_file}', out_file)
            #if os.path.exists(out_file): # PS
            #    os.remove(out_file) 
            #shutil.copyfile(f'{self.HDFS_PATH}/{out_file}',out_file)   # PS EIDH
            logger.debug(f'downloading {out_file}')
            display(FileLink(out_file))
            # folder = '/mnt-jupyter/user/admin/notebooks/Auto-TRD/Report/' # PS
            # shutil.copyfile(f'{self.HDFS_PATH}/{out_file}', folder+out_file)               
            # filename_copy = f'Auto-TRD/Report/{out_file}' 
            # logger.info(f'Copy file to {filename_copy}')

    def _archive_xlsx(self):
        path = './archive'
        if not os.path.exists(path):
            os.mkdir(path)
        for filename in os.listdir():
            if filename in self.out_files: 
                continue
            if re.search('^trd.*?xlsx$', filename, re.IGNORECASE):
                _ = shutil.copyfile(filename, os.path.join(path,filename))
                try: 
                    os.remove(filename)
                except PermissionError:
                    pass
            
    def __call__(self):
        if self.err_code: 
            return 
        self.__query_archive_files()
        for filename in self.filenames:
            lam_obj = Lam2300(filename)
            lam_obj()
            self.rawlog[lam_obj.toolid] = os.path.basename(filename)
            self.rawdata[lam_obj.toolid] = lam_obj.trd
            self.rawinfo[lam_obj.toolid] = {}
            for pmalias, data in lam_obj.module.items():
                aliases = [pmalias]
                aliases += data[self.ATTR_KEY]
                self.rawinfo[lam_obj.toolid].update({alias:data[self.ATTR_MODULE] for alias in aliases})
        self.__query_golden_info() 
        if self.enable_filter: 
            self.__apply_pm_filter()        
        self.main()  

class TkWrapper():
    '''
    https://code-maven.com/slides/python/tk-listbox-multiple
    https://pythonguides.com/python-tkinter-filter/
    '''
    TXT_SIZE = 12
    FACTOR_SIZE = 9 
    def __init__(self, ldata, log_mgs):
        self.opt = ldata
        self.log_mgs = log_mgs
        self.items = []        
        self.ui = tk.Tk()
        self.__make_ui()
    def __init_ui(self):
        #setting tkinter window size
        self.height= self.ui.winfo_screenheight()//2
        self.width = np.max([len(_elem) 
                for _elem in list(self.opt)\
                + re.split('\n', self.log_mgs)])\
                * self.FACTOR_SIZE    

        self.ui.geometry(f"{self.width}x{self.height}")
        self.__add_txt()
        self.__add_regrex()
        #create a <Listbox> object
        self.tkObj = tk.Listbox(self.ui, selectmode=tk.MULTIPLE,
                                font=tk.font.Font(size=self.TXT_SIZE))
        self.__create_scrollbar()
        self.tkObj.pack(expand = tk.YES, fill = "both")         
    def __create_scrollbar(self):
        #create a <Scrollbar> object
        vsb = tk.Scrollbar(self.ui, command=self.tkObj.yview)
        self.tkObj.configure(yscrollcommand=vsb.set)
        vsb.pack(side="right", fill="y")        
    def __add_btn(self):
        btn_query =  tk.Button(self.ui, text='Query', width=self.width, command=self.__query)
        btn_query.pack()      
        btn_exit =  tk.Button(self.ui, text='Exit', width=self.width, command=self.ui.destroy)
        btn_exit.pack()          
    def __add_txt(self):
        txt = tk.Label(self.ui, text = self.log_mgs, 
                        font = ("Times", self.TXT_SIZE+3),
                        justify = tk.LEFT,
                        padx = 10, pady = 10)
        txt.pack()
    def __query(self):
        items = self.tkObj.curselection()  # returns a tuple
        self.items = [self.tkObj.get(_elem) for _elem in items] 
    def __add_regrex(self):
        entry = tk.Entry(self.ui, width=self.width)
        entry.pack()
        entry.bind('<KeyRelease>', self.__run_regex)
    def __run_regex(self, event):
        kw = event.widget.get()      
        self.data = self.opt if kw == ''\
            else [_elem for _elem in self.opt
                if re.search(kw, _elem, re.IGNORECASE)]
        self.__update_ui()
    def __update_ui(self):
        self.tkObj.delete(0, 'end')
        try:
            _ = [self.tkObj.insert(tk.END, _elem) for _elem in self.data]
        except AttributeError:
            _ = [self.tkObj.insert(tk.END, _elem) for _elem in self.opt]
    def __make_ui(self):   
        self.__init_ui()
        self.__update_ui()
        self.__add_btn()
        self.ui.mainloop()
        
class WidgetsWrapper():
    # https://gist.github.com/MattJBritton/9dc26109acb4dfe17820cf72d82f1e6f
    def __init__(self, ldata, log_mgs, ext_btn, **kwargs):
        self.opt = ldata
        self.log_mgs = log_mgs
        self.ext_btn = ext_btn
        if 'regex' in kwargs:
            self.regex = kwargs.get('regex')
        self.items=[]
        self.__make_ui()
        display(self.ui)
    def __add_checkbox(self):
        '''
        create a check box for each entry
        '''
        self.options_dict = {
            _elem: ipywidgets.Checkbox( # PS widgets->ipywidgets 
                description=_elem, 
                value=False,
                style={"description_width":"0px"},
                layout=ipywidgets.Layout(width='100%') # PS widgets->ipywidgets
            ) for _elem in self.opt
        }
    def f(self, **args):
        results = [key for key, value in args.items() if value]
        display(results) 
    def __init_ui(self):
        self.caption_widget = ipywidgets.HTML(value = f"<i><font color='black', font size=4>{self.log_mgs}</b>") # PS widgets->ipywidgets
        if hasattr(self,'regex'):
            self.search_widget = ipywidgets.Dropdown( # PS widgets->ipywidgets
                options=self.regex,
                value=None,
                description='Regex String:',
                disabled=False,
                layout=ipywidgets.Layout(width='50%') # PS widgets->ipywidgets
            )
        else:
            self.search_widget = ipywidgets.Text( # PS widgets->ipywidgets
                description='Regex String:',
                layout=ipywidgets.Layout(width='50%') # PS widgets->ipywidgets
            )
        # self.output_widget = widgets.Output()
        self.options = [x for x in self.options_dict.values()] 
        options_layout = ipywidgets.Layout( # PS widgets->ipywidgets
            overflow='auto',
            border='1px solid black', 
            layout={'width': 'max-content'},
            height='200px',
            flex_flow='column',
            display='flex'
        )
        self.options_widget = ipywidgets.VBox(self.options, layout=options_layout)  # PS widgets->ipywidgets
        self.output_txt = ipywidgets.Label(value=', '.join(self.items),  # PS widgets->ipywidgets
            layout={'width': 'max-content'})

        self.btn = ipywidgets.Button(description="Apply")  # PS widgets->ipywidgets
        self.btn.on_click(self.default_btn_funct)
        self.select_btn = ipywidgets.Button(description="Select All", layout=ipywidgets.Layout(width='24.8%'))  # PS widgets->ipywidgets
        self.select_btn.on_click(self.select_btn_funct)
        self.deselect_btn = ipywidgets.Button(description="Deselect All", layout=ipywidgets.Layout(width='24.8%'))  # PS widgets->ipywidgets
        self.deselect_btn.on_click(self.deselect_btn_funct)

        self.ui = ipywidgets.GridspecLayout(n_rows=10, n_columns = 1,
                height='350px') # PS widgets->ipywidgets
        self.ui[0, :] = self.caption_widget 
        self.ui[1, :] = self.search_widget
        self.ui[2, :] = ipywidgets.HBox([self.select_btn,self.deselect_btn]) # PS widgets->ipywidgets
        self.ui[3:-1, :] = self.options_widget
        self.ui[-1, :] = self.ext_btn # widgets.HBox([self.btn,self.ext_btn])
        # self.ui[3:-2, :] = self.options_widget
        # self.ui[-2, :] = widgets.HBox([self.btn,self.ext_btn])
        # self.ui[-1, :] = self.output_txt
    def __update_ui(self, change):
        search_input = change['new']
        if search_input == '':
            # Reset search field
            new_options = sorted(self.options, key = lambda x: x.value, reverse = True)
        else:            
            matched_obj = [_elem for _elem in self.options_dict.keys() 
                    if re.search(search_input, _elem, re.IGNORECASE)]
            new_options = sorted(
                [_elem for _elem in self.options 
                if (_elem.description in matched_obj)
                or (_elem.value)], 
                key = lambda x: x.value, reverse = True
            ) 
        self.options_widget.children = new_options
    def __make_ui(self):
        self.__add_checkbox()
        self.__init_ui()
        self.search_widget.observe(self.__update_ui, names='value')
        self.output_txt.observe(self.handler_funct, names='value')
        # display(self.output_widget)
    def default_btn_funct(self,button):
        self.items = [_elem.description for _elem in self.options if _elem.value] 
        #CHANGE THE OUTPUT_TXT TO TRIGGER AN EXTERNAL HANDLER
        self.output_txt.value = ', '.join(self.items)
        #DESTROY THE BUTTON AFTER 1 CLICK
        self.btn.disabled=True
    def select_btn_funct(self,button):
        for option, checkbox in self.options_dict.items():
            checkbox.value = True
    def deselect_btn_funct(self,button):
        for option, checkbox in self.options_dict.items():
            checkbox.value = False
    def handler_funct(self, change):
        search_input = change['new']
        self.change = change

class HadoopSandbox(Sandbox):
    '''
    HadoopSandbox class is a autotrd UI. It allows user to interact
    with all this program
    See the docstring of Sandbox for details
    '''
    def __create_button(self, usrcmds, btn_func=None, **kwargs): # btn_func=None
        '''
        __create_button creates a ipywidgets.Button in the nb
        that has access to all attributes of this class
        Parameter:
        usrcmds (list of methods): methods (functions) to be
        executed immediately after receiving user inputs
        '''
        self.btn = ipywidgets.Button(
            description = 'Accept',
            layout=ipywidgets.Layout(width='50%') # PS widgets->ipywidgets
        )
        def default_btn_funct(button): 
            # clear_output() # PS
            self.widget_obj.items = [_elem.description 
                for _elem in self.widget_obj.options if _elem.value] 
            if self.err_msg:
                logger.error(self.err_msg)
                self.err_msg = None
            logger.info(f'{self.widget_obj.log_mgs}\nSelected Options: {self.widget_obj.items}')
            #DESTROY THE BUTTON AFTER 1 CLICK
            self.btn.disabled=True
            _ = [funt(**kwargs) for funt in usrcmds]
        #self.btn.on_click(default_btn_funct)
        if btn_func: # PS
            self.btn.on_click(btn_func)
        else:
            self.btn.on_click(default_btn_funct)

    def __run_shell_script(self, path, pipe=[], recursive_mode=False, inplace=True):
        '''
        __run_shell_script excutes the <-ls> command and creates
        the stdout attribute for this class
        Parameter:
        path (str) :: aboslute path
        pipe (list):: post processing cmds
        '''
        opt = '-R' if recursive_mode else ''
        pipe = ' '.join(f'| {_elem}'for _elem in pipe)
        cmd = f"hadoop fs -ls {opt} {path} | awk '{{ print $NF }}' {pipe}"
        process = subprocess.run(cmd, stdout=subprocess.PIPE, 
                        shell=True, stderr=subprocess.PIPE,
                        universal_newlines=True)
        stdout = process.stdout.split('\n')
        if inplace:
            self.stdout = stdout
        else:
            return stdout

    def _solr_search(self):
        '''
        _solr_search uses the Solr database to find backup files.
        This method creates self.tool_ids
        '''
        solr_fmt1 = '%Y-%m-%dT%H:%M:%S.%fZ'
        solr_fmt2 = '%Y-%m-%dT%H:%M:%SZ'
        solr_db={}
        params = {'file_type':'13'}
        self.solrObj = rps_solr.SolrAPI(self.solr_auth, fl='', query_inputs=params)
        self.solrObj()
        for response in self.solrObj.json_r['response']['docs']:
            tool_id = response['tool_ID']
            path = response['file']
            basename = os.path.basename(path)
            if re.search(f'^({self.REGEX_ZIP}).*zip$',basename,re.IGNORECASE):
                try:
                    dt = datetime.datetime.strptime(response['timeStamp'], solr_fmt1)
                except ValueError:
                    dt = datetime.datetime.strptime(response['timeStamp'], solr_fmt2) 
                    logger.info( f"Different time format in Solr:{tool_id}-{response['timeStamp']}" )
                try:
                    solr_db[tool_id][basename]=[tool_id, path, dt]
                except KeyError:
                    solr_db[tool_id]={basename:[tool_id, path, dt]}
        for key, data in solr_db.items():
            '''tmp.items() :: item == (key1,value1), item[1] == value1'''
            data = dict(sorted(data.items(), key=lambda item: item[1][2], reverse=True))
            self.tool_ids[key] = data

    def _hdfs_search(self):
        '''
        _hdfs_search searches hdfs to find backup files.
        This method creates self.tool_ids
        '''
        self.__run_shell_script(path='/LamData/{customer}/{site}'.format(**self.hdfs_auth))
        logger.info('Fetching tool ids...')
        #DROP ALL TOOLS WITHOUT BACKUP DIRECTORY            
        hdfs_dirs =  {_elem.split('/')[-1]:_elem 
            for _elem in self.stdout
            if self.hdfs.isdir(_elem)} 
        logger.info('Fetching hdfs backup files...')
        for tool_id, path in hdfs_dirs.items():  
            path = f'{path}/data/backup'
            if self.hdfs.exists(path):
                #QUERY BACKUP FILES FROM THE HADOOP FILE SYSTEM
                self.__run_shell_script(path=path,
                    # split the input by '/', return the input and the last element
                    pipe=["awk -F'/' '{ print  $0, $NF}'", 
                    # run the regex command, and if matched then return the first input
                        "awk '$NF ~ /^all.*zip/ {print $1}'"],
                     recursive_mode=True)
                tmp = {_elem.split('/')[-1]:[tool_id, 
                    _elem, 
                    datetime.datetime.fromtimestamp(self.hdfs.info(_elem)['last_modified'])]
                    for _elem in self.stdout 
                    if self.hdfs.exists(_elem)}
                if len(tmp) > 0:
                    '''tmp.items() :: item == (key1,value1), item[1] == value1'''
                    tmp = dict(sorted(tmp.items(), key=lambda item: item[1][2], reverse=True))
                    self.tool_ids[tool_id] = tmp

    def query_hdfs_toolids(self):
        '''
        query_hdfs_toolids searches for the folders in the root dir,
        and creates tool_ids attribute
        '''
        if self.err_code: return
        try:
            assert(hasattr(self, 'tool_ids'))
            log_msg = f' The following platforms are found in /LamData/{{customer}}/{{site}}, Select all platforms to be used in the analysis'.format(**self.hdfs_auth)
            self.__create_button([self.__query_hdfs_toolids]) # self.__query_hdfs_files
            self.widget_obj = WidgetsWrapper(list(self.tool_ids.keys()), log_msg, self.btn)                 
        except AssertionError:
            self.tool_ids={}
            if self.solr_auth:
                self._solr_search()
            else:
                self._hdfs_search()
            logger.info('Indexing complete...')
            #IF THE TOOL LIST IS EMPTY, UPDATE THE ERROR CODE TO 2
            if len(self.tool_ids) == 0:
                self.err_code = 2
                raise ValueError ('no archive platforms found.')
            else:
                self.query_hdfs_toolids()

    def query_hdfs_toolids_new(self):
        '''
        query_hdfs_toolids searches for the folders in the root dir,
        and creates tool_ids attribute
        '''
        if self.err_code: 
            return
        try:
            assert(hasattr(self, 'tool_ids'))
            log_msg = f'Select tooltype_application for backup files found in /LamData/{{customer}}/{{site}}.'.format(**self.hdfs_auth)
            self.__create_button([self.query_hdfs_toolids2])  
            self.widget_obj = WidgetsWrapper(list(self.tool_ids.keys()), log_msg, self.btn)                 
        except AssertionError:
            self.tool_ids={}
            if self.solr_auth:
                logger.info(f'Using solr to search backup files') # PS
                self._solr_search() #
            else:
                self._hdfs_search()
            logger.info('Indexing complete...')
            #IF THE TOOL LIST IS EMPTY, UPDATE THE ERROR CODE TO 2
            if len(self.tool_ids) == 0:
                self.err_code = 2
                raise ValueError ('no archive platforms found.')
            else:
                self.tool_ids_copy = copy.deepcopy(self.tool_ids)
                self.tool_ids={}
                # update self.tool_ids with key tooltype_application from self.tool_ids_copy
                for tool_id, data in self.tool_ids_copy.items():
                    if tool_id in self.hash_table_temp.keys():
                        tooltype_app = self.hash_table_temp[tool_id][0]
                        if tooltype_app in self.tool_ids.keys():
                            self.tool_ids[tooltype_app].update(  {tool_id: data}  )
                        else:
                            self.tool_ids.update( { tooltype_app: {tool_id: data}  } )
                self.query_hdfs_toolids_new()
        print("line 5686")
        logger.info(f'self.tool_ids_hdfs-{self.tool_ids}')

    def query_hdfs_toolids2(self): 
        '''
        query_hdfs_toolids2 (1) shows tools in selected tooltype_application 
        in EIDH to update self.tool_ids 
        '''        
        try: 
            assert self.widget_obj.items
            if len(self.widget_obj.items) >1:
                raise ValueError
            self.tool_ids_new = self.tool_ids[ self.widget_obj.items[0] ] # PS
            self.application = self.widget_obj.items[0].split('_')[-1] # selected applicaiton # PS
            self.tooltype_application = self.widget_obj.items[0]            
        except AssertionError:
            logger.info( 'No tooltype_application selected.' )
            return self.__call__()     
        except ValueError:
            logger.info( 'More than 1 tooltype_application are selected. Please re-select' )
            return self.__call__()          
        log_msg = f'Select tools in /LamData/{{customer}}/{{site}}.'.format(**self.hdfs_auth)
        self.__create_button([self.query_hdfs_backup]) #  query_archive_files
        self.widget_obj = WidgetsWrapper(list(self.tool_ids_new.keys()), log_msg, self.btn)  # PS

    def query_hdfs_backup(self): 
        '''
        query_hdfs_backup (1) shows backup files for selected tools to
        create self.zipfiles
        '''          
        try:
            assert self.widget_obj.items
            self.tool_ids_new = {_elem:self.tool_ids_new[_elem] for _elem in self.widget_obj.items} # PS
            self.zipfiles={}
            _ = [self.zipfiles.update(_elem) for _elem in self.tool_ids_new.values()] # PS
            log_msg = f'Select backup files from each of the following tools, {", ".join(self.tool_ids_new.keys())}' # PS
            self.__create_button([self.query_archive_files], fs_type=1)
            zipfiles = [key for key, data in self.zipfiles.items()]
            #f'{data[0]}, {data[-1].strftime("%Y-%m-%d")}, {key}' 
            self.widget_obj = WidgetsWrapper(zipfiles, log_msg, self.btn, regex=list(self.tool_ids_new.keys())) # PS         
        except AssertionError:
            logger.info( 'No tool(s) selected.' )
            return self.__call__()

    def __list_local_files(self): # PS
        '''
        __list_local_files (1) searches all files in the archive folder; (2) updates self.tool_ids
        '''
        if isinstance(self.archive_path, str):
            if os.path.exists(self.archive_path):     
                self.tool_ids = {} 
                for _elem in os.listdir(self.archive_path):
                    #  Determine the file is backup zip file or xml.zip
                    if re.search( f'^({self.REGEX_ZIP}).*zip$', _elem, re.IGNORECASE):
                        parser = re.compile( r'[g|l]_([\w]*)_' )
                        key = 'backup'
                        if key not in self.tool_ids:
                            self.tool_ids.update( { key:{} } )    
                    elif re.search( '(xml).zip$', _elem, re.IGNORECASE):  # f'^(?!{self.REGEX_ZIP}).*zip$'
                        parser = re.compile( r'_I_([\w]*)_M_' ) # r'_([\w]{6})_M__D' 
                        key = 'xml'
                        if key not in self.tool_ids:
                            self.tool_ids.update( { key:{} } )
                    else: # Other file type; ex: ..zip or ...rpt
                        parser = []
                    try:
                        if parser:
                            tool_id = parser.findall( _elem )[0]
                            if tool_id in self.hash_table_temp.keys():
                                tooltype_app = self.hash_table_temp[tool_id][0]
                                if tooltype_app in self.tool_ids[key].keys():
                                    if tool_id in self.tool_ids[key][tooltype_app].keys():
                                        self.tool_ids[key][tooltype_app][tool_id].update( { _elem : [  tool_id, os.path.join(self.archive_path,_elem),  
                                                                            (datetime.datetime.fromtimestamp(
                                                                                os.path.getmtime(
                                                                                    os.path.join(self.archive_path,_elem) )  ) )  ]  } )                                
                                    else: 
                                        self.tool_ids[key][tooltype_app].update( { tool_id : { _elem : [  tool_id, os.path.join(self.archive_path,_elem),  
                                                                            (datetime.datetime.fromtimestamp(
                                                                                os.path.getmtime(
                                                                                    os.path.join(self.archive_path,_elem) )  ) )  ] } } )
                                else:     
                                    self.tool_ids[key].update( { tooltype_app: { tool_id : { _elem : [  tool_id, os.path.join(self.archive_path,_elem),  
                                                                            (datetime.datetime.fromtimestamp(
                                                                                os.path.getmtime(
                                                                                    os.path.join(self.archive_path,_elem) )  ) )  ] } } }  ) # .strftime('%Y-%m-%d')
                    except IndexError:
                        logger.info(f'Failed to extract tool name for {_elem}')
            else:
                raise ValueError (f'{self.archive_path} does not exists')     
        else:   # ps
            pass

    def __query_local_files_new(self):
        '''
        __query_local_files_new (1) searches for the archive files
        in the local fs to create self.tool_ids with key "tooltype_application"
        '''
        if self.err_code: 
            return
        self.__list_local_files() # update self.tool_ids (backup and xml)
        if self.autopm_auth and 'xml' in self.tool_ids: # PS
            self.key = 'xml'
        else: 
            self.key = 'backup'
            
        if hasattr( self, 'application'): # 'application' in self.__dict__.keys()
            self.query_local_toolids() # PS
        else: # Local
            self.__create_button([self.query_local_toolids])             
            log_msg = f'Select tooltype_application for {self.key} files from {self.archive_path}' 
            self.widget_obj = WidgetsWrapper( list(self.tool_ids[self.key].keys()) , log_msg, self.btn)            

    def query_local_toolids(self): 
        '''
        query_local_toolids (1) shows tools in selected tooltype_application 
        for the archive files in the local fs to update self.tool_ids 
        '''        
        try: 
            if hasattr( self, 'application' ): #  'application' in self.__dict__.keys()
                pass
            else:
                assert self.widget_obj.items
                if len(self.widget_obj.items) >1:
                    raise ValueError        
                self.application = self.widget_obj.items[0].split('_')[-1] # selected applicaiton
                self.tooltype_application = self.widget_obj.items[0]
        except AssertionError:
            logger.info( 'No tooltype_application selected.' )
            return self.__call__()     
        except ValueError:
            logger.info( 'More than 1 tooltype_application are selected. Please re-select' )
            return self.__call__()          
        log_msg = f'Select needed tools for {self.key} files from {self.archive_path}'
        self.__create_button([self.query_local_backup]) 
        self.widget_obj = WidgetsWrapper(list(self.tool_ids[self.key][self.tooltype_application].keys()), log_msg, self.btn) 

    def query_local_backup(self): 
        '''
        query_local_backup (1) shows backup files / xml files for selected tools to
        create self.zipfiles
        '''          
        try:
            logger.info( f'{self.widget_obj.items}' )
            assert self.widget_obj.items
            self.tool_ids_new = {_elem: self.tool_ids[self.key][self.tooltype_application][_elem] for _elem in self.widget_obj.items} # PS
            self.zipfiles={}
            _ = [self.zipfiles.update(_elem) for _elem in self.tool_ids_new.values()]
            # PS reverse time for local backup files
            self.zipfiles = dict( sorted( self.zipfiles.items(), key = lambda x: x[1][-1], reverse = True ) ) 
            
            zipfiles = [key for key, data in self.zipfiles.items()]
            if self.key == 'xml': # xml files
                log_msg = f'Select {self.key} files from each of the following tools, {", ".join(self.tool_ids_new.keys())}'
                self.__create_button([self._exec_autopm], **self.autopm_auth) # PS
                self.widget_obj = WidgetsWrapper(zipfiles, log_msg, self.btn )  
            else: # backup files
                log_msg = f'Select {self.key} files from each of the following tools, {", ".join(self.tool_ids_new.keys())}'
                self.__create_button([self.query_archive_files], fs_type=0)    
                self.widget_obj = WidgetsWrapper(zipfiles, log_msg, self.btn, regex=list(self.tool_ids_new.keys() ) )           
        except AssertionError:
            if hasattr(self, 'hdfs'): # PS not select local files in EIDH
                if self.key == 'xml':
                    self.key = 'backup'
                    self.query_local_toolids()
                else:
                    self.__query_golden_info()
            else:
                logger.info( 'No tool(s) selected.' )
                return self.__call__()    
        
    def _exec_hdfs_autopm(self, toolid, tool_info, timestamp, xml, duration, basename, **kwargs):
        '''
        _exec_hdfs_autopm parses xml files via autopm.AutoPM for local files
        Parameter:
        see self.__init__() for autopm descriptions
        '''
        logger.info(f'Executing SOLR query for {toolid}*{basename}*.xml')
        dtfmt = '%Y-%m-%d'
        start = (timestamp - datetime.timedelta(days=duration)).strftime(dtfmt)
        end = (timestamp + datetime.timedelta(days=duration)).strftime(dtfmt)
        self.autopmObj = autopm.AutoPM(xml)
        self.autopmObj._solr_search(
            solr_auth=self.solr_auth, 
            tool_ID=toolid, 
            chamber_ID='', 
            file_type=4, 
            start_time=f'{start} TO {end}', 
            basename=basename,
            tool_info=tool_info
        )
        #logger.info(f'self.autopmObj.hdfs_paths-{self.autopmObj.hdfs_paths}')
        #logger.info(f'self.autopmObj.solr_properties-{self.autopmObj.solr_properties}')
        # self.autopm_obj._local_search(archive_file)
        self.autopmObj()

    def _exec_autopm(self, xml={}, **kwargs):
        '''
        _exec_autopm parses xml files via autopm.AutoPM for local files
        Parameter:
        see self.__init__() for autopm descriptions        
        '''
        self.backup_xmls = {} # PS
        self.autopmObj = autopm.AutoPM(xml) # sqlpath
        if self.widget_obj.items:
            archive_files = [re.split(',',_elem)[-1].strip() for _elem in self.widget_obj.items]
            self.zipfiles = {_elem:self.zipfiles[_elem] for _elem in archive_files}
            
            tool_ids, filenames, timestamps = list(zip(*[_elem for _elem in self.zipfiles.values()] ) )       
            for filename in filenames:
                logger.info(f'Parsing {filename}')
                self.autopmObj._local_search(filename)
                self.backup_xmls.update(
                    {filename:[item['hdfs_path'] for item in self.autopmObj.solr_properties.values()]}
                )
                self.autopmObj()
                file_paths = list(itertools.chain.from_iterable(self.backup_xmls.values()))
            #logger.info(f'self.autopmObj.trd -{self.autopmObj.trd }')  # PS debug
        else:
            self.autopmObj.trd={}
        # Parse backup files
        self.key = 'backup'
        self.query_local_toolids()

    def query_archive_files(self, fs_type = 1):
        '''
        query_archive_files (1) takes the usr input, (2) updates self.zipfiles,
        (3) creates self.filenames and (4) run the data ingestion
        The data ingestion process updates self.rawdata, & self.rawinfo
        Parameter:
        fs_type (0/1): 1 [=] 'hadoop', 0 [=] 'local'
        '''
        try:
            if len(self.widget_obj.items) == 0:
                if fs_type:
                    raise ValueError (f'No archive files was selected from HDFS')
                else:
                    if hasattr(self, 'hdfs'): 
                        pass
                    else:
                        raise ValueError (f'No archive files was selected from {self.archive_path}')
            else:
                archive_files = [re.split(',',_elem)[-1].strip() for _elem in self.widget_obj.items]
                self.zipfiles = {_elem:self.zipfiles[_elem] for _elem in archive_files}
                tool_ids, self.filenames, timestamps = list(zip(*[_elem for _elem in self.zipfiles.values()]))
                kwargs={}
                if fs_type:
                    #VETTING USER INPUTS
                    #misg_tools = set(self.tool_ids.keys()).difference(tool_ids)
                    #if len(misg_tools)>0:
                    #    self.err_code = 4
                    #    raise ValueError (f'No archive files was selected from {misg_tools}')
                    kwargs['hdfs'] = self.hdfs
                archive_db={}
                func = getattr(MultiProcessHelper, 'ingest_archive_file')
                if kwargs:
                    func = functools.partial(func, **kwargs)
                if self.workers == 1:
                    for idx, filename in enumerate(self.filenames):
                        archive_db.update(func(filename, **kwargs))
                else:
                    pool_obj = mp.Pool(processes=self.workers)
                    pool_output = pool_obj.map(func, self.filenames)
                    for data in pool_output:
                        archive_db.update(data)
                for idx, (basename, values) in enumerate(archive_db.items()):
                    toolid, trd, module_info = values
                    #logger.info( f'self.autopmObj.trd-{ self.autopmObj.trd }' ) # local only
                    if self.autopm_auth:
                        if self.solr_auth and fs_type:
                            self._exec_hdfs_autopm(toolid, module_info, timestamps[idx], **self.autopm_auth)
                        if hasattr(self, 'autopmObj'): # no xml files loaded # 'autopmObj' in self.__dict__.keys()
                            if toolid in self.autopmObj.trd:
                                xmlObj = {key:value
                                    for key, value in self.autopmObj.trd[toolid].items()
                                    if key not in trd
                                } 
                                trd.update(**xmlObj)
                                if fs_type:
                                    del self.autopmObj
                    if toolid in self.rawdata:  
                        toolid = f'{toolid}-{idx}'
                    self.rawlog[toolid] = basename                 
                    self.rawdata[toolid] = trd
                    self.rawinfo[toolid] = module_info                    
        except ValueError as err:
            self.err_code = 0
            self.err_msg = err
            if fs_type:
                logger.info(f'{self.err_msg}')
                return self.query_hdfs_toolids_new() # PS self.__query_hdfs_files() 
            else:
                logger.info( f'{self.err_msg}' )
                return self.__query_local_files_new()
        if fs_type and self.local_query : # PS Need to query local files
            self.__query_local_files_new()  # PS EIDH test
            #self.__query_golden_info()
        else:
            self.__query_golden_info()

    def __query_golden_info(self, **kwargs):
        '''
        __query_golden_info allows user to select 1x archive file
        to represent a golden platform
        The external button triggers self.__query_golden_pm()
        '''
        # if there is only one platform, set self.enable_golden to False
        self.enable_golden = (len(self.rawinfo) > 1) and (self.enable_golden)
        if self.enable_golden:
            log_msg = f'''\ 
            Select 1x golden tool that you wish to specify the golden chamber. 
            Select Accept to not specify any golden chamber chamber for this analysis
            ''' # PS
            self.__create_button([self.__query_golden_pm]) 
            self.widget_obj = WidgetsWrapper(self.rawinfo.keys(), log_msg, self.btn)
        else:
            self.widget_obj.items=[]
            self.__query_golden_pm()

    def __query_golden_pm(self):
        '''
        __query_golden_pm (1) creates self.sample, & self.golden_tool,
        (2) allows user to select a golden chamber among the pms from 
        the golden platform
        The external button triggers self.query_golden_pm(), self.main()
        '''      
        self.sample = list(self.rawinfo.keys())
        try:
            self.golden_tool = self.widget_obj.items[0]
            self.sample.remove(self.golden_tool)
            #pmaliases = [_elem for _elem in self.rawinfo[self.golden_tool]
            #    if re.search(r'pm\d+', _elem, re.IGNORECASE)]
            self.delimiter = '_'
            pmaliases =  [f'{_elem}{self.delimiter}{module_type}' 
                           for _elem, module_type in self.rawinfo[self.golden_tool].items()
                           if re.search(r'pm\d+', _elem, re.IGNORECASE)]  # PS PM1_MODULE
            pmaliases = sorted(pmaliases) # sort pmaliases # PS
            
            log_msg = f'Select 1x PM from {self.golden_tool} to be specified as the golden chamber'
            self.__create_button([self.query_golden_pm]) 
            self.widget_obj = WidgetsWrapper(pmaliases, log_msg, self.btn)
        except IndexError:
            logger.info('User opted to not specify any golden chambers for this analysis')
            self.enable_golden = False
            self.widget_obj.items=[]
            self.query_golden_pm()            

    def query_golden_pm(self):
        '''
        query_golden_pm (1) updates self.golden, & self.rawdata
        '''
        try:
            golden_pm = self.widget_obj.items[0] # 'PM1' -> 'PM1_Metal'
            golden_pm = re.split( self.delimiter, golden_pm)[0] # 'PM1' 
            self.golden[self.golden_tool] = golden_pm
            pmaliases =  [_elem for _elem in self.rawinfo[self.golden_tool]
                    if re.search(r'pm\d+', _elem, re.IGNORECASE)]
            pmaliases.remove(golden_pm)
            for pmalias in pmaliases:
                self.__purge_pm_data(
                    toolid=self.golden_tool, 
                    pmalias=pmalias
                )
            # change golden pm name
            self.__change_golden_name( toolid = self.golden_tool, golden = golden_pm )
        except IndexError:
            if self.enable_golden:
                return self.__query_golden_info()

        if self.enable_filter: 
            self.__apply_pm_filter()
        else:
            self.main()

    def __change_golden_name(self, toolid, golden ): # tool; PM1
        '''
        Parameter:
        toolid (str): toolID
        golden (str): pmID, e.g., PM1
        '''
        for subsys, info in self.rawdata[toolid].items():
            golden_new = golden+'(golden)' # PM1(golden)
            if golden in info[self.ZIP_FNAME]:
                info[self.ZIP_FNAME][golden_new] = info[self.ZIP_FNAME].pop(golden)
                #del info[self.ZIP_FNAME][pmalias]
            if golden in info[self.ZIP_DATA]:
                #del info[self.ZIP_DATA][pmalias]
                info[self.ZIP_DATA][golden_new] = info[self.ZIP_DATA].pop(golden)

    def __apply_pm_filter(self):
        '''
        __apply_pm_filter allows user to (1) select all pms
        to be used in this analysism and (2) updates self.rawdata
        '''
        self.delimiter = '_'
        pmaliases =  [[f'{toolid}{self.delimiter}{_elem}{self.delimiter}{module_type}' 
                for _elem, module_type in self.rawinfo[toolid].items()
                if re.search(r'pm\d+', _elem, re.IGNORECASE)]
                for toolid in self.sample]
        pmaliases = list(itertools.chain.from_iterable(pmaliases))
        pmaliases = sorted(pmaliases) # PS
        log_msg = f'Select all PMs to be used in the analysis'
        self.__create_button([self.apply_pm_filter], pmaliases=pmaliases) #, self.main
        self.widget_obj = WidgetsWrapper(pmaliases, log_msg, self.btn)

    def apply_pm_filter(self, pmaliases):
        try:
            if len(self.widget_obj.items) == 0: 
                raise ValueError('No PMs was selected in this analysis')
            pmaliases = [_elem for _elem in pmaliases 
                        if _elem not in self.widget_obj.items]
            for pmalias in pmaliases:
                parsed_text = re.split(f'\\{self.delimiter}', pmalias)
                if len(parsed_text) > 3:
                    parsed_text = [item for item in parsed_text
                        if re.search(r'pm\d+', item, re.IGNORECASE)
                    ]
                    parsed_text = parsed_text[0]
                    toolid, pmalias = re.search(f'^(.*?)_({parsed_text})',pmalias).groups()
                else:
                    toolid, pmalias, module_type = parsed_text
                self.__purge_pm_data(toolid, pmalias)
        except ValueError as err:
            logger.error(err)
            return self.__apply_pm_filter()
        self.main()

    def __purge_pm_data(self, toolid, pmalias):
        '''
        Parameter:
        toolid (str): toolID
        pmalias (str): pmID, e.g., PM1
        '''
        for subsys, info in self.rawdata[toolid].items():
            if pmalias in info[self.ZIP_FNAME]:
                del info[self.ZIP_FNAME][pmalias]
            if pmalias in info[self.ZIP_DATA]:
                del info[self.ZIP_DATA][pmalias]
            
    def __call__(self): # PS
        path, filename = os.path.split(self.sqlObj._whoami(abspath=True))
        log_msg = f'Select the application that you would like launch'
        applications = ['Update applications for tools', 'Generate AutoTRD']
        self.__create_button([self._exec_app]) 
        self.widget_obj = WidgetsWrapper(applications, log_msg, self.btn, limit=True )     
        
    def _exec_app(self):
        path, filename = os.path.split(self.sqlObj._whoami(abspath=True)) # PS
        self.updateDBObj = UpdateDB( filename=filename, path=path )
        self.updateDBObj()
        self.hash_table_temp = self.updateDBObj.hash_table
        self.df_temp = self.updateDBObj.df
        #logger.info( f'{self.df_temp}' ) # PS  
        try:
            app = self.widget_obj.items[0]           
            if app =='Generate AutoTRD':
                try: 
                    if hasattr(self, 'hdfs'):
                        self.query_hdfs_toolids_new() 
                    else:
                        self.__query_local_files_new()
                except ValueError: 
                    pass
            else:
                #path, filename = os.path.split(self.sqlObj._whoami(abspath=True)) # PS
                self.__create_button([], btn_func=self._update_application)     
                self.updateappObj = UpdateApplication( filename=filename, path=path )
                self.updateappObj(self.btn)
        except ValueError: 
            logger.info('Value Error')
            btnObj = ipywidgets.Button()
            def btnfunc(btnObj):
                self.__call__()
            btnObj.on_click(btnfunc)            

    def _update_application(self, button): # PS
        try:
            #clear_output() # PS
            self.updateappObj._fetchall()
            if len(self.updateappObj.data) != 3:
                raise AssertionError
            if_exists = self.updateappObj._check_update()            
            self.__call__()
        except AssertionError:
            self.__call__()

class IpynbGUI(lamsqlite.LamSqliteUtil): # PS
    def __init__(self, filename, path='.'):
        super().__init__(os.path.join(path,filename))
        self.err_msg = None        
        # super().__call__()

    def _create_button(self, usrcmds, 
        description='Accept', 
        autorefresh=True, 
        btn_func=None,
        **kwargs):
        '''
        __create_button creates a ipywidgets.Button in the nb
        that has access to all attributes of this class
        Parameter:
        usrcmds (list of methods): methods (functions) to be
        executed immediately after receiving user inputs
        '''
        self.btn = ipywidgets.Button(
            description = description,
            layout=ipywidgets.Layout(width='50%')
        )
        def default_btn_funct(button):
            self.widget_obj.items = [_elem.description 
                for _elem in self.widget_obj.options if _elem.value] 
            if self.err_msg:
                logger.error(self.err_msg)
                self.err_msg = None
            #<True> DESTROY THE BUTTON AFTER 1 CLICK
            if autorefresh:
                clear_output()
                logger.info(f'{self.widget_obj.log_mgs}\nSelected Options: {self.widget_obj.items}')
            self.btn.disabled=False
            _ = [funt(**kwargs) for funt in usrcmds]
        if btn_func:
            self.btn.on_click(btn_func)
        else:
            self.btn.on_click(default_btn_funct)

    def _exec_fetch(self, cmd):
        '''
        fetch a single attribute from the table
        '''
        self.cursor.execute(cmd)
        return [_elem[0] for _elem in self.cursor.fetchall()]

    def _add_param(self, func, objname, 
        options=[], description='', 
        handler=None, width=120, **kwargs):
        '''
        _add_param creates a oneline widget object
        Parameter
        objname (str): attribute name
        options (list): [1, 2, 3] or [(one, 1), (two, 2), (three, 3)]
        description (str):
        '''
        params = kwargs
        if options:
            if isinstance(options[0], tuple):
                default_value = options[0][1]
            else:
                default_value = options[0]
            params.update({
                'options':options,
                'value':[default_value] if func == 'SelectMultiple' else default_value
            })
        self.__dict__[objname] = getattr(ipywidgets, func)(
            description=description,
            disabled=False,
            layout=ipywidgets.Layout(height='auto', width='auto'),
            style = {'description_width': f'{width}px'},
            **params
        )
        if handler:
            self.__dict__[objname].observe(handler, names='value')

    def _augm_layout(self, widgetObj=None):
        if widgetObj == None:
            widgetObj = self.widget_obj

        attrs = ['caption_widget', 'search_widget', 'ext_btn', 'select_btn', 'deselect_btn']
        for attr in attrs: 
            getattr(widgetObj, attr).layout = ipywidgets.Layout(height='auto', width='auto')
        getattr(widgetObj, 'options_widget').layout = ipywidgets.Layout(
            overflow='auto',
            border='1px solid black', 
            height='auto', 
            width='auto',
            flex_flow='column',
            display='flex'
        )
        return widgetObj

    def _extract_dt(self, widgetObj):
        try:
            dt = datetime.datetime.strftime(widgetObj.value, '%Y-%m-%d')
        except TypeError:
            dt = ''
        return dt

    def _build_hash_table(self, group, tree={}):
        value = group.pop(0)
        try:
            if len(group)==1:
                tree[value].extend(group)
            else:
                self._build_hash_table(group, tree[value])
        except KeyError:
            if len(group)==1:
                tree.update({value:group})
            else:
                tree[value]={}
                self._build_hash_table(group, tree[value])

    def _handler_parent_option(self, change, parent, child, search_type=0):
        '''
        Parameter:
        search_type (1/0): 1 [=] optional, 0 [=] required
        '''
        hash_levels = []
        flag = False
        for key in self.hash_levels.keys():
            if flag: break
            if key == parent: flag=True
            value = self.__dict__[key].value
            if isinstance(value, tuple):
                value = list(value)
            hash_levels.append(value)
        # hash_levels.append(list(self.__dict__[parent].value))
        
        if isinstance(hash_levels[0], (list,tuple)):
            hash_levels = [list(path) for path in itertools.product(*hash_levels)]
        else:
            hash_levels = [hash_levels]
        self._update_child_options(child, hash_levels, search_type)

    def _exists_nested_data(self, data, path):
        if path:
            key = path.pop(0)
            self.dummy=data
            if isinstance(data, (dict,list)):
                data = data[key]
                return self._exists_nested_data(data, path)
            else:
                return key == data  

    def _update_child_options(self, name, hash_levels, search_type=0):
        options = []
        for path in hash_levels:
            try:
                data = self._extract_nested_data(data=self.hash_table, path=path)
                if isinstance(data, dict):
                    data = list(data.keys())
            except KeyError:
                data = []
            options.extend(data)
        options = list(set(options))
        options.sort()
        if search_type:
            options.insert(0,'')
        self.__dict__[name].options = options
        self.__dict__[name].hidden_options = []

    def _select_btn(self, button, objname):
        self.__dict__[objname].value = self.__dict__[objname].options

    def _deselect_btn(self, button, objname):
        self.__dict__[objname].value = ()

    def _exec_regex(self, change, objname):
        # values = self.__dict__[objname].value
        options = list(self.__dict__[objname].options)
        hidden_options = self.__dict__[objname].hidden_options
        options.extend(hidden_options)
        regex_value = change['new']
        if regex_value:
            hidden_options = [_elem for _elem in options
                if not re.search(regex_value, _elem, re.IGNORECASE)]
        else:
            hidden_options = []
        options = [option for option in options if option not in hidden_options]
        self.__dict__[objname].hidden_options = hidden_options
        self.__dict__[objname].options = options

    def _extract_nested_data(self, data, path):
        if path:
            key = path.pop(0)
            data = data[key]
            return self._extract_nested_data(data, path)
        else:
            return data


class UpdateDB(Logger, IpynbGUI): # PS
    def __init__(self, filename, path):
        super().__init__(filename=filename, path=path)
        #logger.info( f'{self.filename}; {self.path}' ) # autotrd_logger_test.db; /tmp/
    def _hashing(self):
        self.hash_levels = {
            'tooltypeObj':{
                'alias':'tooltype',
                'attrs':['tooltype_name'],
                'ftm':'tooltype_name as tooltype'
            },
            'toolObj':{
                'alias':'tool',
                'attrs':['tool_name'],
                'ftm':'tool_name as tool',
                'limit': 6
            },
            'applicationObj': {
                'alias':'application',
                'attrs':['application'],
                'ftm':'' # 
            },
        }
        self._exec_query(  
            leaf_tb=self.TB_SCENARIOS,
            attrs=[obj['ftm'] for obj in self.hash_levels.values() if obj['ftm']],
            inplace=True
        )
        attrs = ', '.join([obj['alias'] for obj in self.hash_levels.values()])
        cmd = f'SELECT DISTINCT {attrs} FROM TEMP_HEADER'       
        df = pd.read_sql_query(cmd, self.cnx)
        self.hash_table = {}
        df['tooltype_app'] = df['tooltype']+ '_' + df['application']
        df = df.drop( ['tooltype', 'application' ], axis = 1 )
        for col in df.columns:
            df[col] = df[col].str.upper()
        for data in df.values.tolist():
            self._build_hash_table(data, self.hash_table)
        self.df = df
        logger.info(f'df-{df}') # PS 

    def __call__(self):
        super().__call__()
        self._hashing()

class UpdateApplication(UpdateDB): # PS
    PLACEHOLDERS = { 
        'tooltype_name': [
            'KiyoCX', 'KiyoEXP', 'KiyoFXE', 'Flex45', 'FlexFL', 'FlexGL', 'MetalM', 'MetalNXP', 'VersysStrip', 'Solo', 'KiyoGX'],
        'tool_name': 'ABCDE1'
    } # 'Coronus', 'CoronusHP', 'Argos', 'KiyoGX', 'KiyoEX', 'FlexEL', 'FlexGL'

    def __init__(self, filename, path):
        super().__init__(filename=filename, path=path)
        #logger.info( f'{self.filename}; {self.path}' ) # autotrd_logger_test.db; /tmp/
    def copy_logger(self, filename, path): # PS
        if filename in os.listdir('./'): #  if exists in current folder, remove first
            os.remove( './' + filename )
        shutil.copy( path+'/'+filename, './' + filename ) # copy from /tmp to current folder

    def _check_update(self): # PS
        if_exists = True
        tooltype = self.data['tooltype_name'] 
        tool = self.data['tool_name'] 
        application = self.data['application']
        if str.upper(tool) in self.hash_table.keys(): # check if tool exists
            # check if tooltype exists
            if str.upper(tooltype) in self.hash_table[ str.upper(tool) ][0]: # only update application
                logger.info(f'Application exists for {str.upper(tooltype)}-{str.upper(tool)} and update application "{application}" ')
                cmd = f'''
                UPDATE {self.TB_TOOLS}
                SET application = ?
                WHERE tool_name = ?
                '''
                self.cursor.execute(cmd, (application, tool))
                self.cnx.commit()
                self.copy_logger(self.filename, self.path)
                logger.info( f'Copy db file to ./{self.filename}' )
                # update self.hash_table
                self._hashing()           
            else: # tool type diferent => update tool type and application
                logger.info(f'Update tool type "{str.upper(tooltype)}" and application "{application}" for {str.upper(tool)}')
                # Find tooltype_id_value
                tooltype_id_value = self._lookup_tooltype_id( tooltype )
                # Find tool_id_value
                key = self._get_primary_key(self.TB_TOOLS)
                cmd = self._exec_query( leaf_tb=self.TB_TOOLS, tool_name = tool, attrs= [key], inplace=False)
                cmd = f'SELECT {key} FROM ({cmd})'
                self.cursor.execute(cmd)
                tool_id_value = self.cursor.fetchone()[0]              
                # Update tooltype_id and application in table TB_TOOLS
                cmd = f'''
                UPDATE {self.TB_TOOLS}
                SET tooltype_id = ?,
                application = ?
                WHERE tool_name = ?
                '''
                self.cursor.execute(cmd, (tooltype_id_value, application, tool))
                self.cnx.commit()
                # Update tooltype_id in table TB_SCENARIOS
                cmd = f'''
                UPDATE {self.TB_SCENARIOS}
                SET tooltype_id = ?
                WHERE tool_id = ?
                '''
                self.cursor.execute(cmd, (tooltype_id_value, tool_id_value))
                self.cnx.commit()                
                
                self.copy_logger(self.filename, self.path)
                logger.info( f'Copy db file to ./{self.filename}' )
                # update self.hash_table
                self._hashing() 
        else:
            if_exists = False
            func = functools.partial(self._insert_tables)
            s_id = func(**self.data)  
            logger.info(f'Add application {application} for {str.upper(tooltype)}-{str.upper(tool)} ')
            self.copy_logger(self.filename, self.path)
            logger.info( f'Copy db file to ./{self.filename}' )
            # update self.hash_table
            self._hashing() 
            logger.info( f'self.hash_table:{self.hash_table}') # PS
        return if_exists
    
    def _insert_tables(self, tooltype_name, tool_name, application , **kwargs):   
        tooltype_id = self._insert(
            tb=self.TB_TOOLTYPE,
            tooltype_name=tooltype_name
        )
        tool_id = self._insert(
            tb=self.TB_TOOLS,
            tool_name = tool_name, 
            tooltype_id = tooltype_id,   
            application = application,
        )
        s_id = self._insert(
            tb=self.TB_SCENARIOS,
            tooltype_id = tooltype_id,
            tool_id = tool_id,
        )
        self.cnx.commit()
        return s_id
    
    def _lookup_tooltype_id(self, tooltype_ ): # PS
        key = self._get_primary_key(self.TB_TOOLTYPE)
        cmd = self._exec_query( leaf_tb=self.TB_TOOLTYPE, tooltype_name = tooltype_, attrs= [key], inplace=False )
        cmd = f'SELECT {key} FROM ({cmd})'
        self.cursor.execute(cmd)
        try:
            tooltype_id = self.cursor.fetchone()[0]      
            return tooltype_id
        except TypeError:
            tooltype_id = self._insert( tb = self.TB_TOOLTYPE, tooltype_name = tooltype_ )
            return tooltype_id

    def _fetchall(self):
        self.data={}
        try:
            for key, data  in self.hash_levels.items():
                for attr in data.get('attrs'):
                    objname = f'{attr}Obj'
                    value = self.__dict__[objname].value
                    value = value.lower().strip()
                    if value:
                        value = value.lower().strip()
                        if 'limit' in data.keys():
                            limit = data.get('limit')
                            if len(value) != limit: 
                                raise ValueError
                            else:
                                self.data.update({attr:value})
                            #value = re.sub(r'\s{2,}',' ',value)
                        else:
                            self.data.update({attr:value})
                    else: #
                        self.data={}
                        raise TypeError 
        except ValueError:
            logger.info('Length of tool name you typed is not 6. Please update again')
        except TypeError:
            logger.info('The value you typed is empty. Please update again')            
            
    def _application_editor(self, cell_height, objWidth):
        self._add_param(
            func='HTML',
            objname=f'editorObj_html', 
            description='', 
            indent=False,
            value=f"<i><font color='black', font size=4>Edit Appliction</b>"
        )
        for key in ['tooltypeObj', 'toolObj', 'applicationObj']:
            for attr in self.hash_levels.get(key).get('attrs'):
                objname = f'{attr}Obj'
                placeholder=self.PLACEHOLDERS.get(attr,'')
                mo = re.search('^(.*?)(_?)name', attr)
                if mo:
                    attr = f'{mo.group(1).title()} Name' 
                else:
                    attr = attr.title()
                params = {
                    'objname':objname,
                    'description':f'{attr}:',
                    'width':objWidth,
                    'indent':False
                }
                if isinstance(placeholder,str):
                    params.update({
                        'func':'Text',
                        'placeholder':placeholder
                    })
                else:
                    params.update({
                        'func':'Dropdown',
                        'options':placeholder
                    })
                self._add_param(
                    **params
                )

        # self.passcodeObj.layout = ipywidgets.Layout(height='auto', width='auto')
        btnObj = ipywidgets.Button(description='Update Application')
        btnObj.layout = ipywidgets.Layout(height='auto', width='auto')
        def btnfunc(btnObj):
            self.btnObj.click()
        btnObj.on_click(btnfunc)

        editorObj = ipywidgets.GridspecLayout(
            height=f'{cell_height}px',
            n_rows=cell_height//40, 
            n_columns=4
        )
        editorObj[0,1:3] = self.editorObj_html
        editorObj[1,1:3] = self.tooltype_nameObj
        editorObj[2,1:3] = self.tool_nameObj
        editorObj[3,1:3] = self.applicationObj
        editorObj[4,1:3] = btnObj       
        return editorObj

    def __call__(self, btnObj, cell_height=220, objWidth=100, inplace=True):
        self.btnObj = btnObj 
        self.btnObj.layout = ipywidgets.Layout(height='auto', width='auto')
        super().__call__()
        #self._hashing()
        editorObj = self._application_editor(cell_height,objWidth)
        self.ui = ipywidgets.Tab()
        pannels = [editorObj] 
        keys = ['Editor']
        self.ui.children = pannels
        for pannel_id in range(len(self.ui.children)):
            self.ui.set_title(pannel_id, keys[pannel_id])
        if inplace:
            display(self.ui)

class MultiProcessHelper():
    @staticmethod
    def create_shell_cmd(func, workers, args, 
            ofile='Ofile.pickle', 
            kwargs=None,
            script='autotrd_mp_via_shell.py',
            inplace=False
        ):
        logger.info('Generating shell command...')
        ifile = 'Ifile.pickle'
        if not isinstance(args, list):
            args = list(args)
        workers = workers if workers < len(args) else len(args)

        try:
            if isinstance(args[0],str): raise TypeError
            args = list(map(list, args))
        except TypeError:
            pass

        data = {
            'workers':workers,
            'args': re.sub(', ',',',f'{args}'),
            'ofile':ofile,
            'func': func
        }
        if kwargs:
            with open(ifile, 'wb') as fobj:
                pickle.dump(kwargs, fobj)
                data['ifile']=ifile
        flags = ' '.join([f'-{key}={value}'for key, value in data.items()])
        sys_paths = ' '.join(sys.path)
        cmd = f"python {script} {sys_paths} {flags}"
        logger.debug(cmd)
        logger.info(f'Running {func} via subprocess...')
        if inplace:
            process = subprocess.run(cmd, 
                    shell=True,
                    stdout=subprocess.PIPE, 
                    stderr=subprocess.PIPE,
                    universal_newlines=True
                )
            return process
        else:
            return cmd

    @staticmethod
    def parse_shell_cmd(cmd):
        ATTR_KEYS = ['func', 'ifile', 'workers', 'args', 'ofile']
        data = {}
        for key in ATTR_KEYS:
            mo = re.search(f'-{key}=(.*?)(\W-|$)', cmd, re.IGNORECASE)
            if mo:
                data[key] = mo.group(1)
        logger.debug(data)
        try: 
            args = [[value for  value in _elem.split(',') if len(value)>0]
                    for _elem in re.findall(r'\[(.*?)\]', data['args'][1:-1])]                    
            assert (len(args) > 0)
            data['args'] = args
        except AssertionError:
            data['args'] = [_elem.strip() for _elem in data['args'][1:-1].split(',')]
            data['args'] = [eval(_elem) for _elem in data['args'] if len(_elem) > 0]
        data['workers'] = int(data['workers'])
        if 'ifile' in data:
            kwargs = MultiProcessHelper.pickle_load_obj(data['ifile'])
            return data, kwargs
        else:
            return data

    @staticmethod
    def pickle_load_obj(ofile='Ofile.pickle'):
        obj = []
        if os.path.exists(ofile):
            with open(ofile, 'rb') as fobj:
                obj = pickle.load(fobj)
            os.remove(ofile)
        return obj

    @staticmethod
    def query_hdfs_path(tool_id, path, hdfs):
        path = f'{path}/data/backup'
        if hdfs.exists(path):
            #QUERY BACKUP FILES FROM THE HADOOP FILE SYSTEM
            stdout = HadoopSandbox._HadoopSandbox__run_shell_script(HadoopSandbox,
                    path=path,
                    pipe=["awk -F'/' '{ print  $0, $NF}'", # split the input by '/', return the input and the last element 
                        "awk '$NF ~ /^all.*zip/ {print $1}'"],# run the regex command, and if matched then return the first input
                    recursive_mode=True,
                    inplace=False
                )
            tmp = {_elem.split('/')[-1]:[tool_id, 
                _elem, 
                datetime.datetime.fromtimestamp(hdfs.info(_elem)['last_modified'])]
                for _elem in stdout 
                if hdfs.exists(_elem)}
            if len(tmp) > 0:
                tmp = dict(sorted(tmp.items(), key=lambda item: item[1][2], reverse=True))
                return {tool_id:tmp}
                
    @staticmethod
    def ingest_archive_file(filename, **kwargs):
        ATTR_MODULE = 'module' # The ingestion class used to load the backup files
        INGEST_FUNCT = 'funct1'
        lam_obj = Lam2300(filename, **kwargs)
        lam_obj()
        for key, data in lam_obj.trd.items():
            try:
                del data[INGEST_FUNCT]
            except KeyError:
                pass
        basename = os.path.basename(filename)
        trd = lam_obj.trd
        module_info = {}
        for pmalias, data in lam_obj.module.items():
                aliases = [pmalias]
                aliases += data[lam_obj.ATTR_KEY]
                module_info.update({alias:data[lam_obj.ATTR_MODULE] for alias in aliases})
        toolid = lam_obj.toolid
        return {basename:(toolid, trd, module_info)}

    @staticmethod
    def create_trd(key, **kwargs):
        autotrd_obj = autotrd()
        autotrd_obj.__dict__.update(**kwargs)
        autotrd_obj._autotrd__create_trd(key)
        db = {}
        for attr, data in autotrd_obj.trd[key].items():
            if attr not in kwargs:
                db[attr] = data
        logger.info(f'create_trd: db: {db}')
        return {key:db}
